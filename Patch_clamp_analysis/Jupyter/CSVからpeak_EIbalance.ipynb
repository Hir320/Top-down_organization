{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6f502e-bf83-47e4-ad45-fc935e3fb792",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext cudf.pandas\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys; sys.path.append('./')\n",
    "import log_search\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Define a function to extract the pattern 'B_1 A 2.4 0.31 ms' or similar\n",
    "def extract_pattern(log_value):\n",
    "    if isinstance(log_value, str):\n",
    "        match = re.search(r'[BR]_\\d+ A (\\d+\\.\\d+%?|\\d+%?|\\d+) \\d+(\\.\\d+)? ms', log_value)\n",
    "        if match:\n",
    "            return match.group(0)\n",
    "    return None\n",
    "def get_filenames_by_group(df):\n",
    "    # Group by 'SliceID', 'CellID', and 'extracted_log'\n",
    "    grouped = df.groupby(['SliceID', 'CellID', 'extracted_log'])['filename'].apply(list)\n",
    "    \n",
    "    return grouped.reset_index(name='filenames')\n",
    "\n",
    "def read_csvs_in_directory(filenames, log_data):\n",
    "    # Dictionary to store dataframes\n",
    "    dfs = []\n",
    "    for filename in filenames:\n",
    "        abffilename = filename[0:8] + '.abf'\n",
    "        # Filter the dataframe for the target filename\n",
    "        target_row = log_data[log_data['filename'] == abffilename]\n",
    "        \n",
    "        # Extract the log entry for the target row\n",
    "        log_entry = target_row.iloc[0]['log']\n",
    "        slice_id = target_row.iloc[0]['SliceID']\n",
    "        cell_id = target_row.iloc[0]['CellID']\n",
    "        \n",
    "        csv_file = os.path.join(csv_directory, filename)\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            df['filename'] = filename[0:8]\n",
    "            df['SliceID'] = slice_id\n",
    "            df['CellID'] = cell_id\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    dfs = pd.concat(dfs, axis=0, sort=True)\n",
    "    return dfs\n",
    "\n",
    "def max_abs_im_scaled_during_period(data, start_time, end_time):\n",
    "    \"\"\"\n",
    "    Calculate the maximum absolute value of Im_scaled for each filename during a specified period.\n",
    "    \n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The original dataframe containing the data.\n",
    "    start_time (float): The start time of the period.\n",
    "    end_time (float): The end time of the period.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: A dataframe with filenames and their corresponding max absolute Im_scaled values during the specified period.\n",
    "    \"\"\"\n",
    "    # Filter the data based on the specified time period\n",
    "    filtered_data = data[(data['Time (seconds)'] >= start_time) & (data['Time (seconds)'] <= end_time)]\n",
    "    \n",
    "    # Calculate the maximum absolute value of Im_scaled for each filename within the specified period\n",
    "    max_abs_im_scaled_period = filtered_data.groupby('filename')['Im_scaled'].apply(lambda x: x.abs().max()).reset_index()\n",
    "    \n",
    "    # Rename the columns for better readability\n",
    "    max_abs_im_scaled_period.columns = ['filename', 'max_abs_im_scaled']\n",
    "    \n",
    "    return max_abs_im_scaled_period\n",
    "\n",
    "def calculate_mean_peak_times(file_paths, file_color):\n",
    "    \"\"\"\n",
    "    Calculate the mean of Peak Time for a given ADC_Name based on the color passed for a list of file paths.\n",
    "    \n",
    "    Parameters:\n",
    "    file_paths (list of str): The list of paths to the CSV files.\n",
    "    file_color (str): The color to determine the ADC_Name ('Blue' or 'Red').\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame with file names (without extension) and their corresponding mean Peak Times.\n",
    "    \"\"\"\n",
    "    mean_peak_times = []\n",
    "    file_names = []\n",
    "    \n",
    "    # Determine the ADC_Name based on the color\n",
    "    adc_name = 'Stim_2' if file_color == 'Blue' else 'Stim_3'\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        # Extract the file name without extension\n",
    "        file_name = os.path.splitext(os.path.basename(file_path))[0].replace('_peak', '')\n",
    "        file_names.append(file_name)\n",
    "        \n",
    "        # Check if the file exists\n",
    "        if os.path.exists(file_path):\n",
    "            # Load the CSV file\n",
    "            data = pd.read_csv(file_path)\n",
    "            \n",
    "            # Filter the data based on the ADC_Name\n",
    "            filtered_data = data[data['ADC Name'] == adc_name]\n",
    "            \n",
    "            # Calculate the mean of Peak Time\n",
    "            mean_peak_time = filtered_data['Peak Time'].mean()\n",
    "            \n",
    "            # Append the mean peak time to the list\n",
    "            mean_peak_times.append(mean_peak_time)\n",
    "        else:\n",
    "            # If the file does not exist, append None to the list\n",
    "            mean_peak_times.append(None)\n",
    "    \n",
    "    # Create a DataFrame with file names and mean peak times\n",
    "    result_df = pd.DataFrame({\n",
    "        'filename': file_names,\n",
    "        'mean_peak_time': mean_peak_times\n",
    "    })\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf120aa4-8ee4-44ca-9b80-dfcaaa2545a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_filename = '2024.XX.XX.csv'\n",
    "# Find the file recursively\n",
    "directory_log_filename = log_search.find_file_recursively(\"./Data\", log_filename)\n",
    "log_data = pd.read_csv(directory_log_filename)\n",
    "csv_directory = os.path.dirname(directory_log_filename)\n",
    "\n",
    "# Apply the function to the 'log' column to extract the pattern\n",
    "log_data['extracted_log'] = log_data['log'].apply(extract_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6759641-597f-4593-a93d-2ccf24d6248e",
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_filenames = log_data['filename'].str.replace('.abf', '_bluestim.csv', regex=False).to_list()\n",
    "peak_filenames = log_data['filename'].str.replace('.abf', '_peak.csv', regex=False).to_list()\n",
    "blue_data = read_csvs_in_directory(blue_filenames, log_data)\n",
    "peak_data = read_csvs_in_directory(peak_filenames, log_data) #結局使ってない?\n",
    "\n",
    "mean_blue_data = blue_data.groupby(['filename', 'SliceID', 'CellID', 'Time (seconds)']).mean()\n",
    "mean_blue_data = mean_blue_data.reset_index()\n",
    "\n",
    "log_data_for_merge = log_data\n",
    "log_data_for_merge['filename'] = log_data_for_merge['filename'].str.replace('.abf', '')\n",
    "# Filter rows where 'Time (seconds)' is between 1 ~ 2\n",
    "filtered_df = mean_blue_data[(mean_blue_data['Time (seconds)'] >= 1) & \n",
    "                        (mean_blue_data['Time (seconds)'] <= 1.25)]\n",
    "\n",
    "# Find the maximum absolute value of 'Im_scaled' for each filename within this period\n",
    "peak_values_df = filtered_df.groupby('filename').apply(lambda x: x.loc[x['Im_scaled'].abs().idxmax()])\n",
    "peak_values_df = peak_values_df.reset_index(drop=True)\n",
    "peak_values_df_log = pd.merge(peak_values_df, log_data_for_merge, on = ['filename', 'SliceID', 'CellID'])\n",
    "peak_values_df_log.to_csv(os.path.join(csv_directory, 'Peak_data_blue.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f230faf-1544-4de0-aba9-ab68f66ebf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "red_filenames = log_data['filename'].str.replace('.abf', '_redstim.csv', regex=False).to_list()\n",
    "peak_filenames = log_data['filename'].str.replace('.abf', '_peak.csv', regex=False).to_list()\n",
    "red_data = read_csvs_in_directory(red_filenames, log_data)\n",
    "peak_data = read_csvs_in_directory(peak_filenames, log_data) #結局使ってない?\n",
    "\n",
    "mean_red_data = red_data.groupby(['filename', 'SliceID', 'CellID', 'Time (seconds)']).mean()\n",
    "mean_red_data = mean_red_data.reset_index()\n",
    "\n",
    "log_data_for_merge = log_data\n",
    "log_data_for_merge['filename'] = log_data_for_merge['filename'].str.replace('.abf', '')\n",
    "# Filter rows where 'Time (seconds)' is between 1 ~ 2\n",
    "filtered_df = mean_red_data[(mean_red_data['Time (seconds)'] >= 1) & \n",
    "                        (mean_red_data['Time (seconds)'] <= 1.25)]\n",
    "\n",
    "# Find the maximum absolute value of 'Im_scaled' for each filename within this period\n",
    "peak_values_df = filtered_df.groupby('filename').apply(lambda x: x.loc[x['Im_scaled'].abs().idxmax()])\n",
    "peak_values_df = peak_values_df.reset_index(drop=True)\n",
    "peak_values_df_log = pd.merge(peak_values_df, log_data_for_merge, on = ['filename', 'SliceID', 'CellID'])\n",
    "peak_values_df_log.to_csv(os.path.join(csv_directory, 'Peak_data_red.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b49777-7d6a-4199-84f9-aaa364be4067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column for the absolute value of 'Im_scaled'\n",
    "peak_values_df_log['Im_scaled_abs'] = peak_values_df_log['Im_scaled'].abs()\n",
    "\n",
    "# Group by 'SliceID', 'CellID', and 'extracted_log' and separate the data\n",
    "grouped_data = peak_values_df_log.groupby(['SliceID', 'CellID', 'extracted_log'])\n",
    "\n",
    "# Creating a list of dataframes for each group\n",
    "separated_data = {name: group for name, group in grouped_data}\n",
    "separated_data = {name: group for name, group in separated_data.items() if len(group) > 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9acd2c-a6f5-4930-847a-71070115332b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct the naming issue by properly handling the tuple\n",
    "for group_name, group_data in separated_data.items():\n",
    "    # Determine colors based on 'log' containing 'PSEM'\n",
    "    colors = ['red' if 'PSEM' in log else 'blue' for log in group_data['log']]\n",
    "    \n",
    "    group_name_str = \"_\".join(group_name)  # Convert tuple to string for naming the file\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(group_data['filename'], group_data['Im_scaled_abs'], color = colors)\n",
    "    plt.xlabel('Filename')\n",
    "    plt.ylabel('Im_scaled (Absolute)')\n",
    "    plt.title(f'Im_scaled (Absolute) vs Filename for Group: {group_name}')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
