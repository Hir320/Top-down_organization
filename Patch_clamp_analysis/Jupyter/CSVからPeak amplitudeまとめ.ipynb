{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a4d26d-7571-4f18-9f64-829f1cdea389",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext cudf.pandas\n",
    "import sys; sys.path.append('./')\n",
    "import log_search\n",
    "import pandas as pd\n",
    "from scipy.signal import find_peaks\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "import math\n",
    "\n",
    "def filter_by_interval_time(df):\n",
    "    # Drop rows with NaN values in the 'log' column\n",
    "    df = df.dropna(subset=['log'])\n",
    "    \n",
    "    # Regular expression to match \"IT_XXX ms\"\n",
    "    pattern = re.compile(r'IT_\\d+ ms')\n",
    "    \n",
    "    # Filter the DataFrame\n",
    "    filtered_df = df[df['log'].str.contains(pattern)]\n",
    "    \n",
    "    return filtered_df\n",
    "    \n",
    "def read_csvs_in_directory(filenames, log_data):\n",
    "    # Dictionary to store dataframes\n",
    "    dfs = []\n",
    "    for filename in filenames:\n",
    "        abffilename = filename[0:8] + '.abf'\n",
    "        # Filter the dataframe for the target filename\n",
    "        target_row = log_data[log_data['filename'] == abffilename]\n",
    "        \n",
    "        # Extract the log entry for the target row\n",
    "        log_entry = target_row.iloc[0]['log']\n",
    "        slice_id = target_row.iloc[0]['SliceID']\n",
    "        cell_id = target_row.iloc[0]['CellID']\n",
    "        \n",
    "        # Use regular expression to find \"IT_XXX ms\"\n",
    "        match = re.search(r'IT_(\\d+) ms', log_entry)\n",
    "        inter_stimulus_interval_ms = int(match.group(1))\n",
    "        inter_stimulus_interval_s = inter_stimulus_interval_ms / 1000.0\n",
    "        \n",
    "        csv_file = os.path.join(csv_directory, filename)\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            df['filename'] = filename[0:8]\n",
    "            df['interval'] = inter_stimulus_interval_s\n",
    "            df['Slice ID'] = slice_id\n",
    "            df['Cell ID'] = cell_id\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    dfs = pd.concat(dfs, axis=0, sort=True)\n",
    "    return dfs\n",
    "\n",
    "def find_peaks_in_file(data, column_for_peaks, peak_times_df, plots_per_sheet=12):\n",
    "    # Define the parameters\n",
    "    min_peak_height = 0.1  # You can adjust this based on your data if needed\n",
    "    \n",
    "    # Initialize a list to store the peak data\n",
    "    peak_data = []\n",
    "\n",
    "    # Check if required columns exist\n",
    "    if column_for_peaks not in data.columns:\n",
    "        print(\"Available columns:\", data.columns)\n",
    "        raise KeyError(f\"'{column_for_peaks}' column not found in the dataframe\")\n",
    "\n",
    "    # Prepare for plotting\n",
    "    num_groups = len(data.groupby(['filename', 'Slice ID', 'Cell ID', 'interval']))\n",
    "    num_sheets = math.ceil(num_groups / plots_per_sheet)\n",
    "\n",
    "    group_list = list(data.groupby(['filename', 'Slice ID', 'Cell ID', 'interval']))\n",
    "    \n",
    "    directory = os.path.dirname(directory_log_filename)\n",
    "    output_directory = os.path.join(directory, 'output_sheets')\n",
    "    # Save the current sheet as a JPG file\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(out_put_directory)\n",
    "        print(f\"Directory '{output_directory}' created.\")\n",
    "    else:\n",
    "        print(f\"Directory '{output_directory}' already exists.\")\n",
    "    \n",
    "    if 'Combined' in column_for_peaks:\n",
    "        output_path = os.path.join(output_directory, 'combined_peaks')\n",
    "    else:\n",
    "        output_path = os.path.join(output_directory, 'first_peaks')\n",
    "\n",
    "    for sheet_idx in range(num_sheets):\n",
    "        start_idx = sheet_idx * plots_per_sheet\n",
    "        end_idx = min(start_idx + plots_per_sheet, num_groups)\n",
    "        num_plots = end_idx - start_idx\n",
    "        num_cols = 2  # Number of columns of subplots\n",
    "        num_rows = math.ceil(num_plots / num_cols)  # Number of rows of subplots\n",
    "\n",
    "        fig, axes = plt.subplots(num_rows, num_cols, figsize=(8.27, 11.69))  # A4 size in inches\n",
    "        axes = axes.flatten()  # Flatten the axes array for easy iteration\n",
    "\n",
    "        # Iterate over each group in the current sheet\n",
    "        for i in range(start_idx, end_idx):\n",
    "            (filename, slice_id, cell_id, interval), group = group_list[i]\n",
    "            # Reset the index to properly handle the time points\n",
    "            group = group.reset_index()\n",
    "            time_points = group['Time (seconds)'].values\n",
    "            data_values = group[column_for_peaks].values\n",
    "            sweep = group['Sweep'].iloc[0]\n",
    "            #print(f'This is {filename}, and the interval is {interval}.')\n",
    "        \n",
    "            # Find the First Peak Time from the second CSV\n",
    "            #print(peak_times_df)\n",
    "            first_stim_time = peak_times_df[\n",
    "                (peak_times_df['filename'] == filename) & \n",
    "                (peak_times_df['Slice ID'] == slice_id) & \n",
    "                (peak_times_df['Cell ID'] == cell_id) &\n",
    "                (peak_times_df['ADC Name'] == 'Stim3')\n",
    "            ]['First Peak Time'].values[0]\n",
    "            #print(f'First peak is {first_stim_time}')\n",
    "\n",
    "            second_stim_time = peak_times_df[\n",
    "                (peak_times_df['filename'] == filename) & \n",
    "                (peak_times_df['Slice ID'] == slice_id) & \n",
    "                (peak_times_df['Cell ID'] == cell_id) &\n",
    "                (peak_times_df['ADC Name'] == 'Stim_2')\n",
    "            ]['First Peak Time'].values[0]\n",
    "            #print(f'Second peak is {second_stim_time}')\n",
    "            \n",
    "            # Narrow the time window to between first and second peak time\n",
    "            first_time_window_start = 1\n",
    "            first_time_window_end = 1 + interval\n",
    "            second_time_window_start = 1 + interval\n",
    "            second_time_window_end = 1 + 1\n",
    "            \n",
    "            # Filter the group data based on the time window\n",
    "            first_mask = (time_points >= first_time_window_start) & (time_points <= first_time_window_end)\n",
    "            first_filtered_time_points = time_points[first_mask]\n",
    "            first_filtered_data_values = data_values[first_mask]\n",
    "            \n",
    "            second_mask = (time_points >= second_time_window_start) & (time_points <= second_time_window_end)\n",
    "            second_filtered_time_points = time_points[second_mask]\n",
    "            second_filtered_data_values = data_values[second_mask]\n",
    "            \n",
    "            # Find peaks within the narrowed time window\n",
    "            first_peaks, first_properties = find_peaks(first_filtered_data_values, height=min_peak_height, prominence=0.1)\n",
    "            first_peak = first_peaks[0] if len(first_peaks) > 0 else None\n",
    "            \n",
    "            second_peaks, second_properties = find_peaks(second_filtered_data_values, height=min_peak_height, prominence=0.1)\n",
    "            second_peak = second_peaks[0] if len(second_peaks) > 0 else None\n",
    "            \n",
    "            peak_values = [first_filtered_data_values[first_peak] if first_peak is not None else None,\n",
    "                           second_filtered_data_values[second_peak] if second_peak is not None else None]\n",
    "            peak_times = [first_filtered_time_points[first_peak] if first_peak is not None else None,\n",
    "                          second_filtered_time_points[second_peak] if second_peak is not None else None]\n",
    "            peak_data.append((filename, slice_id, cell_id, interval, sweep, peak_values[0], \n",
    "                              peak_times[0], peak_values[1], peak_times[1]))\n",
    "            \n",
    "            # Plot the data and the detected peaks\n",
    "            ax = axes[i - start_idx]\n",
    "            ax.plot(time_points, data_values, label='Vm_scaled')\n",
    "            if first_peak is not None:\n",
    "                ax.plot(first_filtered_time_points[first_peak], first_filtered_data_values[first_peak], 'ro', label='First Peak')\n",
    "            if second_peak is not None:\n",
    "                ax.plot(second_filtered_time_points[second_peak], second_filtered_data_values[second_peak], 'go', label='Second Peak')\n",
    "            ax.set_xlabel('Time (seconds)')\n",
    "            ax.set_ylabel(column_for_peaks)\n",
    "            ax.set_title(f'{filename} (Slice: {slice_id}, Cell: {cell_id})')\n",
    "            ax.legend(fontsize='xx-small', loc='best')\n",
    "            ax.set_xlim(0.5, 2)\n",
    "\n",
    "        # Adjust layout and remove empty subplots\n",
    "        for j in range(num_plots, len(axes)):\n",
    "            fig.delaxes(axes[j])\n",
    "        fig.tight_layout()\n",
    "\n",
    "        plt.savefig(f'{output_path}_{sheet_idx + 1}.jpg', format='jpg', dpi=300)\n",
    "        plt.close(fig)\n",
    "\n",
    "    # Convert the peak data to a DataFrame for better visualization\n",
    "    peak_df = pd.DataFrame(peak_data, columns=['filename', 'Slice ID', 'Cell ID', 'Interval', 'Sweep', \n",
    "                                               'First Peak Value', 'First Peak Time', 'Second Peak Value', 'Second Peak Time'])\n",
    "    return peak_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c772caa1-002e-4c78-afa2-038746f0f517",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Load the CSV file\n",
    "log_filename = '2025.XX.XX.csv'\n",
    "# Find the file recursively\n",
    "directory_log_filename = log_search.find_file_recursively(\"./Data\", log_filename)\n",
    "out_put_directory = os.path.join(os.path.dirname(directory_log_filename), 'output_sheets')\n",
    "log_data = pd.read_csv(directory_log_filename)\n",
    "# Apply the function\n",
    "filtered_log = filter_by_interval_time(log_data)\n",
    "csv_directory = os.path.dirname(directory_log_filename)\n",
    "\n",
    "# Extract the filenames without extensions\n",
    "combined_filenames = filtered_log['filename'].str.replace('.abf', '_combined.csv', regex=False).to_list()\n",
    "first_filenames = filtered_log['filename'].str.replace('.abf', '_firststim.csv', regex=False).to_list()\n",
    "peak_filenames = filtered_log['filename'].str.replace('.abf', '_peak.csv', regex=False).to_list()\n",
    "\n",
    "\n",
    "combined_data = read_csvs_in_directory(combined_filenames, log_data)\n",
    "first_data = read_csvs_in_directory(first_filenames, log_data)\n",
    "peak_data = read_csvs_in_directory(peak_filenames, log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a2d3a1-a0be-45be-beb9-34e442eb919b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_combined_data = combined_data.groupby(['filename', 'interval', 'Time (seconds)', 'Slice ID', 'Cell ID']).mean()\n",
    "mean_first_data = first_data.groupby(['filename', 'interval', 'Time (seconds)', 'Slice ID', 'Cell ID']).mean().reset_index()\n",
    "mean_peak_data = peak_data.groupby(['filename', 'ADC Name', 'interval', 'Slice ID', 'Cell ID']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81acbd40-e621-423a-b31e-f84083f66525",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2438c9-ce2a-4651-9696-06f14aa198f2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Use the function to find peaks\n",
    "first_peak = find_peaks_in_file(mean_first_data, 'Vm_scaled', peak_data)\n",
    "combined_peak = find_peaks_in_file(mean_combined_data, 'Combined Vm_scaled', peak_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6b0147-aa60-4637-a14f-cd55b1bd5c34",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "first_peak = first_peak.rename(columns={\n",
    "    'First Peak Value': 'First Peak Value Combined',\n",
    "    'First Peak Time': 'First Peak Time Combined',\n",
    "    'Second Peak Value': 'Actual Peak Value',\n",
    "    'Second Peak Time': 'Actual Peak Time Combined'\n",
    "})\n",
    "combined_peak = combined_peak.rename(columns={\n",
    "    'First Peak Value': 'Third Peak Value Combined',\n",
    "    'First Peak Time': 'Third Peak Time Combined',\n",
    "    'Second Peak Value': 'Estimated Peak Value',\n",
    "    'Second Peak Time': 'Estimated Peak Time Combined'\n",
    "})\n",
    "# Merge the data frames on the shared columns\n",
    "merged_peak = pd.merge(first_peak, combined_peak, on=['filename', 'Slice ID', 'Cell ID', 'Interval'], how='inner')\n",
    "log_data['filename'] = log_data['filename'].str.replace('.abf', '')\n",
    "merged_peak = pd.merge(merged_peak, log_data, on = ['filename'])\n",
    "#merged_peak['contains_ptx'] = merged_peak['log'].apply(lambda x: 'PTX' in str(x))\n",
    "merged_peak.to_csv(os.path.join(csv_directory, 'merged_mean_peak.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbe6a01-e1a9-4f6d-a24a-326d8e550183",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4cbcf5-fdbd-4e18-9319-7df755c0b262",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
