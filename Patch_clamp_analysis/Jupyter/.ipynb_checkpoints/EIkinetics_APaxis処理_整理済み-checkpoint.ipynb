{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b33c764f",
   "metadata": {},
   "source": [
    "# データの整理と前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99d921e-7ec5-4066-bc7a-f462467c9413",
   "metadata": {},
   "source": [
    "### csv (YYYY.MM.DD.csv)をまとめて、master.csvをつくる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710399b6-c932-45de-a391-4cc07b448060",
   "metadata": {},
   "source": [
    "#### 関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811aefbe-bfe1-4527-b1e8-e3526bab9449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def clean_log_string(raw_str):\n",
    "    \"\"\"\n",
    "    Remove extra RTF/encoding artifacts like '\\lang1033', '\\f4', etc.\n",
    "    \"\"\"\n",
    "    # 1) Remove sequences like \"\\lang1234\" or \"\\f12\"\n",
    "    cleaned = re.sub(r'\\\\lang\\d+|\\\\f\\d+', '', raw_str)\n",
    "    # 2) Collapse multiple whitespaces\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "    # 3) Trim leading/trailing whitespace\n",
    "    cleaned = cleaned.strip()\n",
    "    return cleaned\n",
    "\n",
    "def parse_log_string(log_str):\n",
    "    \"\"\"\n",
    "    Parse relevant fields from a single trial's log string.\n",
    "    This function is just an example. Adjust as needed!\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"VoltageHold\": None,   # e.g., -55, +10\n",
    "        \"Color\": None,         # \"blue\" or \"red\"\n",
    "        \"DrugList\": [],\n",
    "        \"StimPower\": None,     # numeric or maybe \"100%\" as string\n",
    "        \"StimDuration\": None,  # numeric (milliseconds)\n",
    "    }\n",
    "    \n",
    "    # --- 1) Voltage Hold: e.g. \"-55\" or \"+10\" ---\n",
    "    match_v = re.search(r'([+-]\\d+)', log_str)\n",
    "    if match_v:\n",
    "        results[\"VoltageHold\"] = match_v.group(1)\n",
    "    \n",
    "    # --- 2) Color (B_ => blue, R_ => red) ---\n",
    "    if \"B_\" in log_str:\n",
    "        results[\"Color\"] = \"blue\"\n",
    "    elif \"R_\" in log_str:\n",
    "        results[\"Color\"] = \"red\"\n",
    "    \n",
    "    # --- 3) Drug List (simple detection) ---\n",
    "    known_drugs = [\"NBQX\", \"AP-V\", \"PTX\"]\n",
    "    found_drugs = []\n",
    "    for drug in known_drugs:\n",
    "        if drug in log_str:\n",
    "            # Try capturing \"NBQX 10 uM\" etc.\n",
    "            match_drug = re.search(rf\"({drug}\\s*\\S+)\", log_str)\n",
    "            if match_drug:\n",
    "                found_drugs.append(match_drug.group(1))\n",
    "            else:\n",
    "                found_drugs.append(drug)\n",
    "    # Convert to semicolon-delimited string\n",
    "    results[\"DrugList\"] = \";\".join(found_drugs) if found_drugs else \"\"\n",
    "    \n",
    "    # --- 4) Stimulus Power & Duration ---\n",
    "    # Example pattern from your snippet (for \"B_1 A 4.2 0.13 ms sweep 20 s\"):\n",
    "    #    B_1\\sA\\s+([\\d\\.%]+)\\s+([\\d\\.]+)\\s+ms\\s+sweep\\s+(\\d+)\\s+s\n",
    "    # Adjust if your real logs differ.\n",
    "    stim_pattern = re.compile(r\"(R|B)_1\\sA\\s+([\\d.%]+)\\s+([\\d.]+)\\s*ms\\s+(?:IT_(\\d{1,3})\\s+ms\\s+)?sweep\\s+(\\d+)\\s*s\")\n",
    "    m = stim_pattern.search(log_str)\n",
    "    if m:\n",
    "        # Group(1): 'R' or 'B'\n",
    "        color_code = m.group(1)  \n",
    "        # Convert to text if you want\n",
    "        color_str = \"red\" if color_code == \"R\" else \"blue\"\n",
    "\n",
    "        power_str    = m.group(2)  # e.g. '4.2', '100%', '5'\n",
    "        duration_str = m.group(3)  # e.g. '0.13', '1', '2.0'\n",
    "        it_str       = m.group(4)  # e.g. '50' or '999' if present; else None\n",
    "        sweep_str    = m.group(5)  # e.g. '20', '10', '5', '15'\n",
    "\n",
    "        # Convert power\n",
    "        if power_str.endswith('%'):\n",
    "            # handle the \"100%\" case\n",
    "            # you might store it as float(100.0) or just keep as string \"100%\"\n",
    "            try:\n",
    "                power_float = float(power_str.replace('%',''))\n",
    "                results[\"StimPower\"] = power_float\n",
    "            except ValueError:\n",
    "                # fallback: store raw string\n",
    "                results[\"StimPower\"] = power_str\n",
    "        else:\n",
    "            try:\n",
    "                results[\"StimPower\"] = float(power_str)\n",
    "            except ValueError:\n",
    "                results[\"StimPower\"] = power_str\n",
    "        \n",
    "        # Convert duration\n",
    "        try:\n",
    "            results[\"StimDuration\"] = float(duration_str)\n",
    "        except ValueError:\n",
    "            results[\"StimDuration\"] = duration_str\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def parse_single_brain_log(log_csv_path, opsin, region, brain_id):\n",
    "    \"\"\"\n",
    "    Reads the log CSV named something like '2024.10.12.csv'.\n",
    "    Returns a DataFrame with one row per trial, plus parsed columns.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(log_csv_path)\n",
    "    \n",
    "    # Guarantee columns exist or create placeholders\n",
    "    # Adjust depending on your real CSV columns\n",
    "    if \"SliceID\" not in df.columns:\n",
    "        df[\"SliceID\"] = None\n",
    "    if \"CellID\" not in df.columns:\n",
    "        df[\"CellID\"] = None\n",
    "    if \"filename\" not in df.columns:\n",
    "        df[\"filename\"] = None\n",
    "    if \"log\" not in df.columns:\n",
    "        df[\"log\"] = \"\"\n",
    "    \n",
    "    # Add high-level columns\n",
    "    df[\"Opsin\"] = opsin\n",
    "    df[\"Region\"] = region\n",
    "    df[\"BrainID\"] = brain_id\n",
    "    \n",
    "    # Parse each row's \"log\" text:\n",
    "    # Coerce `log` to a string to avoid TypeError\n",
    "    df[\"log\"] = df[\"log\"].fillna(\"\").astype(str)\n",
    "    # 1) Clean up each log string\n",
    "    df[\"Cleaned_Log\"] = df[\"log\"].apply(clean_log_string)\n",
    "    parse_results = df[\"Cleaned_Log\"].apply(lambda txt: parse_log_string(str(txt)))\n",
    "    \n",
    "    df[\"VoltageHold\"]  = parse_results.apply(lambda d: d[\"VoltageHold\"])\n",
    "    df[\"Color\"]        = parse_results.apply(lambda d: d[\"Color\"])\n",
    "    df[\"DrugList\"]     = parse_results.apply(lambda d: d[\"DrugList\"])\n",
    "    df[\"StimPower\"]    = parse_results.apply(lambda d: d[\"StimPower\"])\n",
    "    df[\"StimDuration\"] = parse_results.apply(lambda d: d[\"StimDuration\"])\n",
    "    \n",
    "    # Clean up SliceID/CellID if needed\n",
    "    df[\"SliceID\"] = df[\"SliceID\"].astype(str).str.replace(r\"[^A-Za-z0-9]+\", \"\", regex=True)\n",
    "    df[\"CellID\"]  = df[\"CellID\"].astype(str).str.replace(r\"[^A-Za-z0-9]+\", \"\", regex=True)\n",
    "\n",
    "    # Reorder or subset columns for final\n",
    "    final_cols = [\n",
    "        \"Opsin\",\n",
    "        \"Region\",\n",
    "        \"BrainID\",\n",
    "        \"SliceID\",\n",
    "        \"CellID\",\n",
    "        \"VoltageHold\",\n",
    "        \"Color\",\n",
    "        \"StimPower\",\n",
    "        \"StimDuration\",\n",
    "        \"DrugList\",\n",
    "        \"filename\",\n",
    "        # anything else from the original CSV you want to keep\n",
    "    ]\n",
    "    # Keep only columns that exist\n",
    "    final_cols = [c for c in final_cols if c in df.columns]\n",
    "    \n",
    "    return df[final_cols]\n",
    "\n",
    "\n",
    "def collect_all_brains(data_root, out_csv_name):\n",
    "    \"\"\"\n",
    "    Walk through `data_root`, find each directory that has exactly one\n",
    "    'year.month.date.csv' file, parse it, and append to a master DataFrame.\n",
    "    Then save one CSV with one row = one trial (across all brains).\n",
    "    \"\"\"\n",
    "    data_root = Path(data_root)\n",
    "    all_rows = []\n",
    "    \n",
    "    # A pattern to match files like 2024.10.12.csv etc.\n",
    "    # Adjust if your real log file name has a different pattern.\n",
    "    csv_pattern = re.compile(r'^\\d{4}\\.\\d{2}\\.\\d{2}\\.csv$')\n",
    "\n",
    "    # Walk all subfolders\n",
    "    for folder in data_root.rglob(\"*\"):\n",
    "        if folder.is_dir():\n",
    "            # Find exactly one CSV that matches year.month.date.csv\n",
    "            possible_csvs = [f for f in folder.iterdir() \n",
    "                             if f.is_file() and csv_pattern.match(f.name)]\n",
    "            if len(possible_csvs) == 1:\n",
    "                log_csv_path = possible_csvs[0]\n",
    "                \n",
    "                # Heuristics to get opsin / region / brain_id from folder path\n",
    "                # Example folder structure:\n",
    "                #   data_root / ChR2 / ACC / 241012_ID2 / 2024.10.12.csv\n",
    "                # Typically, you can parse:\n",
    "                #   opsin = \"ChR2\"\n",
    "                #   region = \"ACC\"\n",
    "                #   brain_id = \"241012_ID2\"\n",
    "                parts = log_csv_path.relative_to(data_root).parts\n",
    "                # parts might be: (\"ChR2\", \"ACC\", \"241012_ID2\", \"2024.10.12.csv\")\n",
    "                \n",
    "                # You could pick them out carefully:\n",
    "                opsin = None\n",
    "                region = None\n",
    "                brain_id = None\n",
    "                \n",
    "                if len(parts) >= 1:\n",
    "                    opsin = parts[0]  # e.g. \"ChR2\"\n",
    "                if len(parts) >= 2:\n",
    "                    region = parts[1]  # e.g. \"ACC\"\n",
    "                if len(parts) >= 3:\n",
    "                    brain_id = parts[2]  # e.g. \"241012_ID2\"\n",
    "                \n",
    "                # Parse that CSV\n",
    "                df_single = parse_single_brain_log(\n",
    "                    log_csv_path=log_csv_path,\n",
    "                    opsin=opsin,\n",
    "                    region=region,\n",
    "                    brain_id=brain_id\n",
    "                )\n",
    "                \n",
    "                all_rows.append(df_single)\n",
    "\n",
    "    # Concatenate all brains\n",
    "    if all_rows:\n",
    "        master_df = pd.concat(all_rows, ignore_index=True)\n",
    "    else:\n",
    "        # No data found\n",
    "        master_df = pd.DataFrame()\n",
    "\n",
    "    # Save the big CSV\n",
    "    out_csv_path = os.path.join(data_root, out_csv_name)\n",
    "    master_df.to_csv(out_csv_path, index=False)\n",
    "    print(f\"Master CSV written to: {out_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc4c2c6-1ed9-430b-8fa6-47d9762b9f45",
   "metadata": {},
   "source": [
    "#### `master.csv`保存 (log csvをまとめる)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365dfcf3-6277-45ff-8c0f-84f26335a2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"./sorted_directory/\"\n",
    "out_csv_name = \"master.csv\"\n",
    "\n",
    "collect_all_brains(data_root, out_csv_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7c9257-36b0-4e50-a699-e7d457f4075b",
   "metadata": {},
   "source": [
    "ここで、master.csvを見ながらどの画像をチェックすべきか確認し、DAPIとPaxinos and Franklin atlasを対応させ、下のような表をexcelもしくはnumbersでつくり、csvにしてrootディレクトリ(sortex_directoryなど)にアップする。\n",
    "\n",
    "\n",
    "| BrainID       | SliceID | APregion  | RoughAP |\n",
    "|---------------|---------|-----------|---------|\n",
    "| 241220_ID13   | Slice1  | anterior  | -2.5    |\n",
    "| 241215_ID12   | Slice1  | posterior | -4.2    |\n",
    "| 241215_ID12   | Slice2  | middle    | -3.4    |\n",
    "| 241210_ID10   | Slice1  | posterior | -4.3    |\n",
    "| 241210_ID10   | Slice2  | middle    | -3.9    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934957a6",
   "metadata": {},
   "source": [
    "# master.csvとAPregion.csvの統合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a96dfd-10ab-4bb1-a7b8-ba248148ad26",
   "metadata": {},
   "source": [
    "## `master_with_AP.csv`保存(`master.csv`とA`Pregion.csv`をmerge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0fdd42-413e-4f86-ace9-717e7aa0f530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your main dataset\n",
    "df_master = pd.read_csv(\"./sorted_directory/master.csv\")\n",
    "\n",
    "# Load your AP region info\n",
    "df_ap = pd.read_csv(\"./sorted_directory/APregion.csv\")\n",
    "\n",
    "# Merge (left join) on BrainID & SliceID\n",
    "df_merged = pd.merge(\n",
    "    df_master,\n",
    "    df_ap,\n",
    "    on=[\"BrainID\", \"SliceID\"], \n",
    "    how=\"left\"  # So you keep all rows from master.csv\n",
    ")\n",
    "\n",
    "# Now df_merged will have extra columns: APregion, RoughAP\n",
    "# Save it out:\n",
    "df_merged.to_csv(\"./sorted_directory/master_with_AP.csv\", index=False)\n",
    "print(\"Merged CSV saved as master_with_AP.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77260f76",
   "metadata": {},
   "source": [
    "# ABFファイルのフィルタリング"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb26e73-fc34-4796-845b-92f3ab7c67dc",
   "metadata": {},
   "source": [
    "## 処理するabfファイルを選ぶ\n",
    "\n",
    "以下のコードは、ある DataFrame（`df_merged`）に対して複数のフィルタ条件を連結し、最終的に条件に合った行だけを取り出す処理を行っています。具体的には、**Opsin**（例: `\"ChR2\"` など）、**VoltageHold**（例: `-55.0`, `10.0` など）、および **DrugList**（薬剤条件）によるフィルタを組み合わせたものです。\n",
    "\n",
    "---\n",
    "\n",
    "### 全体の流れ\n",
    "\n",
    "1. **DataFrame の読み込み**  \n",
    "   `df_merged` がまだ存在しない場合、`\"./sorted_directory/master_with_AP.csv\"` から読み込みます。  \n",
    "   ```python\n",
    "   if df_merged is None:\n",
    "       df_merged = pd.read_csv(\"./sorted_directory/master_with_AP.csv\")\n",
    "    ```\n",
    "       \n",
    "2. **Opsin フィルタ**  \n",
    "   `opsin_choices = [\"ChR2\"]` のように設定されている場合、  \n",
    "   ```python\n",
    "   opsin_filter = df_merged[\"Opsin\"].isin(opsin_choices)\n",
    "   ```\n",
    "   \n",
    "   によって、`Opsin` 列が `\"ChR2\"` の行だけを `True` とするブール配列が得られます。\n",
    "\n",
    "3. **VoltageHold フィルタ**\n",
    "\n",
    "    たとえば `hold_choices = [-55.0, 10.0]` と指定した場合:\n",
    "\n",
    "    ```python\n",
    "    hold_filter = df_merged[\"VoltageHold\"].isin(hold_choices)\n",
    "    ```\n",
    "    このコードによって、`VoltageHold` 列の値が `-55.0` または `10.0` の行を `True` とし、それ以外の行は `False` とするブール配列が得られます。\n",
    "\n",
    "4. **DrugList フィルタ** \n",
    "\n",
    "   ユーザーが選んだ複数の薬剤条件 `(drug_choices)` に合わせて、`DrugList` 列をチェックします。\n",
    "    例えば、`None` なら `DrugList` が `NaN` または空文字の場合を拾い、\n",
    "    文字列なら\n",
    "    ```python\n",
    "    df_merged[\"DrugList\"].str.contains(...) \n",
    "    ```\n",
    "    で部分一致をチェックします。\n",
    "    これら複数の条件を `OR (|)`で連結したブール配列を最終的に `drug_filter` とします。\n",
    "\n",
    "5. **AND 結合**\n",
    "\n",
    "    Opsin フィルタ、VoltageHold フィルタ、DrugList フィルタをすべて満たす行を取り出すには、\n",
    "\n",
    "   ```python\n",
    "   final_filter = opsin_filter & hold_filter & drug_filter\n",
    "    ```\n",
    "   のように AND 結合を行います。\n",
    "\n",
    "6. **結果の適用・保存**\n",
    "\n",
    "\n",
    "   最後に、\n",
    "\n",
    "   ```python\n",
    "    df_filtered = df_merged[final_filter]\n",
    "    df_filtered.to_csv(\"./sorted_directory/master_with_AP_filtered.csv\", index=False)\n",
    "\n",
    "    ```\n",
    "   とすれば、フィルタを通った行だけが `df_filtered` に入り、それを CSV として保存することもできます。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37122dd9-71f5-4447-a03b-49b5645fbcf4",
   "metadata": {},
   "source": [
    "### `master_with_AP_filtered.csv`保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63421a2f-da9a-4a69-9d49-2c529f377f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_merged is None:\n",
    "    df_merged = pd.read_csv(\"./sorted_directory/master_with_AP.csv\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Filters\n",
    "opsin_choices = [\"Dual_Injection\"] # Opsin filter \"ChR2\", \"Dual_Injection\", \"GtCCR4\"\n",
    "hold_choices = [-55.0, 10.0] # VoltageHold filter\n",
    "drug_choices = [None]  # DrugList filter (includes the blank)\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# 1) Filter by Opsin\n",
    "opsin_filter = df_merged[\"Opsin\"].isin(opsin_choices)\n",
    "\n",
    "# 2) Filter by VoltageHold\n",
    "hold_filter = df_merged[\"VoltageHold\"].isin(hold_choices)\n",
    "\n",
    "# 3) Build the drug filter (which can be multiple OR conditions)\n",
    "drug_conditions = []\n",
    "for choice in drug_choices:\n",
    "    if choice is None:\n",
    "        # \"select rows where DrugList is NaN or empty\"\n",
    "        drug_conditions.append(df_merged[\"DrugList\"].isna() | (df_merged[\"DrugList\"] == \"\"))\n",
    "    else:\n",
    "        # \"DrugList contains the chosen substring\"\n",
    "        drug_conditions.append(df_merged[\"DrugList\"].str.contains(choice, na=False))\n",
    "\n",
    "# Combine all drug conditions with OR\n",
    "if len(drug_conditions) > 0:\n",
    "    drug_filter = drug_conditions[0]\n",
    "    for c in drug_conditions[1:]:\n",
    "        drug_filter = drug_filter | c\n",
    "else:\n",
    "    # If, for some reason, no drug choices exist, default to \"True\" \n",
    "    # (meaning no restriction on DrugList)\n",
    "    drug_filter = True\n",
    "\n",
    "# 4) Combine *all* filters with AND\n",
    "final_filter = opsin_filter & hold_filter & drug_filter\n",
    "\n",
    "# 5) Apply to the DataFrame\n",
    "df_filtered = df_merged[final_filter]\n",
    "df_filtered.to_csv(\"./sorted_directory/master_with_AP_filtered.csv\", index=False)\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af55448d-2e5e-40c4-b337-14084e6b4f2b",
   "metadata": {},
   "source": [
    "# df_filteredに含まれるファイルを解析する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf35c774-471c-48d9-a32e-169ab9ba82ab",
   "metadata": {},
   "source": [
    "## 関数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7356ecae-bbbb-4d26-acc7-2897f3715406",
   "metadata": {},
   "source": [
    "### find_abf_files関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f67523-0d7e-4874-9ed4-41345655cf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def find_abf_files(root_dir):\n",
    "    \"\"\"\n",
    "    Recursively search for all *.abf files under `root_dir`.\n",
    "    Return a dictionary mapping the short filename (e.g., '24n15005.abf')\n",
    "    -> the absolute path to that file.\n",
    "    \"\"\"\n",
    "    abf_dict = {}\n",
    "    for root, dirs, files in os.walk(root_dir):\n",
    "        for fname in files:\n",
    "            if fname.lower().endswith(\".abf\"):\n",
    "                full_path = os.path.join(root, fname)\n",
    "                abf_dict[fname] = full_path\n",
    "    return abf_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8872a6f-cc11-4152-a7d8-3c117ea742d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyabf\n",
    "from pyabf.tools.memtest import Memtest\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "###########################\n",
    "# Bi-exponential function #\n",
    "###########################\n",
    "def biexponential_fit(t, m, tau, n, c):\n",
    "    \"\"\"\n",
    "    A 2-term exponential decay function with a forced ratio for tau_1 and tau_2.\n",
    "    For the fit, t is in seconds, and:\n",
    "      - tau_1 = 0.1 * tau\n",
    "      - tau_2 = 0.9 * tau\n",
    "    \"\"\"\n",
    "    tau_1 = 0.1 * tau\n",
    "    tau_2 = 0.9 * tau\n",
    "    return m * np.exp(-t / tau_1) + n * np.exp(-t / tau_2) + c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc5d445-9400-4997-a8fc-9a25c550cfbe",
   "metadata": {},
   "source": [
    "### `find_stim_time_digital`関数\n",
    "\n",
    "この関数は、指定されたスイープにおいて、選択したデジタル出力チャンネル（redまたはblue）が0から1に遷移する最初の時刻を秒単位で返します。\n",
    "\n",
    "---\n",
    "\n",
    "##### 引数:\n",
    "- **`abf`**: `pyabf.ABF`オブジェクト。ABFファイルを操作するためのオブジェクトです。\n",
    "- **`sweep_index`**: `int`型。解析対象のスイープインデックスを指定します。\n",
    "- **`color`**: `str`型。デジタル出力チャンネルを指定します。`\"red\"`または`\"blue\"`を選択可能です。\n",
    "\n",
    "---\n",
    "\n",
    "##### 戻り値:\n",
    "- **`float`**: デジタル信号が1に変化する最初のサンプルの時間（秒単位）。\n",
    "- **`None`**: 該当するデジタル信号が見つからなかった場合。\n",
    "\n",
    "---\n",
    "\n",
    "##### 処理の流れ:\n",
    "1. **デジタル出力チャンネルの選択**:\n",
    "   - `color`が`\"red\"`の場合、チャンネル番号は`0`。\n",
    "   - `color`が`\"blue\"`の場合、チャンネル番号は`3`。\n",
    "   - その他の値の場合は、`None`を返して終了。\n",
    "\n",
    "2. **スイープデータのロード**:\n",
    "   - 指定された`abf`オブジェクトのスイープデータを`abf.setSweep`でロードします。\n",
    "   - 時間データ（秒単位）は`abf.sweepX`に、デジタル信号は`abf.sweepD(digOutNum)`に格納されます。\n",
    "\n",
    "3. **デジタル信号の遷移検出**:\n",
    "   - デジタル信号が`1`になるインデックス（`idxs`）を`np.where`で検索します。\n",
    "   - 該当するインデックスがない場合、`None`を返します。\n",
    "\n",
    "4. **最初の遷移時刻を返す**:\n",
    "   - 最初のインデックス`idx_stim`に対応する時間を`time_s[idx_stim]`として返します。\n",
    "\n",
    "---\n",
    "\n",
    "##### サンプルコード:\n",
    "```python\n",
    "stim_time = find_stim_time_digital(abf, sweep_index=0, color=\"blue\")\n",
    "if stim_time is not None:\n",
    "    print(f\"刺激時間: {stim_time:.3f} 秒\")\n",
    "else:\n",
    "    print(\"デジタル信号が検出されませんでした。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c27b66-7b9f-4431-97f9-045e118f4244",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Detect digital stimulus time #\n",
    "################################\n",
    "def find_stim_time_digital(abf, sweep_index, color):\n",
    "    \"\"\"\n",
    "    Return the FIRST time in seconds the chosen digital output transitions from 0 to 1.\n",
    "    \n",
    "    Args:\n",
    "        abf: a pyabf.ABF object\n",
    "        sweep_index (int): which sweep to examine\n",
    "        color (str): \"red\" or \"blue\" to pick which digital channel to read\n",
    "    \n",
    "    Returns:\n",
    "        float or None: the time (in seconds) of the first sample where digital_sig == 1.\n",
    "    \"\"\"\n",
    "    if color == \"red\":\n",
    "        digOutNum = 0\n",
    "    elif color == \"blue\":\n",
    "        digOutNum = 3\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    abf.setSweep(sweep_index)\n",
    "    time_s = abf.sweepX\n",
    "    digital_sig = abf.sweepD(digOutNum)  # 0 or 1 array\n",
    "\n",
    "    # indices where digital_sig is 1\n",
    "    idxs = np.where(digital_sig == 1)[0]\n",
    "    if len(idxs) == 0:\n",
    "        return None\n",
    "    \n",
    "    idx_stim = idxs[0]\n",
    "    return time_s[idx_stim]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c98b603-87d0-48d4-bfa9-50981ece0b30",
   "metadata": {},
   "source": [
    "### refined_onset_time関数\n",
    "\n",
    "この関数は、PSC（Post-Synaptic Current）の応答開始時間（オンセットタイム）を精密に特定するためのものです。特に、刺激時間（stim_time_s）と指定された終了時間（onset_end）の間に制限して特定します。\n",
    "\n",
    "---\n",
    "\n",
    "#### 引数:\n",
    "- **`time_s`**: `array`  \n",
    "  時間データ（秒単位）の配列。\n",
    "\n",
    "- **`current_pA`**: `array`  \n",
    "  電流データ（pA単位）の配列。\n",
    "\n",
    "- **`stim_time_s`**: `float`  \n",
    "  刺激時間（秒単位）。\n",
    "\n",
    "- **`expect_inward`**: `bool`, デフォルト: `True`  \n",
    "  EPSCが内向き（負の電流）であるかを指定。`True`の場合、負の電流を正に変換して解析を行います。\n",
    "\n",
    "- **`onset_end`**: `float`, デフォルト: `None`  \n",
    "  オンセットタイムの最大値を制限（秒単位）。デフォルトでは制限なし（`np.inf`）。\n",
    "\n",
    "---\n",
    "\n",
    "#### 戻り値:\n",
    "- **`float`**: 精密化された応答開始時間（秒単位）。  \n",
    "  候補が見つからない場合は`stim_time_s`を返します。\n",
    "\n",
    "---\n",
    "\n",
    "#### 処理の流れ:\n",
    "\n",
    "1. **電流データの符号変換**:\n",
    "   - EPSC（負の電流）の場合、`current_pA`を符号反転（負→正）。\n",
    "   - 外向きの電流（正の場合）ではそのまま。\n",
    "\n",
    "2. **平滑化**:\n",
    "   - Savitzky-Golayフィルタ（`window_length=151`, `polyorder=3`）を適用し、電流データを平滑化。\n",
    "\n",
    "3. **微分**:\n",
    "   - 平滑化されたデータの微分を計算（`np.gradient`）。\n",
    "\n",
    "4. **ピークの検出**:\n",
    "   - `stim_time_s`から`onset_end`の範囲内でピークを検索。\n",
    "   - 最大値（または、EPSCの場合の最小値）を持つインデックスを取得。\n",
    "\n",
    "5. **閾値の計算**:\n",
    "   - ピーク値の20%を閾値として定義。\n",
    "\n",
    "6. **潜在的なオンセット候補のフィルタリング**:\n",
    "   - オンセット候補は以下を満たす必要があります:\n",
    "     - `stim_time_s`以上、ピーク時間以下。\n",
    "     - 平滑化された電流が閾値を超える。\n",
    "     - EPSC（内向き電流）の場合、微分値が負（`d1 < 0`）。\n",
    "     - 外向き電流の場合、微分値が正（`d1 > 0`）。\n",
    "\n",
    "7. **最適なオンセットの選択**:\n",
    "   - 候補の中から`stim_time_s`に最も近い時刻を選択。\n",
    "\n",
    "---\n",
    "\n",
    "#### 注意点:\n",
    "- データの長さが151点未満の場合、平滑化が行えないため`stim_time_s`を返します。\n",
    "- 潜在的なオンセット候補が見つからない場合も同様に`stim_time_s`を返します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbca7cb-0763-462a-8928-0f01065861a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refined_onset_time(\n",
    "    time_s, current_pA, stim_time_s, expect_inward=True, onset_end=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Refine the response onset time by analyzing the slope around the PSC,\n",
    "    but force it to be between [stim_time_s, onset_end].\n",
    "\n",
    "    Steps (for an inward EPSC, expect_inward=True):\n",
    "      1) Flip current if negative (EPSC) so the \"peak\" becomes a max.\n",
    "      2) Smooth with Savitzky-Golay (window=151, poly=3).\n",
    "      3) Use gradient (np.gradient) to find slope.\n",
    "      4) Find \"peak\" after stim_time.\n",
    "      5) 20% threshold => potential onsets must exceed that threshold,\n",
    "         also slope < 0 if flipped (for negative events).\n",
    "      6) Onset must be >= stim_time_s AND <= onset_end.\n",
    "      7) Pick the candidate onset closest to stim_time_s (but not < stim_time_s).\n",
    "    \"\"\"\n",
    "    # 1) Flip sign if needed\n",
    "    if expect_inward:\n",
    "        cur_flipped = -current_pA\n",
    "    else:\n",
    "        cur_flipped = current_pA\n",
    "\n",
    "    # 2) Smooth\n",
    "    if len(cur_flipped) < 151:\n",
    "        return stim_time_s\n",
    "    cur_smoothed = savgol_filter(cur_flipped, window_length=151, polyorder=3)\n",
    "\n",
    "    # 3) derivative\n",
    "    d1 = np.gradient(cur_smoothed, time_s)\n",
    "\n",
    "    # 4) Find the peak AFTER stim_time\n",
    "    if onset_end is None:\n",
    "        onset_end = np.inf  # if not provided\n",
    "\n",
    "    # only look in [stim_time_s, onset_end] for the peak\n",
    "    post_mask = (time_s >= stim_time_s) & (time_s <= onset_end)\n",
    "    if not np.any(post_mask):\n",
    "        return stim_time_s\n",
    "\n",
    "    idx_max_local = np.argmax(cur_smoothed[post_mask])\n",
    "    offset_idx = np.where(post_mask)[0][0]\n",
    "    peak_idx = idx_max_local + offset_idx\n",
    "    peak_time = time_s[peak_idx]\n",
    "    peak_value = cur_smoothed[peak_idx]\n",
    "\n",
    "    # 5) threshold = 20% of that peak\n",
    "    threshold_20pct = 0.2 * peak_value\n",
    "\n",
    "    if expect_inward:\n",
    "        potential_mask = (\n",
    "            (time_s >= stim_time_s) & \n",
    "            (time_s <= peak_time) &     # must be before the peak\n",
    "            (cur_smoothed > threshold_20pct) &\n",
    "            (d1 < 0)\n",
    "        )\n",
    "    else:\n",
    "        potential_mask = (\n",
    "            (time_s >= stim_time_s) &\n",
    "            (time_s <= peak_time) &\n",
    "            (cur_smoothed > threshold_20pct) &\n",
    "            (d1 > 0)\n",
    "        )\n",
    "\n",
    "    idx_candidates = np.where(potential_mask)[0]\n",
    "    if len(idx_candidates) == 0:\n",
    "        return stim_time_s\n",
    "\n",
    "    # among candidates, choose the one closest to stim_time_s\n",
    "    times_candidates = time_s[idx_candidates]\n",
    "    # they are all >= stim_time_s, so just pick the earliest\n",
    "    best_idx = idx_candidates[0]\n",
    "    return time_s[best_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8098bc-aa47-435d-b9e5-b6652616289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refined_onsetのためのパラメータ確認plot作成\n",
    "# %%\n",
    "import numpy as np\n",
    "import pyabf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "def plot_poststim_segment(\n",
    "    abf_path,\n",
    "    sweep_index,\n",
    "    color=\"blue\",\n",
    "    rec_chan=0,\n",
    "    voltage_hold=-55.0,    # <<< passed in just like in measure_sweep\n",
    "    baseline_window=0.01,\n",
    "    post_window=0.050,\n",
    "    smooth_win=151,\n",
    "    smooth_poly=3\n",
    "):\n",
    "    # Load and find stim\n",
    "    abf = pyabf.ABF(abf_path)\n",
    "    stim_time = find_stim_time_digital(abf, sweep_index, color)\n",
    "    if stim_time is None:\n",
    "        print(f\"No stim in sweep {sweep_index}\")\n",
    "        return\n",
    "\n",
    "    # Baseline subtract exactly as in measure_sweep\n",
    "    abf.setSweep(\n",
    "        sweepNumber=sweep_index,\n",
    "        channel=rec_chan,\n",
    "        baseline=[stim_time - baseline_window, stim_time]\n",
    "    )\n",
    "    t = abf.sweepX\n",
    "    y = abf.sweepY\n",
    "\n",
    "    # Isolate the 0–post_window after stim\n",
    "    mask   = (t >= stim_time) & (t <= stim_time + post_window)\n",
    "    t_post = t[mask] - stim_time\n",
    "    y_post = y[mask]\n",
    "\n",
    "    # Determine sign flip from voltage_hold, same logic as measure_sweep\n",
    "    expect_inward = (voltage_hold < 10)\n",
    "    y_flip = -y_post if expect_inward else y_post\n",
    "\n",
    "    # Smooth + derivative\n",
    "    if len(y_flip) >= smooth_win:\n",
    "        y_smooth = savgol_filter(y_flip, smooth_win, smooth_poly)\n",
    "    else:\n",
    "        y_smooth = y_flip\n",
    "    dy = np.gradient(y_smooth, t_post)\n",
    "    dy = savgol_filter(dy, smooth_win, smooth_poly)\n",
    "\n",
    "    # Plot\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8,6), sharex=True)\n",
    "    ax1.plot(t_post*1e3, y_post,    label=\"raw\")\n",
    "    ax1.plot(t_post*1e3, y_smooth * ( -1 if expect_inward else 1 ),\n",
    "             label=\"smoothed\", lw=2)\n",
    "    ax1.set_ylabel(\"Current (pA)\")\n",
    "    ax1.legend()\n",
    "    ax1.set_title(f\"Sweep {sweep_index}: 0–{post_window*1e3:.0f} ms post-stim\")\n",
    "\n",
    "    ax2.plot(t_post*1e3, dy, label=\"dI/dt\")\n",
    "    ax2.set_xlabel(\"Time after stim (ms)\")\n",
    "    ax2.set_ylabel(\"dI/dt (pA/s)\")\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# — Example usage —\n",
    "plot_poststim_segment(\n",
    "    \"./sorted_directory/ChR2/RSC/241215_ID12/24d15001.abf\",\n",
    "    sweep_index=4,\n",
    "    color=\"blue\",\n",
    "    rec_chan=0,\n",
    "    baseline_window=0.01,\n",
    "    post_window=0.050,\n",
    "    smooth_win=151,\n",
    "    smooth_poly=3\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254bc6ce-f5c3-4da9-82eb-b71c1db97dd6",
   "metadata": {},
   "source": [
    "### measure_sweep 関数\n",
    "\n",
    "この関数は、ABFファイルの1つのスイープ（sweep）に対して以下の解析を行います：\n",
    "\n",
    "1. 刺激時間の検出\n",
    "2. ベースライン補正\n",
    "3. ピーク検出\n",
    "4. 応答の開始時間（オンセット）の精密化\n",
    "5. レイテンシ（応答の遅延時間）計算\n",
    "6. 立ち上がり時間（10% -> 90%）計算\n",
    "7. 減衰時間定数の2重指数関数フィット\n",
    "\n",
    "---\n",
    "\n",
    "#### 引数:\n",
    "- **`abf`**: `pyabf.ABF` オブジェクト  \n",
    "  読み込まれたABFファイル。\n",
    "\n",
    "- **`sweep_index`**: `int`  \n",
    "  処理するスイープのインデックス。\n",
    "\n",
    "- **`color`**: `str`, デフォルト: `\"blue\"`  \n",
    "  刺激のデジタル出力の色（`\"red\"` または `\"blue\"`）。\n",
    "\n",
    "- **`rec_chan`**: `int`, デフォルト: `0`  \n",
    "  処理する記録チャネル。\n",
    "\n",
    "- **`baseline_window_sec`**: `float`, デフォルト: `0.01`  \n",
    "  ベースライン補正用のウィンドウサイズ（秒単位）。\n",
    "\n",
    "- **`voltage_hold`**: `float`, デフォルト: `-55.0`  \n",
    "  ホールド電圧。電流の符号（正または負）を決定するために使用。\n",
    "\n",
    "---\n",
    "\n",
    "#### 戻り値:\n",
    "- **`dict`**  \n",
    "  スイープの解析結果を含む辞書を返します。  \n",
    "  主なキー:\n",
    "  - `\"Sweep\"`: スイープインデックス\n",
    "  - `\"Color\"`: 刺激の色\n",
    "  - `\"StimTime_s\"`: デジタル出力に基づく刺激時間\n",
    "  - `\"RefinedOnset_s\"`: 精密化された応答開始時間\n",
    "  - `\"PeakAmplitude_pA\"`: ピーク振幅\n",
    "  - `\"Latency_ms\"`: レイテンシ（刺激から応答までの遅延）\n",
    "  - `\"RiseTime_ms\"`: 立ち上がり時間（10% -> 90%）\n",
    "  - `\"DecayTau\"`: 減衰時間定数\n",
    "  - `\"FitParams\"`: 2重指数関数フィットのパラメータ\n",
    "\n",
    "---\n",
    "\n",
    "#### 処理の流れ:\n",
    "\n",
    "1. **刺激時間の検出**  \n",
    "   `find_stim_time_digital` 関数を使用して、デジタル出力信号が1に遷移する最初の時間を特定します。\n",
    "\n",
    "2. **ベースライン補正**  \n",
    "   `stim_time_s` の直前の `baseline_window_sec` 秒間をベースラインとして補正します。\n",
    "\n",
    "3. **ピーク検出**  \n",
    "   - 刺激後50 ms以内のウィンドウでピーク（最大または最小）を検出。\n",
    "   - ホールド電圧に基づき、ピークが負（`voltage_hold < 10`）か正かを判断します。\n",
    "\n",
    "4. **10%値（t10）の検出**  \n",
    "   - ピークの10%値を計算。\n",
    "   - 最初にこの値を超えるタイムポイント（t10）を特定。\n",
    "\n",
    "5. **精密化された応答開始時間の計算**  \n",
    "   - `refined_onset_time` 関数を使用して、`stim_time_s` と `t10_abs` の間で応答開始時間を特定。\n",
    "\n",
    "6. **レイテンシの計算**  \n",
    "   - `stim_time_s` と精密化されたオンセット時間の差を計算（ms単位）。\n",
    "\n",
    "7. **立ち上がり時間（10% -> 90%）**  \n",
    "   - ピークの10%から90%までの時間差を計算（ms単位）。\n",
    "\n",
    "8. **減衰時間定数のフィット**  \n",
    "   - ピーク時間から100 ms以内のデータを使用し、2重指数関数でフィット。\n",
    "   - 減衰時間定数（`DecayTau`）を取得。\n",
    "\n",
    "---\n",
    "\n",
    "#### 注意:\n",
    "- データが不足している場合、適切な解析ができず `np.nan` を返す場合があります。\n",
    "- 精密化された応答開始時間を計算するために、10%値（t10）を制限として使用します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74b245e-acd1-483a-a735-faf87c021cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_sweep(\n",
    "    abf, \n",
    "    sweep_index, \n",
    "    color=\"blue\",\n",
    "    rec_chan=0,\n",
    "    baseline_window_sec=0.01,\n",
    "    voltage_hold=-55.0\n",
    "):\n",
    "    \"\"\"\n",
    "    - Detect digital stimulus time\n",
    "    - Baseline subtract\n",
    "    - Find 10% crossing time => pass as 'onset_end' to refined_onset_time\n",
    "    - Decide negative/positive peak, measure amplitude, latency, etc.\n",
    "    - Return extra info (peak time, 10% time, decay window) so we can re-plot easily.\n",
    "    \"\"\"\n",
    "    # 1) Stim time\n",
    "    stim_time_s = find_stim_time_digital(abf, sweep_index, color=color)\n",
    "    if stim_time_s is None:\n",
    "        return {}\n",
    "\n",
    "    # 2) Baseline subtract\n",
    "    baseline_start_sec = stim_time_s - baseline_window_sec\n",
    "    baseline_end_sec   = stim_time_s\n",
    "    abf.setSweep(\n",
    "        sweepNumber=sweep_index,\n",
    "        channel=rec_chan,\n",
    "        baseline=[baseline_start_sec, baseline_end_sec]\n",
    "    )\n",
    "    current_bs = abf.sweepY\n",
    "    time_s = abf.sweepX\n",
    "\n",
    "    # 3) We'll pick 0–50 ms after stim for \"peak\" detection\n",
    "    window_start = stim_time_s\n",
    "    if voltage_hold==10.0:\n",
    "        window_end   = stim_time_s + 0.050\n",
    "    else:\n",
    "        window_end   = stim_time_s + 0.020\n",
    "    mask = (time_s >= window_start) & (time_s <= window_end)\n",
    "    t_post = time_s[mask] - stim_time_s\n",
    "    c_post = current_bs[mask]\n",
    "    if len(c_post) == 0:\n",
    "        return {}\n",
    "\n",
    "    # 4) Negative vs positive peak\n",
    "    if voltage_hold < 10:\n",
    "        peak_idx = np.argmin(c_post)\n",
    "        expect_inward = True\n",
    "    else:\n",
    "        peak_idx = np.argmax(c_post)\n",
    "        expect_inward = False\n",
    "\n",
    "    peak_amplitude = c_post[peak_idx]\n",
    "    peak_time_rel = t_post[peak_idx]  # relative to stim\n",
    "    peak_time_abs = stim_time_s + peak_time_rel\n",
    "\n",
    "    # 5) 10% of peak for rise-time\n",
    "    #    We'll flip sign if it's negative\n",
    "    sign_factor = -1 if expect_inward else 1\n",
    "    c_post_flipped = c_post * sign_factor\n",
    "    peak_flipped = c_post_flipped[peak_idx]\n",
    "    val10 = 0.1 * peak_flipped\n",
    "    # find earliest crossing of 10% in c_post_flipped\n",
    "    idx_10 = np.where(c_post_flipped >= val10)[0]\n",
    "    if len(idx_10) > 0:\n",
    "        t10_rel = t_post[idx_10[0]]\n",
    "        t10_abs = stim_time_s + t10_rel\n",
    "    else:\n",
    "        t10_abs = peak_time_abs\n",
    "\n",
    "    # 6) Refined onset detection in [stim_time_s, t10_abs]\n",
    "    onset_s = refined_onset_time(\n",
    "        time_s,\n",
    "        current_bs,\n",
    "        stim_time_s,\n",
    "        expect_inward=expect_inward,\n",
    "        onset_end=t10_abs\n",
    "    )\n",
    "    latency_ms = (onset_s - stim_time_s) * 1000.0\n",
    "\n",
    "    # 7) Rise time (10% -> 90%)\n",
    "    val90 = 0.9 * peak_flipped\n",
    "    idx_90 = np.where(c_post_flipped >= val90)[0]\n",
    "    if (len(idx_10) > 0) and (len(idx_90) > 0):\n",
    "        t10 = t_post[idx_10[0]]\n",
    "        t90 = t_post[idx_90[0]]\n",
    "        rise_time_ms = (t90 - t10) * 1000.0\n",
    "    else:\n",
    "        rise_time_ms = np.nan\n",
    "\n",
    "    # 8) Decay fit\n",
    "    fit_window_sec = 0.100\n",
    "    decay_start = peak_time_abs\n",
    "    decay_end   = decay_start + fit_window_sec\n",
    "    decay_mask = (time_s >= decay_start) & (time_s <= decay_end)\n",
    "    t_decay = time_s[decay_mask] - decay_start\n",
    "    c_decay = current_bs[decay_mask]\n",
    "\n",
    "    if len(t_decay) < 3:\n",
    "        return {\n",
    "            \"Sweep\": sweep_index,\n",
    "            \"Color\": color,\n",
    "            \"VoltageHold\": voltage_hold,\n",
    "            \"StimTime_s\": stim_time_s,\n",
    "            \"BaselineStart_s\": baseline_start_sec,\n",
    "            \"BaselineEnd_s\": baseline_end_sec,\n",
    "            \"RefinedOnset_s\": onset_s,\n",
    "            \"PeakAmplitude_pA\": peak_amplitude,\n",
    "            \"PeakTime_s\": peak_time_abs,\n",
    "            \"T10Time_s\": t10_abs,\n",
    "            \"Latency_ms\": latency_ms,\n",
    "            \"RiseTime_ms\": rise_time_ms,\n",
    "            \"DecayTau\": np.nan,\n",
    "            \"R2\": np.nan,\n",
    "            \"FitParams\": np.nan,\n",
    "            \"DecayStart_s\": decay_start,\n",
    "            \"DecayEnd_s\": decay_end,\n",
    "        }\n",
    "\n",
    "    def biexp_wrap(t, m, tau, n, c):\n",
    "        return biexponential_fit(t, m, tau, n, c)\n",
    "\n",
    "    p0 = [peak_amplitude, 0.010, peak_amplitude*0.5, 0]\n",
    "    try:\n",
    "        # only constrain tau to [0.5 ms, 100 ms]\n",
    "        lower = [-np.inf,    0,  -np.inf, -np.inf]\n",
    "        upper = [ np.inf,  1,   np.inf,  np.inf]\n",
    "        \n",
    "        popt, _ = curve_fit(\n",
    "            biexp_wrap,\n",
    "            t_decay,\n",
    "            c_decay,\n",
    "            p0=p0,\n",
    "            bounds=(lower, upper),\n",
    "            method='trf'   # must use a solver that supports bounds\n",
    "        )\n",
    "        # —— ADD THIS R² CALCULATION ——\n",
    "        y_fit    = biexp_wrap(t_decay, *popt)\n",
    "        ss_res   = np.sum((c_decay - y_fit)**2)\n",
    "        ss_tot   = np.sum((c_decay - np.mean(c_decay))**2)\n",
    "        r2       = 1 - ss_res/ss_tot if ss_tot>0 else np.nan\n",
    "    except RuntimeError:\n",
    "        popt = [np.nan]*4\n",
    "        r2   = np.nan\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"Sweep\": sweep_index,\n",
    "        \"Color\": color,\n",
    "        \"VoltageHold\": voltage_hold,\n",
    "        \"StimTime_s\": stim_time_s,\n",
    "        \"BaselineStart_s\": baseline_start_sec,\n",
    "        \"BaselineEnd_s\": baseline_end_sec,\n",
    "        \"RefinedOnset_s\": onset_s,\n",
    "        \"PeakAmplitude_pA\": peak_amplitude,\n",
    "        \"PeakTime_s\": peak_time_abs,\n",
    "        \"T10Time_s\": t10_abs,\n",
    "        \"Latency_ms\": latency_ms,\n",
    "        \"RiseTime_ms\": rise_time_ms,\n",
    "        \"DecayTau\": popt[1],\n",
    "        \"R2\":        r2,\n",
    "        \"FitParams\": popt,\n",
    "        \"DecayStart_s\": decay_start,\n",
    "        \"DecayEnd_s\": decay_end,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28e0900-5f12-4ca7-bd5d-e75ece0ec2bc",
   "metadata": {},
   "source": [
    "### analyze_evoked_responses 関数\n",
    "\n",
    "この関数は、指定されたABFファイル内のすべてのスイープに対して解析を行い、各スイープの結果を含むDataFrameを返します。\n",
    "\n",
    "---\n",
    "\n",
    "#### 引数:\n",
    "- **`abf_path`**: `str`  \n",
    "  解析対象のABFファイルのパス。\n",
    "\n",
    "- **`color`**: `str`, デフォルト: `\"blue\"`  \n",
    "  刺激に対応するデジタル出力の色（`\"red\"` または `\"blue\"`）。\n",
    "\n",
    "- **`rec_chan`**: `int`, デフォルト: `0`  \n",
    "  処理する記録チャネルのインデックス。\n",
    "\n",
    "- **`voltage_hold`**: `float`, デフォルト: `-55.0`  \n",
    "  ホールド電圧。この値に基づいて電流が正（アウトワード）か負（インワード）かを決定。\n",
    "\n",
    "---\n",
    "\n",
    "#### 戻り値:\n",
    "- **`pd.DataFrame`**  \n",
    "  各スイープの解析結果を含むDataFrame。\n",
    "\n",
    "---\n",
    "\n",
    "#### 処理の流れ:\n",
    "1. 指定されたABFファイルを `pyabf.ABF` オブジェクトとして読み込む。\n",
    "2. ファイル内のすべてのスイープをループで処理。\n",
    "3. 各スイープについて `measure_sweep` 関数を呼び出し、解析を実行。\n",
    "4. 解析結果をリストに格納し、最終的にDataFrameに変換して返す。\n",
    "\n",
    "---\n",
    "\n",
    "### analyze_abf_files 関数\n",
    "\n",
    "この関数は、指定されたフィルタ済みDataFrame（`df_filtered`）とABFファイルが格納されているディレクトリ（`abf_root`）を基に、各行に対応するABFファイルを解析し、その結果をDataFrameとして返します。\n",
    "\n",
    "---\n",
    "\n",
    "#### 引数:\n",
    "- **`df_filtered`**: `pd.DataFrame`  \n",
    "  解析対象のABFファイルを指定する情報を含むDataFrame。主なカラムには以下が含まれる:\n",
    "  - `\"filename\"`: ABFファイル名（例: `\"24n15005.abf\"`）\n",
    "  - `\"Color\"`: 刺激の色（例: `\"blue\"` または `\"red\"`）\n",
    "  - `\"VoltageHold\"`: ホールド電圧\n",
    "\n",
    "- **`abf_root`**: `str`  \n",
    "  ABFファイルが格納されているディレクトリのルートパス。\n",
    "\n",
    "---\n",
    "\n",
    "#### 戻り値:\n",
    "- **`pd.DataFrame`**  \n",
    "  各スイープの解析結果を含むDataFrame。\n",
    "\n",
    "---\n",
    "\n",
    "#### 処理の流れ:\n",
    "1. **ABFファイルの検索**  \n",
    "   指定されたルートディレクトリ（`abf_root`）内を再帰的に検索し、すべての`.abf`ファイルを短いファイル名（例: `\"24n15005.abf\"`）からフルパスへのマッピングを作成する。\n",
    "\n",
    "2. **フィルタ済みDataFrameの各行をループで処理**  \n",
    "   `df_filtered` の各行を処理し、その行に対応するABFファイルを解析する。\n",
    "\n",
    "3. **ABFファイルのパス取得**  \n",
    "   現在の行の `\"filename\"` カラムを基にABFファイルのフルパスを取得。ファイルが見つからない場合は警告を表示してスキップ。\n",
    "\n",
    "4. **マルチスイープ解析の実行**  \n",
    "   `analyze_evoked_responses` 関数を呼び出し、解析を実行。\n",
    "\n",
    "5. **解析結果の統合**  \n",
    "   - `df_filtered` の行からメタデータ（例: `\"Opsin\"`, `\"Region\"`, `\"VoltageHold\"`）を取得。\n",
    "   - 各スイープの解析結果に対応するメタデータを付加して、リストに格納。\n",
    "\n",
    "6. **最終結果の生成**  \n",
    "   すべてのスイープの結果を統合したDataFrameを作成して返す。\n",
    "\n",
    "---\n",
    "\n",
    "#### 注意:\n",
    "- ABFファイルが見つからない場合は警告を表示し、その行をスキップします。\n",
    "- 各スイープの解析結果は、対応するフィルタ済みDataFrameのメタデータと統合されます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ec25e9-12df-4b6b-8de4-861fb1eb7676",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "# Incorporate into your \"analyze_evoked...\" fn\n",
    "###############################################\n",
    "import concurrent.futures\n",
    "\n",
    "def analyze_evoked_responses(abf_path, color=\"blue\", rec_chan=0, voltage_hold=-55.0):\n",
    "    \"\"\"\n",
    "    Loop over all sweeps in the ABF file, measure the response if a stimulus is detected,\n",
    "    and return a DataFrame with results for each sweep.\n",
    "    \"\"\"\n",
    "    abf = pyabf.ABF(abf_path)\n",
    "    \n",
    "    # ── NEW: run the membrane test and get per-sweep values ──\n",
    "    mem = Memtest(abf)  # passive properties per sweep\n",
    "    Ihs   = mem.Ih.values    # clamp currents (pA)\n",
    "    Rms   = mem.Rm.values    # membrane resistances (MΩ)\n",
    "    Ras   = mem.Ra.values    # access resistances (MΩ)\n",
    "    Cms   = mem.CmStep.values# capacitances (pF)\n",
    "    \n",
    "    # build a time‐vector for sweep start times (sec)\n",
    "    # if abf.sweepTimesSec exists you can use that; otherwise:\n",
    "    times_sec = np.arange(abf.sweepCount) * abf.sweepIntervalSec\n",
    "\n",
    "    sweep_results = []\n",
    "    \n",
    "    for sweep_index in abf.sweepList:\n",
    "        sweep_dict = measure_sweep(\n",
    "            abf,\n",
    "            sweep_index=sweep_index,\n",
    "            color=color,\n",
    "            rec_chan=rec_chan,\n",
    "            baseline_window_sec=0.01,\n",
    "            voltage_hold=voltage_hold\n",
    "        )\n",
    "        if not sweep_dict:\n",
    "            continue\n",
    "        # ── NEW: timestamp & membrane properties for this sweep ──\n",
    "        sweep_dict[\"sweep_time_s\"] = times_sec[sweep_index]\n",
    "        sweep_dict[\"Ih_pA\"]       = Ihs[sweep_index]\n",
    "        sweep_dict[\"Rm_MOhm\"]      = Rms[sweep_index]\n",
    "        sweep_dict[\"Ra_MOhm\"]      = Ras[sweep_index]\n",
    "        sweep_dict[\"Cm_pF\"]        = Cms[sweep_index]\n",
    "\n",
    "        sweep_results.append(sweep_dict)\n",
    "\n",
    "    return pd.DataFrame(sweep_results)\n",
    "\n",
    "\n",
    "\n",
    "def process_filtered_row(args):\n",
    "    idx, row, abf_dict, abf_root = args\n",
    "    abf_name = row[\"filename\"]\n",
    "    if abf_name not in abf_dict:\n",
    "        return []\n",
    "    abf_path = abf_dict[abf_name]\n",
    "    df_sweep = analyze_evoked_responses(\n",
    "        abf_path,\n",
    "        color=row.get(\"Color\",\"blue\"),\n",
    "        rec_chan=0,\n",
    "        voltage_hold=row.get(\"VoltageHold\", -55)\n",
    "    )\n",
    "    out = []\n",
    "    for _, sweep_dict in df_sweep.iterrows():\n",
    "        merged = {\n",
    "            \"index_df_filtered\": idx,\n",
    "            \"filename\": abf_name,\n",
    "            **{k: row.get(k) for k in [\"Opsin\",\"Region\",\"BrainID\",\"SliceID\",\"CellID\",\"VoltageHold\",\"DrugList\",\"APregion\",\"RoughAP\"]},\n",
    "            **sweep_dict.to_dict()\n",
    "        }\n",
    "        out.append(merged)\n",
    "    return out\n",
    "\n",
    "def analyze_abf_files_parallel(df_filtered, abf_root, n_workers=None):\n",
    "    abf_dict = find_abf_files(abf_root)\n",
    "    args_iter = [\n",
    "        (idx, row.to_dict(), abf_dict, abf_root)\n",
    "        for idx, row in df_filtered.iterrows()\n",
    "    ]\n",
    "    results_list = []\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=n_workers) as exe:\n",
    "        for sublist in exe.map(process_filtered_row, args_iter):\n",
    "            results_list.extend(sublist)\n",
    "    return pd.DataFrame(results_list)\n",
    "\n",
    "def analyze_abf_files(df_filtered, abf_root):\n",
    "    \"\"\"\n",
    "    Given a DataFrame (df_filtered) with a 'filename' column and\n",
    "    a directory abf_root that contains (somewhere) the ABF files,\n",
    "    this function:\n",
    "      1) Recursively finds all .abf files in abf_root\n",
    "      2) Loops over each row in df_filtered\n",
    "      3) Loads the ABF file (if found)\n",
    "      4) Calls analyze_evoked_responses(abf_path, color=...)\n",
    "      5) Returns a new DataFrame with analysis results \n",
    "         (one row per sweep, plus appended metadata).\n",
    "    \"\"\"\n",
    "    # 1) Create the dictionary of all ABF files (short name -> full path)\n",
    "    abf_dict = find_abf_files(abf_root)\n",
    "    \n",
    "    results_list = []\n",
    "    \n",
    "    # 2) Loop through each row of df_filtered\n",
    "    for idx, row in df_filtered.iterrows():\n",
    "        abf_name = row[\"filename\"]  # e.g. \"24n15005.abf\"\n",
    "        \n",
    "        # 3) Look for full path in abf_dict\n",
    "        if abf_name not in abf_dict:\n",
    "            print(f\"[WARNING] Cannot find {abf_name} in {abf_root}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        abf_path = abf_dict[abf_name]\n",
    "        \n",
    "        # 4) Perform your multi-sweep analysis \n",
    "        #    (pass color from the current row to the function)\n",
    "        row_color = row.get(\"Color\", \"blue\")  # default to \"blue\" if missing\n",
    "        row_voltage = row.get(\"VoltageHold\", -55)  # default to -55 if missing\n",
    "        \n",
    "        df_sweep = analyze_evoked_responses(\n",
    "            abf_path=abf_path, \n",
    "            color=row_color, \n",
    "            rec_chan=0,\n",
    "            voltage_hold = row_voltage\n",
    "        )\n",
    "        \n",
    "        # For each sweep result, augment with metadata from df_filtered row\n",
    "        for i_sweep, sweep_dict in df_sweep.iterrows():\n",
    "            # sweep_dict holds columns like [\"Sweep\",\"Color\",\"StimTime_s\", etc.]\n",
    "            # We'll make a new dictionary that merges row data and sweep data\n",
    "            merged_result = {\n",
    "                \"index_df_filtered\": idx,\n",
    "                \"filename\": abf_name,\n",
    "                \n",
    "                # copy relevant columns from the row\n",
    "                \"Opsin\":       row.get(\"Opsin\", None),\n",
    "                \"Region\":      row.get(\"Region\", None),\n",
    "                \"BrainID\":     row.get(\"BrainID\", None),\n",
    "                \"SliceID\":     row.get(\"SliceID\", None),\n",
    "                \"CellID\":      row.get(\"CellID\", None),\n",
    "                \"VoltageHold\": row.get(\"VoltageHold\", None),\n",
    "                \"DrugList\":    row.get(\"DrugList\", None),\n",
    "                \"APregion\":    row.get(\"APregion\", None),\n",
    "                \"RoughAP\":     row.get(\"RoughAP\", None),\n",
    "            }\n",
    "            \n",
    "            # Add the columns from the sweep_dict (the actual measurement results)\n",
    "            # sweep_dict is a Series, so we convert to a dict\n",
    "            merged_result.update(sweep_dict.to_dict())\n",
    "            \n",
    "            results_list.append(merged_result)\n",
    "    \n",
    "    # Create a DataFrame from the results\n",
    "    df_results = pd.DataFrame(results_list)\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e202c14-3eb1-47d6-be3c-e4ba7ee07635",
   "metadata": {},
   "source": [
    "## `analysis_results.csv`に保存(df_filteredに含まれるファイル解析)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e1723b-e3d8-4b10-bdc8-1fcd32c7ba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) Suppose we already have df_filtered\n",
    "if df_filtered is None:\n",
    "    df_filtered = pd.read_csv(\"./sorted_directory/master_with_AP_filtered.csv\")\n",
    "\n",
    "\n",
    "# 1) Path where ABF files live (somewhere in subfolders)\n",
    "abf_root = \"./sorted_directory\"\n",
    "\n",
    "# 2) Analyze\n",
    "#df_results = analyze_abf_files(df_filtered, abf_root)\n",
    "# NEW (parallel):\n",
    "df_results = analyze_abf_files_parallel(df_filtered, abf_root, n_workers=8)\n",
    "\n",
    "# 3) Check/save results\n",
    "df_results.to_csv(\"./sorted_directory/analysis_results.csv\", index=False)\n",
    "df_results.to_csv(f\"./sorted_directory/analysis_results_{df_results['Opsin'][1]}.csv\")\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cf6c3b-996c-4972-840f-69e507d9dbe2",
   "metadata": {},
   "source": [
    "## `plot_evoked_sweeps_5x2()` 関数\n",
    "\n",
    "### 関数の概要\n",
    "\n",
    "次の `plot_evoked_sweeps_5x2()` 関数は、ABF ファイル（電気生理学データ）を可視化するためのものです。具体的には以下の処理を行います:\n",
    "\n",
    "1. **ABF ファイルのパスを検索**  \n",
    "   ユーザが指定した `abf_root` ディレクトリを再帰的に探索し、引数で与えられた `filename`（例： `\"24n15005.abf\"`）に対応するフルパスを取得します。  \n",
    "   - 見つからない場合はエラーを出して終了します。\n",
    "\n",
    "2. **ABF データの読み込み**  \n",
    "   `pyabf.ABF(abf_path)` を使って ABF ファイルを読み込み、刺激タイミングや記録波形 (`sweepY`) などをアクセスできるようにします。\n",
    "\n",
    "3. **描画対象のスイープ（sweep）の抽出**  \n",
    "   `df_results`（解析結果を保持する DataFrame）から、指定の `filename` と一致する行のみを抽出します。そこに含まれる `Sweep` 列（スイープ番号）をソートして、最大 10 スイープまでプロット対象にします。\n",
    "\n",
    "4. **グローバルな x/y の描画範囲を決定するための第一段階処理**  \n",
    "   各スイープに対して:\n",
    "   - 該当スイープを `abf.setSweep()` でセットし、必要に応じてベースライン補正 (`baseline=[start, end]`) を行います。\n",
    "   - 刺激タイミング（`StimTime_s`）を取得し、そこから `[stimTime - 0.1, stimTime + 0.5]` の時間範囲を「描画領域」として切り出します（ただし `StimTime_s` が NaN の場合はスイープ全域を使用）。\n",
    "   - 切り出した時間領域 (`t_dom`) と電流波形 (`i_dom`) の最小値・最大値を求め、その情報から “グローバルな” 横軸 (time) の最小値・最大値、縦軸 (電流) の最小値・最大値を更新していきます。\n",
    "\n",
    "5. **第二段階 (プロット本体)**  \n",
    "   第一段階で集めたスイープごとの情報（`t_dom` や `i_dom` など）を用いて実際にグラフを描画します。\n",
    "   - “生”の波形 (`t_dom`, `i_dom`) を `ax.plot()` で描画。\n",
    "   - デジタル信号 (`d_dom`) が 1 になっている区間を、`fill_between()` を用いてシェーディング。\n",
    "   - 刺激時刻が範囲内にあれば、グレーの縦帯 (`axvspan()`) で示す。\n",
    "   - 各スイープに対し、オンセット (`RefinedOnset_s`)、ピーク時刻 (`PeakTime_s`) といった解析済みのマーカーをオーバーレイ（`plot_marker()` 関数で描画）。\n",
    "   - フィット（`FitParams`）がある場合は、2 変数の指数関数などで計算したフィット結果を重ね描画。\n",
    "\n",
    "6. **凡例とレイアウト**  \n",
    "   各サブプロットで生成されたラインやラベルをまとめて、一意のもののみを抽出して凡例を作る（ただしコメントアウトされているので、デフォルトでは凡例を表示していない）。  \n",
    "   タイトルは `fig.suptitle()` で一括設定し、`plt.tight_layout()` でレイアウトを整えた後に `plt.show()` で描画完了。\n",
    "\n",
    "---\n",
    "\n",
    "### 関数の主な引数\n",
    "\n",
    "- **filename (str)**  \n",
    "  描画対象となる ABF ファイル名（拡張子を含む）。  \n",
    "- **df_results (pandas.DataFrame)**  \n",
    "  スイープごとに解析した結果が含まれる表データ。たとえば、列に `Sweep`, `StimTime_s`, `RefinedOnset_s`, `PeakTime_s`, `FitParams` などが入っている想定。\n",
    "- **abf_root (str)**  \n",
    "  ABF ファイルを格納したディレクトリへのパス。\n",
    "\n",
    "---\n",
    "\n",
    "### 処理の流れ\n",
    "\n",
    "1. **ABF ファイル検索**:  \n",
    "   `find_abf_files(abf_root)` を使って、全サブディレクトリを含めて再帰的に `.abf` ファイルを探し、ファイル名（短い名称）からフルパスへの辞書を作る。\n",
    "2. **ABF の読み込み**:  \n",
    "   指定した `filename` が見つかれば `abf_path` を得て、`pyabf.ABF(abf_path)` で読み込み。\n",
    "3. **該当ファイルの解析結果フィルタ**:  \n",
    "   `df_results[\"filename\"] == filename` で行を絞り込み、スイープ番号 (`Sweep` 列) を抽出してソート。\n",
    "4. **グローバル描画範囲の決定 (第一段階)**  \n",
    "   最大 10 個のスイープに対し、ベースライン補正と時間領域切り出しを行い、横軸 (time) と縦軸 (current) の全スイープ共通の min/max を算出。\n",
    "5. **実際の描画 (第二段階)**  \n",
    "   各スイープに対応するプロットを個別のサブプロット（5×2 のグリッド）に配置。時間軸をグローバルに合わせて、電流値もグローバルな min/max になるように設定。  \n",
    "   刺激区間のシェーディング・オンセット/ピークマーカー・フィット曲線等を重ね描画。\n",
    "6. **レジェンド重複の除去**:  \n",
    "   全サブプロットで収集したライン/ラベルを集約し、重複しない形で最終的な legend を構築（ただし、サンプルコードでは最後にコメントアウトされており、実際には表示されない設定になっている）。\n",
    "7. **レイアウト調整 & 表示**:  \n",
    "   `plt.tight_layout()` で図を整形し、`plt.show()` で表示。\n",
    "\n",
    "---\n",
    "\n",
    "### コードを活用する際の注意\n",
    "\n",
    "- 同じ ABF ファイルに複数のスイープが含まれる場合、本関数はその中でも `df_results` に記載されているものだけをプロットします。\n",
    "- 刺激時刻がないスイープ（`StimTime_s` が NaN）の場合は、スイープ全体を描画しますが、x 軸範囲は `[0, abf.sweepX.max()]` 付近になる可能性があります。\n",
    "- `subplot_info` に全スイープぶんの描画用データを一度格納し、二度目のループでグローバル軸範囲を適用して描画しているので、途中でさらにフィルタ等をかけたい場合は適宜ロジックを追加してカスタマイズが可能です。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fec876-8c2d-4481-ac82-13717454886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_evoked_sweeps_5x2(filename, df_results, abf_root):\n",
    "    \"\"\"\n",
    "    - Plot each sweep in a separate subplot (up to 10 sweeps) arranged 5 rows × 2 columns.\n",
    "    - Restrict domain [stimTime - 0.1, stimTime + 0.5].\n",
    "    - Shade digital high times.\n",
    "    - Mark onset/peak times (if present in df_results).\n",
    "    - Use a single legend for the entire figure.\n",
    "    - *All subplots share the same x/y limits* (taken from the min and max found among all sweeps).\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Dictionary of ABF files\n",
    "    abf_dict = find_abf_files(abf_root)\n",
    "\n",
    "    # 2) Get the ABF path\n",
    "    if filename not in abf_dict:\n",
    "        print(f\"[ERROR] ABF file not found: {filename}\")\n",
    "        return\n",
    "    abf_path = abf_dict[filename]\n",
    "\n",
    "    # 3) Load the ABF\n",
    "    abf = pyabf.ABF(abf_path)\n",
    "\n",
    "    # 4) Filter df_results for just this file\n",
    "    df_sub = df_results[df_results[\"filename\"] == filename].copy()\n",
    "    sweep_nums = sorted(df_sub[\"Sweep\"].unique())\n",
    "\n",
    "    # Create figure with 5x2 subplots (up to 10 sweeps)\n",
    "    fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(10, 15), sharex=False, sharey=False)\n",
    "    axes = axes.flatten()  # to index easily as a list\n",
    "\n",
    "    # We'll collect global domain/y-range as we go:\n",
    "    global_t_min = np.inf\n",
    "    global_t_max = -np.inf\n",
    "    global_y_bottom = np.inf\n",
    "    global_y_top = -np.inf\n",
    "\n",
    "    # We'll also store a dictionary with each subplot's data for a second pass\n",
    "    subplot_info = []\n",
    "\n",
    "    # For collecting legend handles/labels from all subplots\n",
    "    all_lines = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i, sweep_num in enumerate(sweep_nums):\n",
    "        if i >= len(axes):\n",
    "            print(f\"[WARNING] More than {len(axes)} sweeps. Only plotting first {len(axes)}.\")\n",
    "            break\n",
    "\n",
    "        ax = axes[i]\n",
    "        row = df_sub[df_sub[\"Sweep\"] == sweep_num].iloc[0]\n",
    "\n",
    "        # Determine digital channel by color\n",
    "        row_color = row.get(\"Color\", \"blue\").lower()\n",
    "        if row_color == \"red\":\n",
    "            digOutNum = 0\n",
    "        elif row_color == \"blue\":\n",
    "            digOutNum = 3\n",
    "        else:\n",
    "            digOutNum = 3  # fallback\n",
    "\n",
    "        hold_val = row.get(\"VoltageHold\", -55)\n",
    "\n",
    "        # Stim time for domain\n",
    "        stim_t = row.get(\"StimTime_s\", np.nan)\n",
    "        if not np.isnan(stim_t):\n",
    "            baseline_start_sec = stim_t - 0.01\n",
    "            baseline_end_sec   = stim_t\n",
    "        else:\n",
    "            baseline_start_sec = 0\n",
    "            baseline_end_sec   = 0.01\n",
    "\n",
    "        # Set sweep with baseline\n",
    "        abf.setSweep(\n",
    "            sweepNumber=sweep_num,\n",
    "            channel=0,\n",
    "            baseline=[baseline_start_sec, baseline_end_sec]\n",
    "        )\n",
    "\n",
    "        time_s = abf.sweepX\n",
    "        current_pA = abf.sweepY\n",
    "        digital_sig = abf.sweepD(digOutNum)  # 0 or 1 array\n",
    "\n",
    "        if np.isnan(stim_t):\n",
    "            # If no stim time, just plot the whole sweep\n",
    "            t_dom = time_s\n",
    "            i_dom = current_pA\n",
    "            d_dom = digital_sig\n",
    "            # No domain limit\n",
    "            local_t_min = t_dom.min()\n",
    "            local_t_max = t_dom.max()\n",
    "        else:\n",
    "            # Domain [stimTime - 0.1, stimTime + 0.5]\n",
    "            t_min = stim_t - 0.1\n",
    "            t_max = stim_t + 0.5\n",
    "            domain_mask = (time_s >= t_min) & (time_s <= t_max)\n",
    "            if not np.any(domain_mask):\n",
    "                # If domain empty, skip this sweep\n",
    "                continue\n",
    "            t_dom = time_s[domain_mask]\n",
    "            i_dom = current_pA[domain_mask]\n",
    "            d_dom = digital_sig[domain_mask]\n",
    "            local_t_min = t_min\n",
    "            local_t_max = t_max\n",
    "\n",
    "        # Prepare data for plotting\n",
    "        curr_min = i_dom.min()\n",
    "        curr_max = i_dom.max()\n",
    "\n",
    "        # Decide local y-limits\n",
    "        if hold_val < 10:\n",
    "            # negative\n",
    "            y_bottom = curr_min * 1.5\n",
    "            y_top    = 0.5 * curr_min * -1\n",
    "        else:\n",
    "            # positive\n",
    "            y_bottom = 0.5 * curr_max * -1\n",
    "            y_top    = curr_max * 1.5\n",
    "\n",
    "        # In case y_top <= y_bottom, swap them\n",
    "        if y_top <= y_bottom:\n",
    "            y_bottom, y_top = min(y_bottom, y_top), max(y_bottom, y_top)\n",
    "\n",
    "        # Update global domain/y-range\n",
    "        global_t_min = min(global_t_min, local_t_min)\n",
    "        global_t_max = max(global_t_max, local_t_max)\n",
    "        global_y_bottom = min(global_y_bottom, y_bottom)\n",
    "        global_y_top    = max(global_y_top, y_top)\n",
    "\n",
    "        # Save the data so we can plot after all min/max are known\n",
    "        subplot_info.append({\n",
    "            \"ax\": ax,\n",
    "            \"t_dom\": t_dom,\n",
    "            \"i_dom\": i_dom,\n",
    "            \"d_dom\": d_dom,\n",
    "            \"row\": row,\n",
    "            \"sweep_num\": sweep_num,\n",
    "            \"row_color\": row_color,\n",
    "            \"stim_t\": stim_t,\n",
    "        })\n",
    "\n",
    "    #\n",
    "    # Second pass: Actually plot everything with the *global* domain and y-limits\n",
    "    #\n",
    "    for info in subplot_info:\n",
    "        ax = info[\"ax\"]\n",
    "        t_dom = info[\"t_dom\"]\n",
    "        i_dom = info[\"i_dom\"]\n",
    "        d_dom = info[\"d_dom\"]\n",
    "        row   = info[\"row\"]\n",
    "        sweep_num = info[\"sweep_num\"]\n",
    "        row_color = info[\"row_color\"]\n",
    "        stim_t    = info[\"stim_t\"]\n",
    "\n",
    "        # Plot data\n",
    "        ln = ax.plot(t_dom, i_dom, label=f\"Sweep {sweep_num}\")\n",
    "        lines, labels = ax.get_legend_handles_labels()\n",
    "        all_lines.extend(lines)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "        # Shade region where digital_sig == 1\n",
    "        ax.fill_between(\n",
    "            t_dom,\n",
    "            i_dom.min(),\n",
    "            i_dom.max(),\n",
    "            where=(d_dom > 0.5),\n",
    "            color=row_color,\n",
    "            alpha=0.2\n",
    "        )\n",
    "\n",
    "        # Draw a thin gray band for the stimulus if valid\n",
    "        if not np.isnan(stim_t):\n",
    "            if (stim_t >= global_t_min) and (stim_t <= global_t_max):\n",
    "                ax.axvspan(stim_t, stim_t+0.001, color=\"gray\", alpha=0.2)\n",
    "\n",
    "        ax.set_xlim(global_t_min, global_t_max)\n",
    "        ax.set_ylim(global_y_bottom, global_y_top)\n",
    "        ax.set_title(f\"Sweep {sweep_num}\")\n",
    "\n",
    "        # Optionally mark onset/peak\n",
    "        onset_t = row.get(\"RefinedOnset_s\", np.nan)\n",
    "        peak_t  = row.get(\"PeakTime_s\", np.nan)\n",
    "\n",
    "        def plot_marker(ax, marker_time, color_marker, label_marker):\n",
    "            if np.isnan(marker_time):\n",
    "                return\n",
    "            if marker_time < global_t_min or marker_time > global_t_max:\n",
    "                return\n",
    "            # find the closest index in t_dom\n",
    "            idx_closest = np.argmin(np.abs(t_dom - marker_time))\n",
    "            ax.plot(\n",
    "                t_dom[idx_closest],\n",
    "                i_dom[idx_closest],\n",
    "                marker=\"o\",\n",
    "                ms=7,\n",
    "                mfc=color_marker,\n",
    "                mec=\"k\",\n",
    "                label=label_marker\n",
    "            )\n",
    "\n",
    "        plot_marker(ax, onset_t, \"green\", \"Onset\")\n",
    "        plot_marker(ax, peak_t,  \"red\",   \"Peak\")\n",
    "\n",
    "        # If there's a fitted decay\n",
    "        popt = row.get(\"FitParams\", None)\n",
    "        decay_start = row.get(\"DecayStart_s\", None)\n",
    "        decay_end   = row.get(\"DecayEnd_s\", None)\n",
    "        if (isinstance(popt,(list,np.ndarray)) \n",
    "            and not np.any(np.isnan(popt))\n",
    "            and decay_start is not None \n",
    "            and decay_end   is not None):\n",
    "            fit_t = np.linspace(decay_start, decay_end, 200)\n",
    "            fit_x = fit_t - decay_start\n",
    "            fit_y = biexponential_fit(fit_x, *popt)\n",
    "            # Only plot portion overlapping with global domain\n",
    "            fit_mask = (fit_t >= global_t_min) & (fit_t <= global_t_max)\n",
    "            ax.plot(fit_t[fit_mask], fit_y[fit_mask], \"k--\", lw=2)\n",
    "\n",
    "    # Remove duplicates in legend\n",
    "    used = set()\n",
    "    unique_lines = []\n",
    "    unique_labels = []\n",
    "    for l, lb in zip(all_lines, all_labels):\n",
    "        if lb not in used:\n",
    "            unique_lines.append(l)\n",
    "            unique_labels.append(lb)\n",
    "            used.add(lb)\n",
    "\n",
    "    # Single legend for the entire figure (optional: adjust loc)\n",
    "    #fig.legend(unique_lines, unique_labels, loc=\"upper center\", ncol=5)\n",
    "    fig.suptitle(f\"Evoked Responses (5x2 grid): {filename}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1c9be7-65b7-4720-a720-2987bb584b29",
   "metadata": {},
   "source": [
    "### `plot_evoked_sweeps_5x2()` 関数呼び出し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deabc2a-0f74-4867-b1f8-d1375b80f1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_results is None:\n",
    "    df_results = pd.read_csv(\"./sorted_directory/analysis_results.csv\")\n",
    "plot_evoked_sweeps_5x2(\"24913043.abf\", df_results, abf_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f34e35-4184-4171-9d1e-9ebb15cbf813",
   "metadata": {},
   "source": [
    "## `plot_sweep9_for_multiple_files_5x2`関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e584bc3-8414-420d-bbfe-28ff52b003e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def plot_sweep9_for_multiple_files_5x2(df_results, abf_root, output_dir=\".\"):\n",
    "    \"\"\"\n",
    "    1) From df_results, gather all unique filenames that contain a row with Sweep=9.\n",
    "    2) Group those filenames in sets of up to 10 (so each figure can have 5x2 subplots).\n",
    "    3) For each filename in that batch:\n",
    "       - Attempt to plot the data for sweep 9.\n",
    "       - Domain: [StimTime_s-0.1, StimTime_s+0.5] if StimTime_s is available, else the entire sweep.\n",
    "       - Mark digital=1 region, etc.\n",
    "    4) Save each figure as a JPG in output_dir.\n",
    "\n",
    "    df_results is assumed to have columns:\n",
    "      [\"filename\",\"Sweep\",\"StimTime_s\",\"Color\",\"VoltageHold\",...,\"RefinedOnset_s\",\"PeakTime_s\",\"FitParams\",...]\n",
    "    \"\"\"\n",
    "    # 1) Filter rows for just Sweep=9\n",
    "    df_sweep9 = df_results[df_results[\"Sweep\"] == 9].copy()\n",
    "    if len(df_sweep9) == 0:\n",
    "        print(\"[WARNING] No rows with Sweep=9 found in df_results.\")\n",
    "        return\n",
    "\n",
    "    # 2) Gather unique filenames which have a row for Sweep=9\n",
    "    filenames_sweep9 = df_sweep9[\"filename\"].unique()\n",
    "    filenames_sweep9 = sorted(filenames_sweep9)\n",
    "\n",
    "    # 3) We'll create a dictionary of ABF paths for quick lookup\n",
    "    abf_dict = find_abf_files(abf_root)\n",
    "\n",
    "    # We define a helper function to do one figure with up to 10 filenames\n",
    "    def plot_batch_of_filenames(filenames_batch, fig_index):\n",
    "        \"\"\"Plot one figure with up to 10 subplots (5x2), each subplot = one filename's sweep=9.\"\"\"\n",
    "        fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(10, 15), sharex=False, sharey=False)\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        # Prepare for global domain/y-limits among these 10 filenames\n",
    "        global_t_min = np.inf\n",
    "        global_t_max = -np.inf\n",
    "        global_y_bottom = np.inf\n",
    "        global_y_top = -np.inf\n",
    "\n",
    "        # We'll store data to plot in a second pass\n",
    "        subplot_info = []\n",
    "\n",
    "        # ─────────────────────────────────────────────────\n",
    "        # FIRST PASS: find global domain among this batch\n",
    "        # ─────────────────────────────────────────────────\n",
    "        for i, fname in enumerate(filenames_batch):\n",
    "            if i >= len(axes):\n",
    "                break\n",
    "            ax = axes[i]\n",
    "            # Look up the row(s) in df_sweep9 for this filename\n",
    "            rows_this_file = df_sweep9[df_sweep9[\"filename\"] == fname]\n",
    "            if len(rows_this_file) == 0:\n",
    "                # No data? We'll leave it blank\n",
    "                subplot_info.append({\"ax\":ax, \"fname\":fname, \"hasData\":False})\n",
    "                continue\n",
    "\n",
    "            # We'll pick the first row to glean color, stim time, etc.\n",
    "            row0 = rows_this_file.iloc[0]\n",
    "            color_str = row0.get(\"Color\",\"blue\").lower()\n",
    "            if color_str == \"red\":\n",
    "                digOutNum = 0\n",
    "            elif color_str == \"blue\":\n",
    "                digOutNum = 3\n",
    "            else:\n",
    "                digOutNum = 3\n",
    "\n",
    "            hold_val = row0.get(\"VoltageHold\", -55)\n",
    "            stim_t   = row0.get(\"StimTime_s\", np.nan)\n",
    "\n",
    "            # Load the ABF\n",
    "            if fname not in abf_dict:\n",
    "                # ABF not found -> blank\n",
    "                subplot_info.append({\"ax\":ax,\"fname\":fname,\"hasData\":False})\n",
    "                continue\n",
    "\n",
    "            abf_path = abf_dict[fname]\n",
    "            abf = pyabf.ABF(abf_path)\n",
    "            # Check if abf actually has that sweep index=9\n",
    "            if 9 not in abf.sweepList:\n",
    "                # This ABF doesn't have a sweep #9\n",
    "                subplot_info.append({\"ax\":ax,\"fname\":fname,\"hasData\":False})\n",
    "                continue\n",
    "\n",
    "            # Load that sweep\n",
    "            baseline_start = (stim_t - 0.01) if not np.isnan(stim_t) else 0\n",
    "            baseline_end   = stim_t if not np.isnan(stim_t) else 0.01\n",
    "            abf.setSweep(9, channel=0, baseline=[baseline_start, baseline_end])\n",
    "            time_s = abf.sweepX\n",
    "            current_pA = abf.sweepY\n",
    "            digital_sig = abf.sweepD(digOutNum)\n",
    "\n",
    "            # domain\n",
    "            if np.isnan(stim_t):\n",
    "                t_dom = time_s\n",
    "                i_dom = current_pA\n",
    "                d_dom = digital_sig\n",
    "                local_t_min = t_dom.min()\n",
    "                local_t_max = t_dom.max()\n",
    "            else:\n",
    "                t_min = stim_t - 0.1\n",
    "                t_max = stim_t + 0.5\n",
    "                domain_mask = (time_s>=t_min)&(time_s<=t_max)\n",
    "                if not np.any(domain_mask):\n",
    "                    # no data\n",
    "                    subplot_info.append({\"ax\":ax,\"fname\":fname,\"hasData\":False})\n",
    "                    continue\n",
    "                t_dom = time_s[domain_mask]\n",
    "                i_dom = current_pA[domain_mask]\n",
    "                d_dom = digital_sig[domain_mask]\n",
    "                local_t_min = t_min\n",
    "                local_t_max = t_max\n",
    "\n",
    "            # local y-limits\n",
    "            curr_min = i_dom.min()\n",
    "            curr_max = i_dom.max()\n",
    "            if hold_val < 10:\n",
    "                y_bottom = curr_min * 1.5\n",
    "                y_top    = 0.5 * curr_min * -1\n",
    "            else:\n",
    "                y_bottom = 0.5 * curr_max * -1\n",
    "                y_top    = curr_max * 1.5\n",
    "            if y_top <= y_bottom:\n",
    "                y_bottom, y_top = min(y_bottom, y_top), max(y_bottom, y_top)\n",
    "\n",
    "            global_t_min = min(global_t_min, local_t_min)\n",
    "            global_t_max = max(global_t_max, local_t_max)\n",
    "            global_y_bottom = min(global_y_bottom, y_bottom)\n",
    "            global_y_top    = max(global_y_top, y_top)\n",
    "\n",
    "            # We'll store all rows for potential onset/peak plotting\n",
    "            subplot_info.append({\n",
    "                \"ax\":ax,\n",
    "                \"fname\":fname,\n",
    "                \"hasData\":True,\n",
    "                \"time_s\":time_s,\n",
    "                \"current_pA\":current_pA,\n",
    "                \"digital_sig\":digital_sig,\n",
    "                \"t_dom\":t_dom,\n",
    "                \"i_dom\":i_dom,\n",
    "                \"d_dom\":d_dom,\n",
    "                \"rows_file\":rows_this_file,  # all rows in df_sweep9 for this filename\n",
    "                \"stim_t\":stim_t,\n",
    "                \"hold_val\":hold_val,\n",
    "                \"digOutNum\":digOutNum,\n",
    "            })\n",
    "\n",
    "        # ──────────────────────────────────────────────\n",
    "        # SECOND PASS: actually do the plotting\n",
    "        # ──────────────────────────────────────────────\n",
    "        all_lines, all_labels = [], []\n",
    "        for info in subplot_info:\n",
    "            ax = info[\"ax\"]\n",
    "            fname = info[\"fname\"]\n",
    "            if not info.get(\"hasData\", False):\n",
    "                ax.set_xlim(0,1)\n",
    "                ax.set_ylim(-1,1)\n",
    "                ax.set_title(f\"{fname}\\n(No data for sweep=9)\")\n",
    "                continue\n",
    "\n",
    "            t_dom = info[\"t_dom\"]\n",
    "            i_dom = info[\"i_dom\"]\n",
    "            d_dom = info[\"d_dom\"]\n",
    "            rows_file = info[\"rows_file\"]\n",
    "            stim_t = info[\"stim_t\"]\n",
    "\n",
    "            # Plot\n",
    "            ln = ax.plot(t_dom, i_dom, label=f\"{fname}\")\n",
    "            lines, labels = ax.get_legend_handles_labels()\n",
    "            all_lines.extend(lines)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "            ax.fill_between(\n",
    "                t_dom,\n",
    "                i_dom.min(), i_dom.max(),\n",
    "                where=(d_dom>0.5),\n",
    "                color=\"blue\",  # or use row color if you prefer\n",
    "                alpha=0.2\n",
    "            )\n",
    "\n",
    "            if not np.isnan(stim_t):\n",
    "                if global_t_min < stim_t < global_t_max:\n",
    "                    ax.axvspan(stim_t, stim_t+0.001, color=\"gray\", alpha=0.2)\n",
    "\n",
    "            ax.set_xlim(global_t_min, global_t_max)\n",
    "            ax.set_ylim(global_y_bottom, global_y_top)\n",
    "            ax.set_title(f\"{fname}\\n(Sweep 9)\")\n",
    "\n",
    "            # If you want to mark single-peak onset/peak from the first row\n",
    "            row0 = rows_file.iloc[0]\n",
    "            onset_t = row0.get(\"RefinedOnset_s\", np.nan)\n",
    "            peak_t  = row0.get(\"PeakTime_s\",    np.nan)\n",
    "\n",
    "            def plot_marker(mtime, color_marker, label_marker):\n",
    "                if np.isnan(mtime):\n",
    "                    return\n",
    "                if not (global_t_min <= mtime <= global_t_max):\n",
    "                    return\n",
    "                idx_c = np.argmin(np.abs(t_dom - mtime))\n",
    "                ax.plot(t_dom[idx_c], i_dom[idx_c],\n",
    "                        marker=\"o\",\n",
    "                        ms=7,\n",
    "                        mfc=color_marker,\n",
    "                        mec=\"k\",\n",
    "                        label=label_marker)\n",
    "            plot_marker(onset_t, \"green\", \"Onset\")\n",
    "            plot_marker(peak_t,  \"red\",   \"Peak\")\n",
    "\n",
    "            # If you want to overlay a fitted decay from row0\n",
    "            popt = row0.get(\"FitParams\", None)\n",
    "            decay_start = row0.get(\"DecayStart_s\", None)\n",
    "            decay_end   = row0.get(\"DecayEnd_s\", None)\n",
    "            if (isinstance(popt,(list,np.ndarray))\n",
    "                and not np.any(np.isnan(popt))\n",
    "                and decay_start is not None \n",
    "                and decay_end   is not None):\n",
    "                fit_t = np.linspace(decay_start, decay_end, 200)\n",
    "                fit_x = fit_t - decay_start\n",
    "                fit_y = biexponential_fit(fit_x,*popt)\n",
    "                mask_ = (fit_t>=global_t_min)&(fit_t<=global_t_max)\n",
    "                ax.plot(fit_t[mask_], fit_y[mask_], \"k--\", lw=2)\n",
    "\n",
    "        used = set()\n",
    "        unique_lines, unique_labels = [], []\n",
    "        for l, lb in zip(all_lines, all_labels):\n",
    "            if lb not in used:\n",
    "                unique_lines.append(l)\n",
    "                unique_labels.append(lb)\n",
    "                used.add(lb)\n",
    "\n",
    "        fig.suptitle(f\"Sweep=9 Plots (Batch {fig_index})\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        out_name = f\"sweep9_batch{fig_index}.jpg\"\n",
    "        out_path = os.path.join(output_dir, out_name)\n",
    "        plt.savefig(out_path, dpi=150)\n",
    "        plt.close(fig)\n",
    "\n",
    "    # Main loop: chunk the filenames into groups of up to 10\n",
    "    filenames_per_fig = 10\n",
    "    total_files = len(filenames_sweep9)\n",
    "    n_figs = (total_files // filenames_per_fig) + (1 if total_files % filenames_per_fig else 0)\n",
    "    for fig_i in range(n_figs):\n",
    "        batch_fnames = filenames_sweep9[fig_i*filenames_per_fig : (fig_i+1)*filenames_per_fig]\n",
    "        plot_batch_of_filenames(batch_fnames, fig_index=fig_i+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0957d11-fdae-44d3-a450-dd8db69449a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_results is None:\n",
    "    df_results = pd.read_csv(\"./sorted_directory/analysis_results.csv\")\n",
    "plot_sweep9_for_multiple_files_5x2(df_results, abf_root, \"./sorted_directory/Sweep9_plots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3779a6b-539a-43f0-a6c4-7ad2377ff7ff",
   "metadata": {},
   "source": [
    "# sweepごとにasynchronous EPSC解析\n",
    "\n",
    "## `measure_sweep_multipeaks()`関数\n",
    "\n",
    "### 関数の概要\n",
    "\n",
    "`measure_sweep_multipeaks()` は、1つのスイープ（sweep）データから複数のピーク（イベント）を検出するための関数です。刺激タイミング（stimulus time）以降、指定した時間窓（`post_stim_window_sec`）内でピーク探索を行い、そのピークの時刻・振幅などをまとめて返します。主なステップは以下の通りです。\n",
    "\n",
    "1. **刺激タイミングの取得 (`find_stim_time_digital`)**  \n",
    "   デジタル信号から刺激が入った時刻 (`stim_time_s`) を取得し、それを中心として解析します。もし刺激が無い（`None`）場合はイベント検出を行わず空リストを返します。\n",
    "\n",
    "2. **ベースライン補正**  \n",
    "   刺激直前の短い時間ウィンドウ（`baseline_window_sec`）で電流値を補正し、基線を 0 に合わせます。\n",
    "\n",
    "3. **ポスト刺激区間の切り出し**  \n",
    "   刺激時刻から `post_stim_window_sec` 後までの区間を抽出し、その間だけピーク検出を行います。区間が見つからない場合は空リストを返します。\n",
    "\n",
    "4. **極性の考慮 (inward / outward)**  \n",
    "   保持電位（`voltage_hold`）が 10 mV 未満ならば、ピークは陰性（inward current）とみなして波形を反転させ、正側のピークとして検出できるようにします。10 mV 以上の場合はそのままピーク検出を行います。\n",
    "\n",
    "5. **ピーク検出 (`scipy.signal.find_peaks`)**  \n",
    "   - `height`: ピークの最小高さ  \n",
    "   - `distance`: ピーク間の最小間隔（サンプル数に変換）  \n",
    "   - `width`: ピークの最小幅（ミリ秒指定をサンプル数に変換）  \n",
    "   - `prominence`: ピークのプロミネンス（局所的にどれだけ突出しているか）  \n",
    "   これらを指定して `find_peaks` を呼び出します。コード内では、ピークの絶対量 (`peak_amp`) に応じて閾値を動的に設定する処理が含まれています。\n",
    "\n",
    "6. **検出結果の格納**  \n",
    "   見つかったピークのインデックス（`peaks`）ごとに、ピーク時刻（`EventTime_s`）、ピーク振幅（`EventAmp_pA`）などの情報を辞書形式でまとめ、リストとして返します。\n",
    "\n",
    "---\n",
    "\n",
    "### 引数の説明\n",
    "\n",
    "- **abf (pyabf.ABF)**  \n",
    "  解析対象となる ABF データを表すオブジェクト。スイープ設定や電流値の取得に利用します。\n",
    "\n",
    "- **sweep_index (int)**  \n",
    "  処理対象とするスイープの番号。\n",
    "\n",
    "- **color (str)**  \n",
    "  刺激を検出するデジタルチャンネルを決めるための色指定。通常 `\"blue\"` か `\"red\"` などで分けます。\n",
    "\n",
    "- **rec_chan (int)**  \n",
    "  記録チャネルのインデックス。ABF ファイル内に複数チャネルがある場合の選択に使います。\n",
    "\n",
    "- **baseline_window_sec (float)**  \n",
    "  ベースライン補正に使用する、刺激前の時間（秒）。この区間の平均値を差し引きます。\n",
    "\n",
    "- **voltage_hold (float)**  \n",
    "  保持電位 (mV)。これが 10 mV 未満だと陰性ピーク（内向き電流）とみなして波形を反転。\n",
    "\n",
    "- **post_stim_window_sec (float)**  \n",
    "  刺激後、ピーク検出を行う時間幅（秒）。例えば 0.25 であれば刺激後 250 ms の間だけ検出する。\n",
    "\n",
    "- **min_peak_height (float)**  \n",
    "  ピークとして検出する際に必要な最小高さ (pA)。ただし関数内で動的に更新される場合あり。\n",
    "\n",
    "- **min_peak_distance (float)**  \n",
    "  ピーク同士の最小間隔 (秒) で、`find_peaks` に与える `distance` パラメータに変換。\n",
    "\n",
    "- **threshold_val, width_ms**  \n",
    "  `find_peaks` の `threshold` / `width` の調整用パラメータ。コード内で適宜変換されて使われます。\n",
    "\n",
    "---\n",
    "\n",
    "### 戻り値\n",
    "\n",
    "- **List[dict]**  \n",
    "  各ピークイベントを表す辞書のリスト。以下のようなキーを含みます。\n",
    "  - `\"Sweep\"`: スイープ番号\n",
    "  - `\"EventIndex\"`: 検出されたピークの通し番号\n",
    "  - `\"EventTime_s\"`: ピークが起きた絶対時刻（秒）\n",
    "  - `\"EventAmp_pA\"`: ピークの振幅（pA）\n",
    "  - `\"StimTime_s\"`: 刺激時刻（秒）\n",
    "  - `\"VoltageHold\"`: 保持電位\n",
    "  - など…\n",
    "\n",
    "ピークが一つも見つからなかった場合は空のリスト `[]` を返します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1322910-52e6-4d77-b84b-0e3ed7041b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyabf\n",
    "import pyabf.filter\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.optimize import curve_fit\n",
    "#################################\n",
    "# Multi-peak measurement (per sweep)\n",
    "#################################\n",
    "def measure_sweep_multipeaks(\n",
    "    abf,\n",
    "    sweep_index,\n",
    "    color=\"blue\",\n",
    "    rec_chan=0,\n",
    "    baseline_window_sec=0.01,\n",
    "    voltage_hold=-55.0,\n",
    "    post_stim_window_sec=0.250,\n",
    "    min_peak_height=10.0,\n",
    "    min_peak_distance=0.002,\n",
    "    threshold_val = 10,\n",
    "    width_ms = 2\n",
    "):\n",
    "    \"\"\"\n",
    "    Identify *multiple* peak events in a post-stim window.\n",
    "    Returns a list of dicts (one for each event).\n",
    "    \n",
    "    Args:\n",
    "        abf: a pyabf.ABF object\n",
    "        sweep_index (int): which sweep to analyze\n",
    "        color (str): \"blue\"/\"red\" for digital channel detection\n",
    "        rec_chan (int): recording channel index\n",
    "        baseline_window_sec (float): how many seconds to subtract\n",
    "        voltage_hold (float): if <10 => inward (negative) events, else outward (positive)\n",
    "        post_stim_window_sec (float): how long after stimulus to search for events\n",
    "        min_peak_height (float): minimum absolute amplitude (pA) for detection\n",
    "        min_peak_distance (float): minimal time in seconds between peaks \n",
    "                                   (converted to # of points below).\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries, each dictionary describing one event:\n",
    "            [\n",
    "              {\n",
    "                \"Sweep\": sweep_index,\n",
    "                \"EventIndex\": 0,\n",
    "                \"EventTime_s\": absolute_time_of_peak,\n",
    "                \"EventAmp_pA\": peak_amplitude,\n",
    "                \"StimTime_s\": ...\n",
    "                ...\n",
    "              },\n",
    "              ...\n",
    "            ]\n",
    "        If no stimulus or no events, returns [].\n",
    "    \"\"\"\n",
    "    # 1) Stim time\n",
    "    stim_time_s = find_stim_time_digital(abf, sweep_index, color=color)\n",
    "    if stim_time_s is None:\n",
    "        return []  # no stim => no events\n",
    "\n",
    "    # 2) Baseline subtract\n",
    "    baseline_start_sec = stim_time_s - baseline_window_sec\n",
    "    baseline_end_sec = stim_time_s\n",
    "    abf.setSweep(sweepNumber=sweep_index, channel=rec_chan,\n",
    "                 baseline=[baseline_start_sec, baseline_end_sec])\n",
    "    current_bs = abf.sweepY\n",
    "    time_s = abf.sweepX\n",
    "    \n",
    "\n",
    "    # 3) Define the post-stim window for searching peaks\n",
    "    window_start = stim_time_s\n",
    "    window_end   = stim_time_s + post_stim_window_sec\n",
    "    mask = (time_s >= window_start) & (time_s <= window_end)\n",
    "    if not np.any(mask):\n",
    "        return []\n",
    "        \n",
    "    t_post = time_s[mask]\n",
    "    c_post = current_bs[mask]\n",
    "\n",
    "    # 4) Negative vs positive peak\n",
    "    if voltage_hold < 10:\n",
    "        peak_idx = np.argmin(c_post)\n",
    "        expect_inward = True\n",
    "    else:\n",
    "        peak_idx = np.argmax(c_post)\n",
    "        expect_inward = False\n",
    "\n",
    "    # 4) Decide if we look for negative or positive peaks\n",
    "    #    If voltage_hold<10, we expect negative/inward, so let's invert.\n",
    "    if voltage_hold < 10:\n",
    "        # inward = negative => invert to find \"positive\" peaks\n",
    "        data_for_find_peaks = -c_post\n",
    "        # also invert min_peak_height\n",
    "    else:\n",
    "        data_for_find_peaks = c_post\n",
    "\n",
    "    # Possibly reduce smoothing:\n",
    "    #data_for_find_peaks_filtered = savgol_filter(data_for_find_peaks, window_length=101, polyorder=3)\n",
    "    data_for_find_peaks_filtered = data_for_find_peaks\n",
    "    \n",
    "    # Dynamically scale thresholds based on the biggest peak in that window:\n",
    "    peak_amp = abs(data_for_find_peaks[peak_idx])\n",
    "    if voltage_hold < 10:\n",
    "        min_peak_height = max(10, 0.05 * peak_amp)\n",
    "        prominence_val = min(5, 0.05 * peak_amp)  # at least 5 pA or 5% of big peak\n",
    "    else:\n",
    "        min_peak_height = max(20, 0.2 * peak_amp)\n",
    "        prominence_val = min(10, 0.1 * peak_amp)  # at least 5 pA or 5% of big peak\n",
    "    \n",
    "    # Let peaks be closer in time:\n",
    "    dt = time_s[1] - time_s[0]\n",
    "    min_distance_pts = int(0.005 // dt)  # 2 ms if you want to detect very fast multi-peaks\n",
    "    width_pts = int(width_ms /1000 // dt)\n",
    "    \n",
    "    peaks, props = find_peaks(\n",
    "        data_for_find_peaks_filtered,\n",
    "        height = min_peak_height,\n",
    "        #threshold=min_peak_threshold,\n",
    "        distance=min_distance_pts,\n",
    "        width = width_pts,\n",
    "        prominence=prominence_val,\n",
    "        rel_height=1\n",
    "    )\n",
    "\n",
    "    # If no peaks, return []\n",
    "    if len(peaks) == 0:\n",
    "        return []\n",
    "\n",
    "    # 6) For each peak index, store amplitude/time\n",
    "    event_dicts = []\n",
    "    for i_evt, p_idx in enumerate(peaks):\n",
    "        # p_idx is index within t_post/c_post\n",
    "        # amplitude is c_post[p_idx]\n",
    "        peak_amp = c_post[p_idx]\n",
    "        peak_time_rel = t_post[p_idx] - stim_time_s\n",
    "        peak_time_abs = t_post[p_idx]\n",
    "\n",
    "        # optional: measure decay or do a smaller \"local\" fit\n",
    "        # for brevity we'll skip the advanced steps.\n",
    "        # We'll just store the amplitude/time in a dictionary.\n",
    "        event_dict = {\n",
    "            \"Sweep\": sweep_index,\n",
    "            \"EventIndex\": i_evt,\n",
    "            \"EventTime_s\": peak_time_abs,\n",
    "            \"EventAmp_pA\": peak_amp,\n",
    "            \"StimTime_s\": stim_time_s,\n",
    "            \"VoltageHold\": voltage_hold,\n",
    "            \"BaselineStart_s\": baseline_start_sec,\n",
    "            \"BaselineEnd_s\": baseline_end_sec,\n",
    "        }\n",
    "        event_dicts.append(event_dict)\n",
    "\n",
    "    return event_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f600af32-6bf7-44f7-9be2-a8e5796d15b0",
   "metadata": {},
   "source": [
    "## `analyze_evoked_responses_multipeaks(abf_path, color=\"blue\", rec_chan=0, voltage_hold=-55.0, post_stim_window_sec=0.500)`\n",
    "\n",
    "この関数は、1つの ABF ファイルを開き、すべてのスイープ（`abf.sweepList`）を順に処理します。各スイープに対して `measure_sweep_multipeaks()` を呼び出し、検出されたすべてのイベント（ピーク）をまとめます。その結果を一つの DataFrame として返します。\n",
    "\n",
    "### 主な処理の流れ\n",
    "\n",
    "1. **ABF ファイルの読み込み**  \n",
    "   `pyabf.ABF(abf_path)` を使って ABF オブジェクトを作成します。\n",
    "\n",
    "2. **フィルタの適用**  \n",
    "    `pyabf.filter.gaussian(abf, 0) pyabf.filter.gaussian(abf, 1)`\n",
    "\n",
    "   ここでは最初に `sigma=0` で既存のフィルタを解除し、その後 `sigma=1` のガウシアンフィルタをかける例になっています。\n",
    "\n",
    "4. **スイープごとのイベント解析**  \n",
    "`for sweep_index in abf.sweepList:` のループで各スイープに対して `measure_sweep_multipeaks()` を呼び出し、得られたイベント情報を `all_events` リストに追記します。\n",
    "\n",
    "5. **DataFrame に変換**  \n",
    "最終的に `all_events`（辞書のリスト）を `pd.DataFrame(all_events)` によって整形し、返します。\n",
    "\n",
    "---\n",
    "\n",
    "## `analyze_abf_files_multipeaks(df_filtered, abf_root)`\n",
    "\n",
    "こちらの関数は、複数の ABF ファイルや複数の設定条件（色/電位など）を含む DataFrame（`df_filtered`）をもとに、各行（各条件）に応じた ABF ファイルを探し出して解析を行います。\n",
    "### 主な処理の流れ\n",
    "1. **ABF ファイルパスの辞書作成**  \n",
    "`abf_dict = find_abf_files(abf_root)` で、ディレクトリ下のすべての ABF ファイルを探して、ファイル名 → 絶対パスの対応を作ります。\n",
    "\n",
    "2. **df_filtered の各行についてループ**  \n",
    "それぞれの行には `filename`、`Color`、`VoltageHold` などの情報が含まれます。  \n",
    "- `abf_name = row[\"filename\"]` で ABF ファイル名を取得  \n",
    "- `row_color` や `row_voltage` で解析条件を取得\n",
    "\n",
    "3. **`analyze_evoked_responses_multipeaks()` の呼び出し**  \n",
    "得られた `abf_path`、`row_color`、`row_voltage` を使って、先述の `analyze_evoked_responses_multipeaks()` を実行します。\n",
    "\n",
    "4. **結果のマージ**  \n",
    "`df_events`（マルチピーク解析結果）に対し、元の `df_filtered` の行のメタデータ（BrainID や Region など）を結合して `results_list` に格納します。\n",
    "\n",
    "5. **DataFrame で返す**  \n",
    "最後に、すべてのマルチピーク解析結果をまとめたリストを `pd.DataFrame(results_list)` として返します。\n",
    "\n",
    "---\n",
    "\n",
    "### 使い所\n",
    "\n",
    "- **単一の ABF ファイルを複数のスイープで解析したい場合**  \n",
    "`analyze_evoked_responses_multipeaks()` を呼べば、1 ファイルあたりのすべてのイベント検出結果が得られます。\n",
    "\n",
    "- **多数のファイル・多数のスライス条件をまとめて処理したい場合**  \n",
    "`analyze_abf_files_multipeaks()` に、ABF ファイル名や保持電位、色の情報を含む DataFrame を与えると、複数条件を一括でマルチピーク解析できます。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840e6df8-0f04-4ab7-87ec-886ca6523510",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Analyzing an ABF with multi-peaks\n",
    "###################################\n",
    "def analyze_evoked_responses_multipeaks(\n",
    "    abf_path, \n",
    "    color=\"blue\", \n",
    "    rec_chan=0, \n",
    "    voltage_hold=-55.0,\n",
    "    post_stim_window_sec=0.500\n",
    "):\n",
    "    \"\"\"\n",
    "    Loop over all sweeps in the ABF file, measure *multiple* peaks if present.\n",
    "    Returns a DataFrame with multiple rows per sweep if multiple peaks are detected.\n",
    "    \"\"\"\n",
    "    abf = pyabf.ABF(abf_path)\n",
    "    pyabf.filter.gaussian(abf, 0)  # remove old filter\n",
    "    pyabf.filter.gaussian(abf, 1)  # apply custom sigma\n",
    "    all_events = []\n",
    "\n",
    "    for sweep_index in abf.sweepList:\n",
    "        events = measure_sweep_multipeaks(\n",
    "            abf=abf,\n",
    "            sweep_index=sweep_index,\n",
    "            color=color,\n",
    "            rec_chan=rec_chan,\n",
    "            baseline_window_sec=0.01,\n",
    "            voltage_hold=voltage_hold,\n",
    "            post_stim_window_sec=post_stim_window_sec\n",
    "        )\n",
    "        # events is a list of dictionaries; each dictionary = one event\n",
    "        all_events.extend(events)\n",
    "\n",
    "    return pd.DataFrame(all_events)\n",
    "\n",
    "#################################\n",
    "# Putting it all together\n",
    "#################################\n",
    "def analyze_abf_files_multipeaks(df_filtered, abf_root):\n",
    "    \"\"\"\n",
    "    Similar to 'analyze_abf_files' but calls analyze_evoked_responses_multipeaks().\n",
    "    Each row in df_filtered => analyze that ABF with that color/hold.\n",
    "    We may produce multiple events per sweep => final DataFrame has many rows.\n",
    "    \"\"\"\n",
    "    abf_dict = find_abf_files(abf_root)  # short name -> full path\n",
    "    results_list = []\n",
    "\n",
    "    for idx, row in df_filtered.iterrows():\n",
    "        abf_name = row[\"filename\"]  # e.g. \"24n15005.abf\"\n",
    "        if abf_name not in abf_dict:\n",
    "            print(f\"[WARNING] Cannot find {abf_name} in {abf_root}, skipping...\")\n",
    "            continue\n",
    "        abf_path = abf_dict[abf_name]\n",
    "\n",
    "        row_color = row.get(\"Color\", \"blue\")\n",
    "        row_voltage = row.get(\"VoltageHold\", -55)\n",
    "\n",
    "        # Analyze for multi-peaks\n",
    "        df_events = analyze_evoked_responses_multipeaks(\n",
    "            abf_path=abf_path,\n",
    "            color=row_color,\n",
    "            rec_chan=0,\n",
    "            voltage_hold=row_voltage,\n",
    "            post_stim_window_sec=0.500\n",
    "        )\n",
    "        # df_events has columns like Sweep, EventIndex, EventTime_s, EventAmp_pA, etc.\n",
    "\n",
    "        # For each event, merge with row metadata:\n",
    "        for _, evt in df_events.iterrows():\n",
    "            row_dict = {\n",
    "                \"index_df_filtered\": idx,\n",
    "                \"filename\": abf_name,\n",
    "                # carry over user columns\n",
    "                \"Opsin\":    row.get(\"Opsin\", None),\n",
    "                \"Region\":   row.get(\"Region\", None),\n",
    "                \"BrainID\":  row.get(\"BrainID\", None),\n",
    "                \"SliceID\":  row.get(\"SliceID\", None),\n",
    "                \"CellID\":   row.get(\"CellID\", None),\n",
    "                \"DrugList\": row.get(\"DrugList\", None),\n",
    "                \"APregion\": row.get(\"APregion\", None),\n",
    "                \"RoughAP\":  row.get(\"RoughAP\", None),\n",
    "            }\n",
    "            # add event columns\n",
    "            row_dict.update(evt.to_dict())\n",
    "\n",
    "            results_list.append(row_dict)\n",
    "\n",
    "    return pd.DataFrame(results_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6254aa-aa6e-45ac-83f9-0dd584b9edb3",
   "metadata": {},
   "source": [
    "## sweepごとにasynchronous EPSC解析処理段階"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52e2692-dcda-499a-9eda-b28444f04419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) Suppose we already have df_filtered\n",
    "if df_filtered is None:\n",
    "    df_filtered = pd.read_csv(\"./sorted_directory/master_with_AP_filtered.csv\")\n",
    "\n",
    "\n",
    "# 1) Path where ABF files live (somewhere in subfolders)\n",
    "abf_root = \"./sorted_directory\"\n",
    "\n",
    "# 2) Analyze\n",
    "df_results_multipeaks = analyze_abf_files_multipeaks(df_filtered, abf_root)\n",
    "\n",
    "# 3) Check/save results\n",
    "df_results_multipeaks.to_csv(\"./sorted_directory/analysis_multipeaks_results.csv\", index=False)\n",
    "df_results_multipeaks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583cb7b7-e95a-40d8-85dc-accaae0ba667",
   "metadata": {},
   "source": [
    "# Asynchronous EPSC peak確認plot\n",
    "\n",
    "## `plot_evoked_sweeps_5x2`説明\n",
    "この関数は、単一の ABF ファイル（例: `\"24n15005.abf\"`)に含まれる複数スイープの記録波形を、5行×2列（最大10スイープ）で図示するためのものです。以下の特徴を持ちます:\n",
    "\n",
    "1. **ドメインの制限**  \n",
    "   それぞれのスイープに対して、刺激 (StimTime) から前後 [−0.1 秒, +0.5 秒] の範囲のみを抽出し、波形をプロットします。  \n",
    "   もし StimTime が存在しない場合 (NaN の場合) は、スイープの全範囲をプロットします。\n",
    "\n",
    "2. **デジタル信号の可視化 (shade)**  \n",
    "   `abf.sweepD(digOutNum)` を用いて取り出したデジタル出力が 1 の区間を、該当スイープ波形の最小値から最大値に渡って色を塗りつぶし (fill_between)、視覚的に刺激タイミングを示します。\n",
    "\n",
    "3. **Stimulus のマーク**  \n",
    "   StimTime が有効な場合、その時点に薄い灰色の帯 (`ax.axvspan`) をプロットし、刺激が入ったタイミングを示します。\n",
    "\n",
    "4. **単一ピーク解析の結果のマーカー**  \n",
    "   引数で与えられる `df_results` (単一ピーク解析などの結果) から、  \n",
    "   - `RefinedOnset_s` (発現時刻)  \n",
    "   - `PeakTime_s` (ピーク時刻)  \n",
    "   を取得し、該当時刻をプロット上で緑や赤のマーカーとして描画します。\n",
    "\n",
    "5. **マルチピーク解析の結果のマーカー**  \n",
    "   引数 `df_events` (マルチピーク解析の結果) から、  \n",
    "   - `EventTime_s` (イベント時刻)  \n",
    "   - `EventAmp_pA` (イベント振幅)  \n",
    "   を取得し、ダイヤ型（`marker=\"D\"`, 色はマゼンタ）として複数のイベントをプロットに重ねます。\n",
    "\n",
    "6. **全サブプロットでの共通スケール**  \n",
    "   全スイープについて、まずは各スイープごとに局所的な x・y 範囲を見積もり、その最小値・最大値を集めて「グローバルな最小・最大」を決めます。  \n",
    "   そして 2 回目のループで、すべてのサブプロットに対して同じ `xlim`・`ylim` を設定して統一的に表示します。\n",
    "\n",
    "7. **レジェンド**  \n",
    "   全てのサブプロットから集めた凡例要素（`all_lines` と `all_labels`）を元に、重複しない形でまとめることができます。  \n",
    "   ※ ここではコメントアウトされていますが、`fig.legend` を使うことで図全体のレジェンドをまとめて表示可能です。\n",
    "\n",
    "---\n",
    "\n",
    "### 関数引数\n",
    "\n",
    "- `filename (str)`  \n",
    "  解析・描画対象の ABF ファイル名 (例: `\"24n15005.abf\"`)\n",
    "\n",
    "- `df_results (DataFrame)`  \n",
    "  単一ピーク解析やメインピーク解析結果の DataFrame。  \n",
    "  該当ファイル・スイープに対して、  \n",
    "  - `Sweep`,  \n",
    "  - `RefinedOnset_s`,  \n",
    "  - `PeakTime_s`,  \n",
    "  - `FitParams`  \n",
    "  などの列を持つ。\n",
    "\n",
    "- `df_events (DataFrame)`  \n",
    "  マルチピーク解析結果の DataFrame。  \n",
    "  - `Sweep`,  \n",
    "  - `EventTime_s`,  \n",
    "  - `EventAmp_pA`  \n",
    "  などの列を持つ。\n",
    "\n",
    "- `abf_root (str)`  \n",
    "  ABF ファイルが存在するディレクトリへのパス。  \n",
    "  関数内部で `find_abf_files(abf_root)` を呼び、ファイル名→フルパスの辞書を作る。\n",
    "\n",
    "---\n",
    "\n",
    "### 処理の流れ\n",
    "\n",
    "1. **ABF ファイルの存在確認**  \n",
    "   `find_abf_files(abf_root)` でファイル名からパスを探し、該当するかチェック。見つからなければエラーを表示して終了。\n",
    "\n",
    "2. **ABF ファイルの読み込み & フィルタ処理**  \n",
    "   `abf = pyabf.ABF(abf_path)` でロードし、\n",
    "\n",
    "   `pyabf.filter.gaussian(abf, 0) pyabf.filter.gaussian(abf, 1)`\n",
    "    のようにガウシアンフィルタの適用例が記述されている。\n",
    "\n",
    "4. **データの絞り込みと下準備**  \n",
    "- `df_sub_main` : 該当ファイルに対する単一ピーク解析結果を抽出  \n",
    "- `df_sub_evts` : 該当ファイルに対するマルチピーク解析結果を抽出  \n",
    "- `sweep_nums` : プロット対象となるスイープ番号（ソート済み）\n",
    "\n",
    "4. **サブプロットの準備 (5×2 = 10)**  \n",
    "`fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(10, 15))` で 10 面までプロット可能。\n",
    "\n",
    "5. **第一段階: x軸・y軸の全体範囲を決める**  \n",
    "各スイープに対して刺激時刻をもとに `[stimTime - 0.1, stimTime + 0.5]` の区間だけ抜き出し、最小値・最大値を計算。  \n",
    "全スイープの合計で `global_t_min, global_t_max, global_y_bottom, global_y_top` を決定する。\n",
    "\n",
    "6. **第二段階: 実際にプロット**  \n",
    "- 抜き出した区間の波形 (`t_dom` vs `i_dom`) をプロット  \n",
    "- デジタル出力 (`d_dom`) が 1 の区間を `fill_between` で着色  \n",
    "- StimTime (灰色の帯)  \n",
    "- 単一ピークの Onset (緑) / Peak (赤)  \n",
    "- マルチピークのイベント (マゼンタのダイヤマーカー)  \n",
    "- フィットされた減衰カーブ (二重指数関数など) を描画\n",
    "\n",
    "7. **レジェンド・タイトル・レイアウト**  \n",
    "- 重複しない形のレジェンドエントリを作成  \n",
    "- `fig.suptitle` で全体タイトルを設定  \n",
    "- `plt.tight_layout()` でサブプロット間のレイアウトを整える\n",
    "\n",
    "---\n",
    "\n",
    "### 注意点\n",
    "\n",
    "- 単一ピーク解析 (`df_results`) とマルチピーク解析 (`df_events`) は別々の DataFrame を受け取る設計。  \n",
    "- スイープごとに複数行ある場合、実装次第で複数マーカーが表示される可能性がある。  \n",
    "- フィルタ (`pyabf.filter.gaussian`) の効果やパラメータは例示であり、実際の解析要件に合わせる必要がある。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f7961b-eb67-47c6-b2dc-501c1d5e1c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pyabf\n",
    "import pyabf.filter\n",
    "\n",
    "def plot_evoked_sweeps_5x2(filename, df_results, df_events, abf_root):\n",
    "    \"\"\"\n",
    "    - Plot each sweep in a separate subplot (up to 10 sweeps) arranged 5 rows × 2 columns.\n",
    "    - Restrict domain [stimTime - 0.1, stimTime + 0.5].\n",
    "    - Shade digital high times.\n",
    "    - Mark onset/peak times (if present in df_results).\n",
    "    - Mark multi-peak events from df_events using (EventTime_s, EventAmp_pA).\n",
    "    - Use a single legend for the entire figure.\n",
    "    - All subplots share the same x/y limits (taken from the min and max found among all sweeps).\n",
    "    \n",
    "    Args:\n",
    "        filename (str): e.g. \"24n15005.abf\"\n",
    "        df_results (DataFrame): single-peak or main-peak results \n",
    "                                (has columns like Sweep, RefinedOnset_s, PeakTime_s, etc.)\n",
    "        df_events (DataFrame): multi-peak results \n",
    "                               (has columns like Sweep, EventTime_s, EventAmp_pA, etc.)\n",
    "        abf_root (str): path containing the ABF files\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Dictionary of ABF files\n",
    "    abf_dict = find_abf_files(abf_root)\n",
    "\n",
    "    # 2) Get the ABF path\n",
    "    if filename not in abf_dict:\n",
    "        print(f\"[ERROR] ABF file not found: {filename}\")\n",
    "        return\n",
    "    abf_path = abf_dict[filename]\n",
    "\n",
    "    # 3) Load the ABF\n",
    "    abf = pyabf.ABF(abf_path)\n",
    "    pyabf.filter.gaussian(abf, 0)  # remove old filter\n",
    "    pyabf.filter.gaussian(abf, 1)  # apply custom sigma\n",
    "\n",
    "    # 4) Filter df_results for just this file\n",
    "    df_sub_main = df_results[df_results[\"filename\"] == filename].copy()\n",
    "    df_sub_evts = df_events[df_events[\"filename\"] == filename].copy()\n",
    "\n",
    "    sweep_nums = sorted(df_sub_main[\"Sweep\"].unique())  # or union of sweeps from both dataframes\n",
    "\n",
    "    # Create figure with 5x2 subplots (up to 10 sweeps)\n",
    "    fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(10, 15))\n",
    "    axes = axes.flatten()  # to index easily as a list\n",
    "\n",
    "    # We'll collect global domain/y-range as we go:\n",
    "    global_t_min = np.inf\n",
    "    global_t_max = -np.inf\n",
    "    global_y_bottom = np.inf\n",
    "    global_y_top = -np.inf\n",
    "\n",
    "    # We'll store info to plot in a second pass\n",
    "    subplot_info = []\n",
    "\n",
    "    # Collect legend handles/labels for a single legend at the end\n",
    "    all_lines = []\n",
    "    all_labels = []\n",
    "\n",
    "    # ─────────────────────────────────────────────────────────\n",
    "    # FIRST PASS: determine global axis limits\n",
    "    # ─────────────────────────────────────────────────────────\n",
    "    for i, sweep_num in enumerate(sweep_nums):\n",
    "        if i >= len(axes):\n",
    "            print(f\"[WARNING] More than {len(axes)} sweeps. Only plotting first {len(axes)}.\")\n",
    "            break\n",
    "\n",
    "        ax = axes[i]\n",
    "\n",
    "        # Grab one representative row from df_sub_main for that sweep\n",
    "        # (assuming each sweep has at least one row there)\n",
    "        row_main = df_sub_main[df_sub_main[\"Sweep\"] == sweep_num].iloc[0]\n",
    "\n",
    "        # Determine digital channel by color\n",
    "        row_color = row_main.get(\"Color\", \"blue\").lower()\n",
    "        if row_color == \"red\":\n",
    "            digOutNum = 0\n",
    "        elif row_color == \"blue\":\n",
    "            digOutNum = 3\n",
    "        else:\n",
    "            digOutNum = 3  # fallback\n",
    "\n",
    "        # Stim time for domain\n",
    "        stim_t = row_main.get(\"StimTime_s\", np.nan)\n",
    "        if not np.isnan(stim_t):\n",
    "            baseline_start_sec = stim_t - 0.01\n",
    "            baseline_end_sec   = stim_t\n",
    "        else:\n",
    "            baseline_start_sec = 0\n",
    "            baseline_end_sec   = 0.01\n",
    "\n",
    "        # Set sweep with baseline\n",
    "        abf.setSweep(\n",
    "            sweepNumber=sweep_num,\n",
    "            channel=0,\n",
    "            baseline=[baseline_start_sec, baseline_end_sec]\n",
    "        )\n",
    "        \n",
    "\n",
    "        time_s = abf.sweepX\n",
    "        current_pA = abf.sweepY\n",
    "        digital_sig = abf.sweepD(digOutNum)  # 0 or 1 array\n",
    "\n",
    "        # Domain [stimTime - 0.1, stimTime + 0.5], if stim_t is valid\n",
    "        if np.isnan(stim_t):\n",
    "            # If no stim time, just plot everything\n",
    "            t_dom = time_s\n",
    "            i_dom = current_pA\n",
    "            d_dom = digital_sig\n",
    "            local_t_min = t_dom.min()\n",
    "            local_t_max = t_dom.max()\n",
    "        else:\n",
    "            t_min = stim_t - 0.1\n",
    "            t_max = stim_t + 0.5\n",
    "            domain_mask = (time_s >= t_min) & (time_s <= t_max)\n",
    "            if not np.any(domain_mask):\n",
    "                continue\n",
    "            t_dom = time_s[domain_mask]\n",
    "            i_dom = current_pA[domain_mask]\n",
    "            d_dom = digital_sig[domain_mask]\n",
    "            local_t_min = t_min\n",
    "            local_t_max = t_max\n",
    "\n",
    "        curr_min = i_dom.min()\n",
    "        curr_max = i_dom.max()\n",
    "\n",
    "        hold_val = row_main.get(\"VoltageHold\", -55)\n",
    "        # Decide local y-limits\n",
    "        if hold_val < 10:\n",
    "            # negative\n",
    "            y_bottom = curr_min * 1.5\n",
    "            y_top    = 0.5 * curr_min * -1\n",
    "        else:\n",
    "            # positive\n",
    "            y_bottom = 0.5 * curr_max * -1\n",
    "            y_top    = curr_max * 1.5\n",
    "\n",
    "        if y_top <= y_bottom:\n",
    "            y_bottom, y_top = min(y_bottom, y_top), max(y_bottom, y_top)\n",
    "\n",
    "        # Update global domain/y-range\n",
    "        global_t_min = min(global_t_min, local_t_min)\n",
    "        global_t_max = max(global_t_max, local_t_max)\n",
    "        global_y_bottom = min(global_y_bottom, y_bottom)\n",
    "        global_y_top    = max(global_y_top, y_top)\n",
    "\n",
    "        # Store info for second pass\n",
    "        subplot_info.append({\n",
    "            \"ax\": ax,\n",
    "            \"sweep_num\": sweep_num,\n",
    "            \"row_color\": row_color,\n",
    "            \"stim_t\": stim_t,\n",
    "            \"t_dom\": t_dom,\n",
    "            \"i_dom\": i_dom,\n",
    "            \"d_dom\": d_dom,\n",
    "        })\n",
    "\n",
    "    # ─────────────────────────────────────────────────────────\n",
    "    # SECOND PASS: do the actual plotting with global limits\n",
    "    # ─────────────────────────────────────────────────────────\n",
    "    for info in subplot_info:\n",
    "        ax = info[\"ax\"]\n",
    "        sweep_num = info[\"sweep_num\"]\n",
    "        row_color = info[\"row_color\"]\n",
    "        stim_t    = info[\"stim_t\"]\n",
    "        t_dom     = info[\"t_dom\"]\n",
    "        i_dom     = info[\"i_dom\"]\n",
    "        d_dom     = info[\"d_dom\"]\n",
    "\n",
    "        # Plot data\n",
    "        ln = ax.plot(t_dom, i_dom, label=f\"Sweep {sweep_num}\")\n",
    "        lines, labels = ax.get_legend_handles_labels()\n",
    "        all_lines.extend(lines)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "        # Shade region where digital_sig == 1\n",
    "        ax.fill_between(\n",
    "            t_dom,\n",
    "            i_dom.min(),\n",
    "            i_dom.max(),\n",
    "            where=(d_dom > 0.5),\n",
    "            color=row_color,\n",
    "            alpha=0.2\n",
    "        )\n",
    "\n",
    "        # Stim mark\n",
    "        if not np.isnan(stim_t):\n",
    "            if global_t_min < stim_t < global_t_max:\n",
    "                ax.axvspan(stim_t, stim_t+0.001, color=\"gray\", alpha=0.2)\n",
    "\n",
    "        ax.set_xlim(global_t_min, global_t_max)\n",
    "        ax.set_ylim(global_y_bottom, global_y_top)\n",
    "        ax.set_title(f\"Sweep {sweep_num}\")\n",
    "\n",
    "        # ─────────────────────────────────────────────────\n",
    "        # 1) Mark single-peak onset/peak (from df_results if present)\n",
    "        # ─────────────────────────────────────────────────\n",
    "        #   *We can have multiple rows for the same sweep in df_sub_main,\n",
    "        #    but commonly you might store just one row per sweep in single-peak scenario.\n",
    "        df_sub_sweep_main = df_sub_main[df_sub_main[\"Sweep\"] == sweep_num]\n",
    "        if len(df_sub_sweep_main) > 0:\n",
    "            # Just use the first row (or loop, depending on your logic)\n",
    "            row_main = df_sub_sweep_main.iloc[0]\n",
    "\n",
    "            onset_t = row_main.get(\"RefinedOnset_s\", np.nan)\n",
    "            peak_t  = row_main.get(\"PeakTime_s\",    np.nan)\n",
    "\n",
    "            def plot_marker(ax, marker_time, color_marker, label_marker):\n",
    "                if np.isnan(marker_time):\n",
    "                    return\n",
    "                if not (global_t_min <= marker_time <= global_t_max):\n",
    "                    return\n",
    "                idx_closest = np.argmin(np.abs(t_dom - marker_time))\n",
    "                ax.plot(\n",
    "                    t_dom[idx_closest],\n",
    "                    i_dom[idx_closest],\n",
    "                    marker=\"o\",\n",
    "                    ms=7,\n",
    "                    mfc=color_marker,\n",
    "                    mec=\"k\",\n",
    "                    label=label_marker\n",
    "                )\n",
    "\n",
    "            plot_marker(ax, onset_t, \"green\", \"Onset\")\n",
    "            plot_marker(ax, peak_t,  \"red\",   \"Peak\")\n",
    "\n",
    "            # Possibly overlay the fitted decay\n",
    "            popt = row_main.get(\"FitParams\", None)\n",
    "            decay_start = row_main.get(\"DecayStart_s\", None)\n",
    "            decay_end   = row_main.get(\"DecayEnd_s\",   None)\n",
    "            if (isinstance(popt,(list,np.ndarray)) \n",
    "                and not np.any(np.isnan(popt))\n",
    "                and decay_start is not None\n",
    "                and decay_end   is not None):\n",
    "                fit_t = np.linspace(decay_start, decay_end, 200)\n",
    "                fit_x = fit_t - decay_start\n",
    "                fit_y = biexponential_fit(fit_x, *popt)\n",
    "                fit_mask = (fit_t >= global_t_min) & (fit_t <= global_t_max)\n",
    "                ax.plot(fit_t[fit_mask], fit_y[fit_mask], \"k--\", lw=2)\n",
    "\n",
    "        # ─────────────────────────────────────────────────\n",
    "        # 2) Mark multi-peak events (df_events, one row per event)\n",
    "        # ─────────────────────────────────────────────────\n",
    "        df_sub_sweep_evts = df_sub_evts[df_sub_evts[\"Sweep\"] == sweep_num]\n",
    "        for _, evt_row in df_sub_sweep_evts.iterrows():\n",
    "            evt_time = evt_row[\"EventTime_s\"]\n",
    "            evt_amp  = evt_row[\"EventAmp_pA\"]  # not always used, but you could check sign etc.\n",
    "\n",
    "            if np.isnan(evt_time):\n",
    "                continue\n",
    "            if not (global_t_min <= evt_time <= global_t_max):\n",
    "                continue\n",
    "\n",
    "            # find closest index in t_dom\n",
    "            idx_closest = np.argmin(np.abs(t_dom - evt_time))\n",
    "            ax.plot(\n",
    "                t_dom[idx_closest],\n",
    "                i_dom[idx_closest],\n",
    "                marker=\"D\",      # diamond shape\n",
    "                alpha = 0.3,\n",
    "                ms=6,\n",
    "                mfc=\"magenta\",   # fill color\n",
    "                mec=\"k\",         # edge color\n",
    "                label=\"Event\"\n",
    "            )\n",
    "\n",
    "    # Remove duplicates in legend\n",
    "    used = set()\n",
    "    unique_lines = []\n",
    "    unique_labels = []\n",
    "    for l, lb in zip(all_lines, all_labels):\n",
    "        if lb not in used:\n",
    "            unique_lines.append(l)\n",
    "            unique_labels.append(lb)\n",
    "            used.add(lb)\n",
    "\n",
    "    # Single legend for the entire figure\n",
    "    # (loc can be changed as you wish, e.g. \"upper right\")\n",
    "    #fig.legend(unique_lines, unique_labels, loc=\"upper center\", ncol=5)\n",
    "\n",
    "    fig.suptitle(f\"Evoked Responses (5x2 grid): {filename}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae3e839-4742-42ea-9215-0ef0a5e59ddd",
   "metadata": {},
   "source": [
    "## 関数呼び出してplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ea50df-87a8-43e7-9725-415e119aa003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) Suppose we already have df_filtered\n",
    "if df_results is None:\n",
    "    df_results = pd.read_csv(\"./sorted_directory/analysis_results.csv\")\n",
    "if df_results_multipeaks is None:\n",
    "    df_results_multipeaks = pd.read_csv(\"./sorted_directory/analysis_multipeaks_results.csv\")\n",
    "df_results = pd.read_csv(\"./sorted_directory/analysis_results_ChR2.csv\")\n",
    "\n",
    "# 1) Path where ABF files live (somewhere in subfolders)\n",
    "abf_root = \"./sorted_directory\"\n",
    "\n",
    "plot_evoked_sweeps_5x2(\"24d20009.abf\", df_results, df_results_multipeaks, abf_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74ce636-5f51-4e03-8ebc-10c009cf3458",
   "metadata": {},
   "source": [
    "# df_pairs作成(-55 mVと+10 mVデータを集める)\n",
    "\n",
    "1) **最初の準備**  \n",
    "   - `df_filtered_indexreset` は、もとの `df_filtered` に行インデックス列（`row_index`）を追加した新しい表です。  \n",
    "   - ここでは、`df_filtered` をリセットして `row_index` という列を作成しておき、後の処理で「どの行が先に出てきたか」を判断できるようにしています。  \n",
    "\n",
    "2) **電位の違いによる分割**  \n",
    "   - `df_minus55` は、`df_filtered_indexreset` の中で `VoltageHold` が -55 の行だけを取り出したものです。  \n",
    "   - `df_plus10` は、`VoltageHold` が 10 の行だけを取り出したものです。  \n",
    "   - これにより、-55 のデータと +10 のデータを別々のテーブルとして扱えるようになります。  \n",
    "\n",
    "3) **マッチング条件の指定**  \n",
    "   - `merge_cols` には、`Opsin` や `Region`、`BrainID`、`SliceID`、`CellID`、`Color`、`StimPower`、`StimDuration` など、「一致していてほしい列名」が入っています。  \n",
    "   - 後の結合（マージ）で、このリストにある列が同じ値の行どうしを組み合わせる、という指定をしています。  \n",
    "\n",
    "4) **マージ（結合）の実行**  \n",
    "   - `df_joined` は、`df_minus55` と `df_plus10` を結合した結果です。  \n",
    "   - ここで、`on=merge_cols` とすることで、両方のテーブルに含まれる指定の列がすべて同じ値の行どうしが結合されます。  \n",
    "   - `suffixes=(\"_minus55\", \"_plus10\")` により、同じ列名が衝突した場合に後ろにつける文字列を指定しています（-55 側の列には `_minus55`、+10 側には `_plus10`）。  \n",
    "   - `how=\"inner\"` は、両方に共通して存在する行だけ残す結合方法を意味します。  \n",
    "\n",
    "5) **行インデックスによるフィルタ**  \n",
    "   - 結合後の `df_joined` には、それぞれの行に対応する `row_index_minus55` と `row_index_plus10` が含まれます。  \n",
    "   - `df_joined[\"row_index_minus55\"] < df_joined[\"row_index_plus10\"]` という条件で、-55 側の行が先に出現し、そのあとに +10 側の行が続く（インデックスが大きい）ケースだけを残しています。  \n",
    "   - これにより、「同じ条件の -55 の記録が、あとから出てくる +10 の記録より前にある」ペアだけを取得することができます。  \n",
    "\n",
    "6) **必要な列を抜き出して df_pairs にまとめる**  \n",
    "   - 最終的に `df_pairs` は、`df_joined` から必要な列だけを取り出したものです。  \n",
    "   - `filename_minus55` や `filename_plus10` などを同時に保持するので、-55 側と +10 側の記録ファイルがどのようにペアになっているかがひと目でわかります。  \n",
    "   - こうすることで、同じパラメータ（`Opsin` や `BrainID` など）でありつつ、`VoltageHold` の異なる 2 行が 1 行としてペア化されます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9330b5-6369-4e52-9573-cb68d9734ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 0) Suppose we already have df_filtered\n",
    "if df_filtered is None:\n",
    "    df_filtered = pd.read_csv(\"./sorted_directory/master_with_AP_filtered.csv\")\n",
    "if df_results is None:\n",
    "    df_results = pd.read_csv(\"./sorted_directory/analysis_results.csv\")\n",
    "\n",
    "\n",
    "# Example DataFrame 'df' with columns like:\n",
    "# Opsin, Region, BrainID, SliceID, CellID, VoltageHold, filename, etc.\n",
    "\n",
    "# 0) Add a row index column\n",
    "df_filtered_indexreset = df_filtered.reset_index(drop=True)\n",
    "df_filtered_indexreset[\"row_index\"] = df_filtered_indexreset.index  # or use df.reset_index() to keep a built-in col\n",
    "\n",
    "# 1) Split\n",
    "df_minus55 = df_filtered_indexreset[df_filtered_indexreset[\"VoltageHold\"] == -55].copy()\n",
    "df_plus10  = df_filtered_indexreset[df_filtered_indexreset[\"VoltageHold\"] == 10].copy()\n",
    "\n",
    "# 2) Merge on matching columns (everything except VoltageHold)\n",
    "#    e.g. Opsin, Region, BrainID, SliceID, CellID, ...\n",
    "#    Add any columns you want to match exactly in the 'on' list.\n",
    "merge_cols = [\n",
    "    \"Opsin\",\"Region\",\"BrainID\",\"SliceID\",\"CellID\",\"Color\",\n",
    "    \"StimPower\",\"StimDuration\", \"APregion\", \"RoughAP\"\n",
    "]\n",
    "df_joined = pd.merge(\n",
    "    df_minus55,\n",
    "    df_plus10,\n",
    "    on=merge_cols,\n",
    "    suffixes=(\"_minus55\", \"_plus10\"),\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# 3) Filter so the -55 row index is smaller than the +10 row index\n",
    "df_joined = df_joined[\n",
    "    df_joined[\"row_index_minus55\"] < df_joined[\"row_index_plus10\"]\n",
    "]\n",
    "\n",
    "# 4) Now df_joined has pairs of rows that match on those columns\n",
    "#    (and the -55 row appears earlier than the +10 row).\n",
    "#    If you only need certain columns, you can select them:\n",
    "df_pairs = df_joined[[\n",
    "    \"Opsin\",\n",
    "    \"Region\",\n",
    "    \"BrainID\",\n",
    "    \"SliceID\",\n",
    "    \"CellID\",\n",
    "    \"Color\",\n",
    "    \"StimPower\",\n",
    "    \"StimDuration\",\n",
    "    \"APregion\", \n",
    "    \"RoughAP\",\n",
    "    \"filename_minus55\",  # or other columns from the -55 side\n",
    "    \"filename_plus10\",   # or other columns from the +10 side\n",
    "    # ...\n",
    "]]\n",
    "\n",
    "# df_pairs now has one row per match, with columns for both the -55 side\n",
    "# and the +10 side. You could rename columns if you want.\n",
    "df_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dd7df9-f622-4128-bf04-36ccfb2d197b",
   "metadata": {},
   "source": [
    "# Result_of_EIkinetics.csv保存\n",
    "\n",
    "## -55 mVデータと+10 mVデータの解析結果をanalysis_results.csvから集める\n",
    "\n",
    "1) **-55 mV 側 (df_minus) と +10 mV 側 (df_plus) の分割**  \n",
    "   `df_results` から、`VoltageHold` が `-55` の行だけを `df_minus` に、`10` の行だけを `df_plus` にそれぞれ抽出します。  \n",
    "   これによって、-55 mV 条件で記録されたデータと +10 mV 条件で記録されたデータを分けて集計する準備ができます。\n",
    "\n",
    "2) **df_minus_agg の作成（-55 mV 側の平均・標準偏差など）**  \n",
    "   `df_minus` に対して `groupby(\"filename\")` を行い、各ファイルごとに `PeakAmplitude_pA` や `Latency_ms` などの列を `\"mean\"`, `\"std\"` で集計しています。  \n",
    "   これにより、同じ ABF ファイル名に含まれる複数のスイープ結果をまとめ、平均値や標準偏差を求められます。  \n",
    "   得られた結果の列名は、複数階層 (`PeakAmplitude_pA mean` や `PeakAmplitude_pA std` など) になってしまうので、`to_flat_index()` などでフラット化し、名前を結合しています。  \n",
    "   最終的に `PeakAmplitude_pA_mean` などのような列に変換されます。  \n",
    "   さらに、標準偏差を平均で割った「変動係数 (CV)」を求めるため、たとえば `PeakAmplitude_pA_CV = PeakAmplitude_pA_std / PeakAmplitude_pA_mean` のように列を追加しています。\n",
    "\n",
    "3) **列名を「minus55」向けにリネーム**  \n",
    "   `filename` や `PeakAmplitude_pA_mean` などの列を、一意にわかりやすくするために `filename_minus55` や `PeakAmp_minus55_mean` のような名前にリネームしています。  \n",
    "   こうすることで、あとで +10 側と区別できるようになります。\n",
    "\n",
    "4) **+10 mV 側の集計 (df_plus_agg)**  \n",
    "   上記と同様に、`df_plus` でも `groupby(\"filename\")` で平均・標準偏差を集計し、列をフラット化します。  \n",
    "   -55 mV と同じように、`PeakAmplitude_pA_CV` や `Latency_ms_CV` などを計算し、今度は列名を「plus10」向け (`filename_plus10`, `PeakAmp_plus10_mean` など) に変更します。\n",
    "\n",
    "5) **ペア化テーブル (df_pairs) と -55 側の集計をマージ**  \n",
    "   すでに作成してあるペア化テーブル (`df_pairs`) は、`filename_minus55` と `filename_plus10` を列として持ち、-55 条件のファイル名と +10 条件のファイル名の対応が記述されています。  \n",
    "   `df_minus_agg` を、`df_pairs` の `filename_minus55` と結合することで、-55 側の集計情報 (`PeakAmp_minus55_mean` など) をペア化テーブルに合流させます。  \n",
    "   `left_on=\"filename_minus55\"`, `right_on=\"filename_minus55\"` により、両方のテーブルに共通する -55 mV 側ファイル名が合致した行同士が結合されます。\n",
    "\n",
    "6) **次に +10 側の集計をマージ**  \n",
    "   前ステップでできた中間テーブル (`df_merged1`) に対し、`df_plus_agg` を `filename_plus10` 列で結合します。  \n",
    "   これにより、+10 mV 側の集計データ (`PeakAmp_plus10_mean` など) も同じペアの行に結合され、最終的に -55 側と +10 側それぞれの平均・標準偏差・CV が 1 行にまとまります。\n",
    "\n",
    "7) **完成したテーブル (df_final)**  \n",
    "   上記の結合の結果、-55 mV と +10 mV のペアごとに、両者の集計情報が 1 行にそろったテーブルが `df_final` です。  \n",
    "   これを使うことで、同じ細胞や同じ刺激条件（ただし電位保持だけ異なる）ごとに、-55 mV 時と +10 mV 時の振る舞いを比較できます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81de06b7-97cd-40c2-b48c-50231d52f7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create aggregated DataFrames including R² ---\n",
    "\n",
    "# Filter by holding potential\n",
    "df_minus = df_results[df_results[\"VoltageHold\"] == -55].copy()\n",
    "df_plus  = df_results[df_results[\"VoltageHold\"] == 10].copy()\n",
    "\n",
    "# --- For the –55 mV side ---\n",
    "df_minus_agg = (\n",
    "    df_minus\n",
    "    .groupby(\"filename\", as_index=False)\n",
    "    .agg({\n",
    "        \"PeakAmplitude_pA\": [\"mean\",\"std\",\"count\"],\n",
    "        \"Latency_ms\":       [\"mean\",\"std\",\"count\"],\n",
    "        \"RiseTime_ms\":      [\"mean\",\"std\",\"count\"],\n",
    "        \"DecayTau\":         [\"mean\",\"std\",\"count\"],\n",
    "        \"R2\":               [\"mean\",\"std\",\"count\"],      # ← include R2\n",
    "        \"Ih_pA\":    [\"mean\",\"std\",\"count\"],\n",
    "        \"Rm_MOhm\":  [\"mean\",\"std\",\"count\"],\n",
    "        \"Ra_MOhm\":  [\"mean\",\"std\",\"count\"],\n",
    "        \"Cm_pF\":    [\"mean\",\"std\",\"count\"]\n",
    "    })\n",
    ")\n",
    "\n",
    "# Flatten MultiIndex columns\n",
    "df_minus_agg.columns = [\n",
    "    \"_\".join(filter(None, tup)) if tup[1] else tup[0]\n",
    "    for tup in df_minus_agg.columns.to_flat_index()\n",
    "]\n",
    "\n",
    "# Compute CVs\n",
    "df_minus_agg[\"PeakAmplitude_pA_CV\"] = (\n",
    "    df_minus_agg[\"PeakAmplitude_pA_std\"] / df_minus_agg[\"PeakAmplitude_pA_mean\"]\n",
    ")\n",
    "df_minus_agg[\"Latency_ms_CV\"] = (\n",
    "    df_minus_agg[\"Latency_ms_std\"] / df_minus_agg[\"Latency_ms_mean\"]\n",
    ")\n",
    "df_minus_agg[\"RiseTime_ms_CV\"] = (\n",
    "    df_minus_agg[\"RiseTime_ms_std\"] / df_minus_agg[\"RiseTime_ms_mean\"]\n",
    ")\n",
    "df_minus_agg[\"DecayTau_CV\"] = (\n",
    "    df_minus_agg[\"DecayTau_std\"] / df_minus_agg[\"DecayTau_mean\"]\n",
    ")\n",
    "df_minus_agg[\"R2_CV\"] = (\n",
    "    df_minus_agg[\"R2_std\"] / df_minus_agg[\"R2_mean\"]\n",
    ")\n",
    "df_minus_agg[\"Ih_pA_CV\"]   = df_minus_agg[\"Ih_pA_std\"]   / df_minus_agg[\"Ih_pA_mean\"]\n",
    "df_minus_agg[\"Rm_MOhm_CV\"] = df_minus_agg[\"Rm_MOhm_std\"] / df_minus_agg[\"Rm_MOhm_mean\"]\n",
    "df_minus_agg[\"Ra_MOhm_CV\"] = df_minus_agg[\"Ra_MOhm_std\"] / df_minus_agg[\"Ra_MOhm_mean\"]\n",
    "df_minus_agg[\"Cm_pF_CV\"]   = df_minus_agg[\"Cm_pF_std\"]   / df_minus_agg[\"Cm_pF_mean\"]\n",
    "\n",
    "# Rename for clarity\n",
    "df_minus_agg = df_minus_agg.rename(columns={\n",
    "    \"filename\":               \"filename_minus55\",\n",
    "    \"PeakAmplitude_pA_mean\":  \"PeakAmp_minus55_mean\",\n",
    "    \"PeakAmplitude_pA_std\":   \"PeakAmp_minus55_std\",\n",
    "    \"PeakAmplitude_pA_count\": \"n_minus55\",\n",
    "    \"PeakAmplitude_pA_CV\":    \"PeakAmp_minus55_CV\",\n",
    "    \"Latency_ms_mean\":        \"Latency_ms_minus55_mean\",\n",
    "    \"Latency_ms_std\":         \"Latency_ms_minus55_std\",\n",
    "    \"Latency_ms_count\":       \"nLatency_minus55\",\n",
    "    \"Latency_ms_CV\":          \"Latency_ms_minus55_CV\",\n",
    "    \"RiseTime_ms_mean\":       \"RiseTime_ms_minus55_mean\",\n",
    "    \"RiseTime_ms_std\":        \"RiseTime_ms_minus55_std\",\n",
    "    \"RiseTime_ms_count\":      \"nRise_minus55\",\n",
    "    \"RiseTime_ms_CV\":         \"RiseTime_ms_minus55_CV\",\n",
    "    \"DecayTau_mean\":          \"DecayTau_minus55_mean\",\n",
    "    \"DecayTau_std\":           \"DecayTau_minus55_std\",\n",
    "    \"DecayTau_count\":         \"nDecay_minus55\",\n",
    "    \"DecayTau_CV\":            \"DecayTau_minus55_CV\",\n",
    "    \"R2_mean\":                \"R2_minus55_mean\",      # ← renamed\n",
    "    \"R2_std\":                 \"R2_minus55_std\",\n",
    "    \"R2_count\":               \"nR2_minus55\",\n",
    "    \"R2_CV\":                  \"R2_minus55_CV\",\n",
    "    \"Ih_pA_mean\":    \"Ih_minus55_mean\",\n",
    "    \"Ih_pA_std\":     \"Ih_minus55_std\",\n",
    "    \"Ih_pA_count\":   \"nIh_minus55\",\n",
    "    \"Ih_pA_CV\":      \"Ih_minus55_CV\",\n",
    "\n",
    "    \"Rm_MOhm_mean\":  \"Rm_minus55_mean\",\n",
    "    \"Rm_MOhm_std\":   \"Rm_minus55_std\",\n",
    "    \"Rm_MOhm_count\": \"nRm_minus55\",\n",
    "    \"Rm_MOhm_CV\":    \"Rm_minus55_CV\",\n",
    "\n",
    "    \"Ra_MOhm_mean\":  \"Ra_minus55_mean\",\n",
    "    \"Ra_MOhm_std\":   \"Ra_minus55_std\",\n",
    "    \"Ra_MOhm_count\": \"nRa_minus55\",\n",
    "    \"Ra_MOhm_CV\":    \"Ra_minus55_CV\",\n",
    "\n",
    "    \"Cm_pF_mean\":    \"Cm_minus55_mean\",\n",
    "    \"Cm_pF_std\":     \"Cm_minus55_std\",\n",
    "    \"Cm_pF_count\":   \"nCm_minus55\",\n",
    "    \"Cm_pF_CV\":      \"Cm_minus55_CV\",\n",
    "    \n",
    "})\n",
    "\n",
    "# --- For the +10 mV side ---\n",
    "df_plus_agg = (\n",
    "    df_plus\n",
    "    .groupby(\"filename\", as_index=False)\n",
    "    .agg({\n",
    "        \"PeakAmplitude_pA\": [\"mean\",\"std\",\"count\"],\n",
    "        \"Latency_ms\":       [\"mean\",\"std\",\"count\"],\n",
    "        \"RiseTime_ms\":      [\"mean\",\"std\",\"count\"],\n",
    "        \"DecayTau\":         [\"mean\",\"std\",\"count\"],\n",
    "        \"R2\":               [\"mean\",\"std\",\"count\"],      # ← include R2\n",
    "        # ─── membrane‐test metrics ───\n",
    "        \"Ih_pA\":    [\"mean\",\"std\",\"count\"],\n",
    "        \"Rm_MOhm\":  [\"mean\",\"std\",\"count\"],\n",
    "        \"Ra_MOhm\":  [\"mean\",\"std\",\"count\"],\n",
    "        \"Cm_pF\":    [\"mean\",\"std\",\"count\"],\n",
    "    })\n",
    ")\n",
    "df_plus_agg.columns = [\n",
    "    \"_\".join(filter(None, tup)) if tup[1] else tup[0]\n",
    "    for tup in df_plus_agg.columns.to_flat_index()\n",
    "]\n",
    "df_plus_agg[\"PeakAmplitude_pA_CV\"] = (\n",
    "    df_plus_agg[\"PeakAmplitude_pA_std\"] / df_plus_agg[\"PeakAmplitude_pA_mean\"]\n",
    ")\n",
    "df_plus_agg[\"Latency_ms_CV\"] = (\n",
    "    df_plus_agg[\"Latency_ms_std\"] / df_plus_agg[\"Latency_ms_mean\"]\n",
    ")\n",
    "df_plus_agg[\"RiseTime_ms_CV\"] = (\n",
    "    df_plus_agg[\"RiseTime_ms_std\"] / df_plus_agg[\"RiseTime_ms_mean\"]\n",
    ")\n",
    "df_plus_agg[\"DecayTau_CV\"] = (\n",
    "    df_plus_agg[\"DecayTau_std\"] / df_plus_agg[\"DecayTau_mean\"]\n",
    ")\n",
    "df_plus_agg[\"R2_CV\"] = (\n",
    "    df_plus_agg[\"R2_std\"] / df_plus_agg[\"R2_mean\"]\n",
    ")\n",
    "df_plus_agg[\"Ih_pA_CV\"]            = df_plus_agg[\"Ih_pA_std\"]               / df_plus_agg[\"Ih_pA_mean\"]\n",
    "df_plus_agg[\"Rm_MOhm_CV\"]          = df_plus_agg[\"Rm_MOhm_std\"]             / df_plus_agg[\"Rm_MOhm_mean\"]\n",
    "df_plus_agg[\"Ra_MOhm_CV\"]          = df_plus_agg[\"Ra_MOhm_std\"]             / df_plus_agg[\"Ra_MOhm_mean\"]\n",
    "df_plus_agg[\"Cm_pF_CV\"]            = df_plus_agg[\"Cm_pF_std\"]               / df_plus_agg[\"Cm_pF_mean\"]\n",
    "\n",
    "df_plus_agg = df_plus_agg.rename(columns={\n",
    "    \"filename\":               \"filename_plus10\",\n",
    "    \"PeakAmplitude_pA_mean\":  \"PeakAmp_plus10_mean\",\n",
    "    \"PeakAmplitude_pA_std\":   \"PeakAmp_plus10_std\",\n",
    "    \"PeakAmplitude_pA_count\": \"n_plus10\",\n",
    "    \"PeakAmplitude_pA_CV\":    \"PeakAmp_plus10_CV\",\n",
    "    \"Latency_ms_mean\":        \"Latency_ms_plus10_mean\",\n",
    "    \"Latency_ms_std\":         \"Latency_ms_plus10_std\",\n",
    "    \"Latency_ms_count\":       \"nLatency_plus10\",\n",
    "    \"Latency_ms_CV\":          \"Latency_ms_plus10_CV\",\n",
    "    \"RiseTime_ms_mean\":       \"RiseTime_ms_plus10_mean\",\n",
    "    \"RiseTime_ms_std\":        \"RiseTime_ms_plus10_std\",\n",
    "    \"RiseTime_ms_count\":      \"nRise_plus10\",\n",
    "    \"RiseTime_ms_CV\":         \"RiseTime_ms_plus10_CV\",\n",
    "    \"DecayTau_mean\":          \"DecayTau_plus10_mean\",\n",
    "    \"DecayTau_std\":           \"DecayTau_plus10_std\",\n",
    "    \"DecayTau_count\":         \"nDecay_plus10\",\n",
    "    \"DecayTau_CV\":            \"DecayTau_plus10_CV\",\n",
    "    \"R2_mean\":                \"R2_plus10_mean\",       # ← renamed\n",
    "    \"R2_std\":                 \"R2_plus10_std\",\n",
    "    \"R2_count\":               \"nR2_plus10\",\n",
    "    \"R2_CV\":                  \"R2_plus10_CV\",\n",
    "    # membrane-test renames\n",
    "    \"Ih_pA_mean\":             \"Ih_plus10_mean\",\n",
    "    \"Ih_pA_std\":              \"Ih_plus10_std\",\n",
    "    \"Ih_pA_count\":            \"nIh_plus10\",\n",
    "    \"Ih_pA_CV\":               \"Ih_plus10_CV\",\n",
    "\n",
    "    \"Rm_MOhm_mean\":           \"Rm_plus10_mean\",\n",
    "    \"Rm_MOhm_std\":            \"Rm_plus10_std\",\n",
    "    \"Rm_MOhm_count\":          \"nRm_plus10\",\n",
    "    \"Rm_MOhm_CV\":             \"Rm_plus10_CV\",\n",
    "\n",
    "    \"Ra_MOhm_mean\":           \"Ra_plus10_mean\",\n",
    "    \"Ra_MOhm_std\":            \"Ra_plus10_std\",\n",
    "    \"Ra_MOhm_count\":          \"nRa_plus10\",\n",
    "    \"Ra_MOhm_CV\":             \"Ra_plus10_CV\",\n",
    "\n",
    "    \"Cm_pF_mean\":             \"Cm_plus10_mean\",\n",
    "    \"Cm_pF_std\":              \"Cm_plus10_std\",\n",
    "    \"Cm_pF_count\":            \"nCm_plus10\",\n",
    "    \"Cm_pF_CV\":               \"Cm_plus10_CV\",\n",
    "})\n",
    "\n",
    "# Merge back into df_pairs and save as before…\n",
    "df_merged1 = pd.merge(df_pairs, df_minus_agg,\n",
    "                      on=\"filename_minus55\", how=\"left\")\n",
    "df_final   = pd.merge(df_merged1, df_plus_agg,\n",
    "                      on=\"filename_plus10\", how=\"left\")\n",
    "\n",
    "df_final.to_csv(\n",
    "    f\"./sorted_directory/Result_of_EIkinetics_{df_final['Opsin'].iat[0]}.csv\",\n",
    "    index=False\n",
    ")\n",
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af2ab7a-a3d6-41ca-8988-432e89f33eed",
   "metadata": {},
   "source": [
    "# EPSC/IPSC trace plot\n",
    "\n",
    "## 単一ファイル\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622f95d2-cb67-49cf-9151-c5ef0685b284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyabf\n",
    "\n",
    "def find_stim_time_digital(abf, sweep_index, dig_out_num=3):\n",
    "    \"\"\"\n",
    "    Example: return first time where digital signal transitions from 0 to 1.\n",
    "    (Adapt as needed, or skip if you have your own method.)\n",
    "    \"\"\"\n",
    "    abf.setSweep(sweep_index)\n",
    "    time_s = abf.sweepX\n",
    "    digital_sig = abf.sweepD(dig_out_num)\n",
    "    idxs = np.where(digital_sig > 0.5)[0]\n",
    "    if len(idxs) == 0:\n",
    "        return None\n",
    "    return time_s[idxs[0]]\n",
    "\n",
    "\n",
    "def load_all_sweeps(abf_path, stim_chan=3, domain_window=(-0.1, 0.5)):\n",
    "    \"\"\"\n",
    "    Load all sweeps from the ABF at abf_path.\n",
    "    Return a dict with:\n",
    "      {\n",
    "        \"raw_traces\": [array_of_pA_1, array_of_pA_2, ...],\n",
    "        \"time_axis\":  array_of_time_s (common for all sweeps),\n",
    "      }\n",
    "    \n",
    "    Steps:\n",
    "      1) Load ABF\n",
    "      2) For each sweep, detect stim_time (if possible). If found, slice domain [stim_time + domain_window].\n",
    "         Otherwise, keep entire sweep or do a fallback domain.\n",
    "      3) Baseline-subtract if desired.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(abf_path):\n",
    "        print(f\"[WARNING] ABF not found: {abf_path}\")\n",
    "        return None\n",
    "\n",
    "    abf = pyabf.ABF(abf_path)\n",
    "\n",
    "    # We store one time array and a list of current arrays\n",
    "    # (We’ll enforce that all sweeps have the same domain size, so we can average.)\n",
    "    all_traces = []\n",
    "    all_time = None\n",
    "\n",
    "    for sweep_idx in abf.sweepList:\n",
    "        time_s = abf.sweepX\n",
    "        # Attempt to find stim time from digital output\n",
    "        stim_time = find_stim_time_digital(abf, sweep_idx, dig_out_num=stim_chan)\n",
    "        if stim_time is not None:\n",
    "            t_min = stim_time + domain_window[0]\n",
    "            t_max = stim_time + domain_window[1]\n",
    "        else:\n",
    "            # If no stim time found, fallback to the entire sweep\n",
    "            t_min = time_s[0]\n",
    "            t_max = time_s[-1]\n",
    "        if not np.isnan(stim_time):\n",
    "            baseline_start_sec = stim_time - 0.1\n",
    "            baseline_end_sec   = stim_time\n",
    "        else:\n",
    "            # fallback if no stim\n",
    "            baseline_start_sec = 1.155\n",
    "            baseline_end_sec   = 1.156\n",
    "\n",
    "        # Make a mask for [t_min, t_max]\n",
    "        mask = (time_s >= t_min) & (time_s <= t_max)\n",
    "        if not np.any(mask):\n",
    "            # If there's no data in that window, skip\n",
    "            continue\n",
    "\n",
    "        abf.setSweep(sweep_idx, channel=0, baseline=[baseline_start_sec, baseline_end_sec])\n",
    "        current_pA = abf.sweepY\n",
    "        \n",
    "        t_sel = time_s[mask]\n",
    "        i_sel = current_pA[mask]\n",
    "\n",
    "        # If it's the first successful sweep, store as reference\n",
    "        if all_time is None:\n",
    "            all_time = t_sel - t_sel[0]  # shift to 0-based time, or keep absolute\n",
    "        else:\n",
    "            # We must ensure we can align all sweeps in the same time array shape\n",
    "            if len(t_sel) != len(all_time):\n",
    "                # If different sample counts, you might do interpolation or skip\n",
    "                continue\n",
    "\n",
    "        all_traces.append(i_sel)\n",
    "\n",
    "    if (all_time is None) or (len(all_traces) == 0):\n",
    "        return None\n",
    "\n",
    "    all_traces = np.array(all_traces)  # shape = (N_sweeps, N_points)\n",
    "    return {\n",
    "        \"time_s\": all_time,\n",
    "        \"raw_traces\": all_traces\n",
    "    }\n",
    "\n",
    "def plot_epsc_and_ipsc(\n",
    "    epsc_path,\n",
    "    ipsc_path,\n",
    "    domain_window=(-0.1, 0.5),\n",
    "    output_png=\"epsc_ipsc.png\",\n",
    "    no_ticks=False,\n",
    "    figsize=(8, 5),\n",
    "    title_fontsize=14,\n",
    "    label_fontsize=11,\n",
    "    show_title=True,\n",
    "    show_legend=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Load EPSC data from epsc_path (e.g., -55 mV) and\n",
    "    IPSC data from ipsc_path (e.g., +10 mV).\n",
    "    Plot all sweeps (EPSC in blue, IPSC in red),\n",
    "    plus their means as thick lines.\n",
    "    Remove rectangle outlines (axes box), and optionally add an L-shaped scalebar.\n",
    "    \"\"\"\n",
    "\n",
    "    data_epsc = load_all_sweeps(epsc_path, stim_chan=3, domain_window=domain_window)\n",
    "    data_ipsc = load_all_sweeps(ipsc_path, stim_chan=3, domain_window=domain_window)\n",
    "\n",
    "    if data_epsc is None and data_ipsc is None:\n",
    "        print(\"No data loaded from either file. Nothing to plot.\")\n",
    "        return\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    # Plot EPSC\n",
    "    if data_epsc is not None:\n",
    "        t = data_epsc[\"time_s\"]\n",
    "        traces = data_epsc[\"raw_traces\"]\n",
    "        for trace in traces:\n",
    "            ax.plot(t, trace, color=\"blue\", alpha=0.3, lw=0.7)\n",
    "        mean_epsc = np.mean(traces, axis=0)\n",
    "        ax.plot(t, mean_epsc, color=\"blue\", lw=2.0, label=\"Mean EPSC\")\n",
    "\n",
    "    # Plot IPSC\n",
    "    if data_ipsc is not None:\n",
    "        t = data_ipsc[\"time_s\"]\n",
    "        traces = data_ipsc[\"raw_traces\"]\n",
    "        for trace in traces:\n",
    "            ax.plot(t, trace, color=\"red\", alpha=0.3, lw=0.7)\n",
    "        mean_ipsc = np.mean(traces, axis=0)\n",
    "        ax.plot(t, mean_ipsc, color=\"red\", lw=2.0, label=\"Mean IPSC\")\n",
    "\n",
    "    # Draw a vertical line marking the stimulus (time= -domain_window[0]) \n",
    "    # in axis-fraction coordinates\n",
    "    ax.axvline(\n",
    "        x=-domain_window[0],\n",
    "        color=\"blue\",\n",
    "        lw=1.5,\n",
    "        alpha=0.6,\n",
    "        label=\"Stim\",\n",
    "        ymin=0.8,\n",
    "        ymax=1.0\n",
    "    )\n",
    "\n",
    "    # Determine Y-limits\n",
    "    all_vals = []\n",
    "    if data_epsc is not None:\n",
    "        all_vals.extend([data_epsc[\"raw_traces\"].min(), data_epsc[\"raw_traces\"].max()])\n",
    "    if data_ipsc is not None:\n",
    "        all_vals.extend([data_ipsc[\"raw_traces\"].min(), data_ipsc[\"raw_traces\"].max()])\n",
    "\n",
    "    if all_vals:\n",
    "        y_min = min(all_vals)\n",
    "        y_max = max(all_vals)\n",
    "        pad = 0.1 * (y_max - y_min)\n",
    "        ax.set_ylim(y_min - pad, y_max + pad)\n",
    "\n",
    "    # Determine X-limits (assuming we re-zeroed time)\n",
    "    x_maxes = []\n",
    "    if data_epsc is not None:\n",
    "        x_maxes.append(data_epsc[\"time_s\"][-1])\n",
    "    if data_ipsc is not None:\n",
    "        x_maxes.append(data_ipsc[\"time_s\"][-1])\n",
    "    if x_maxes:\n",
    "        ax.set_xlim(0, max(x_maxes))\n",
    "\n",
    "    # Optionally show legend\n",
    "    if show_legend:\n",
    "        ax.legend(fontsize=label_fontsize)\n",
    "\n",
    "    # If user requests no ticks, remove axes outlines and ticks\n",
    "    if no_ticks is True:\n",
    "        for spine in [\"top\", \"right\", \"left\", \"bottom\"]:\n",
    "            ax.spines[spine].set_visible(False)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "        # Decide how big the scalebar is\n",
    "        scale_ms = 50e-3  # 50 ms\n",
    "        scale_pA = 50     # 50 pA\n",
    "\n",
    "        # Decide reference point near bottom right\n",
    "        x_min_plot, x_max_plot = ax.get_xlim()\n",
    "        y_min_plot, y_max_plot = ax.get_ylim()\n",
    "        x_range = x_max_plot - x_min_plot\n",
    "        y_range = y_max_plot - y_min_plot\n",
    "\n",
    "        # Place scalebar 5% from right and 10% from bottom\n",
    "        x_ref = x_max_plot - 0.05*x_range - scale_ms\n",
    "        y_ref = y_min_plot + 0.1*y_range\n",
    "\n",
    "        # Horizontal segment\n",
    "        ax.plot(\n",
    "            [x_ref, x_ref + scale_ms],\n",
    "            [y_ref, y_ref],\n",
    "            color='k',\n",
    "            lw=2\n",
    "        )\n",
    "        # Vertical segment\n",
    "        ax.plot(\n",
    "            [x_ref, x_ref],\n",
    "            [y_ref, y_ref + scale_pA],\n",
    "            color='k',\n",
    "            lw=2\n",
    "        )\n",
    "        # Scalebar text\n",
    "        ax.text(\n",
    "            x_ref + scale_ms/2,\n",
    "            y_ref - 0.05 * scale_pA,\n",
    "            f\"{int(scale_ms*1000)} ms\",\n",
    "            ha=\"center\",\n",
    "            va=\"top\",\n",
    "            fontsize=label_fontsize\n",
    "        )\n",
    "        ax.text(\n",
    "            x_ref - 0.05*scale_ms,\n",
    "            y_ref + scale_pA/2,\n",
    "            f\"{scale_pA} pA\",\n",
    "            ha=\"right\",\n",
    "            va=\"center\",\n",
    "            rotation=90,\n",
    "            fontsize=label_fontsize\n",
    "        )\n",
    "    else:\n",
    "        ax.tick_params(axis='both', which='both', length=3, labelsize=label_fontsize)\n",
    "        ax.set_xlabel(\"Time (s)\", fontsize=label_fontsize)\n",
    "        ax.set_ylabel(\"Current (pA)\", fontsize=label_fontsize)\n",
    "\n",
    "    # Optional main title (with adjustable font size)\n",
    "    if show_title is True:\n",
    "        ax.set_title(\"EPSC & IPSC Overlay\", fontsize=title_fontsize)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(output_png, dpi=150)\n",
    "    # print(f\"Saved figure: {output_png}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def filename_to_path(filename, abf_root = \"./sorted_directory\"):    \n",
    "    # 1) Dictionary of ABF files\n",
    "    abf_dict = find_abf_files(abf_root)\n",
    "\n",
    "    # 2) Get the ABF path\n",
    "    if filename not in abf_dict:\n",
    "        print(f\"[ERROR] ABF file not found: {filename}\")\n",
    "        return\n",
    "    else:\n",
    "        return abf_dict[filename]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d80189-241c-4d80-a31b-e22ca9798d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) Suppose we already have df_filtered\n",
    "if df_results is None:\n",
    "    df_results = pd.read_csv(\"./sorted_directory/analysis_results.csv\")\n",
    "if df_final is None:\n",
    "    df_final = pd.read_csv(\"./sorted_directory/Result_of_EIkinetics_ChR2.csv\")\n",
    "\n",
    "#df_for_eiplot = df_final\n",
    "df_for_eiplot = pd.read_csv(\"./sorted_directory/Result_of_EIkinetics_ChR2.csv\")\n",
    "\n",
    "i = 168\n",
    "row = df_for_eiplot.iloc[i]\n",
    "\n",
    "epsc_file = filename_to_path(row[\"filename_minus55\"], abf_root)\n",
    "ipsc_file = filename_to_path(row[\"filename_plus10\"], abf_root)\n",
    "\n",
    "print(\"EPSC ABF file:\", epsc_file)\n",
    "print(\"IPSC ABF file:\", ipsc_file)\n",
    "\n",
    "plot_epsc_and_ipsc(epsc_file, ipsc_file, \n",
    "                   domain_window=(-0.1, 0.5), \n",
    "                   output_png=\"my_epsc_ipsc_plot.png\", \n",
    "                   no_ticks = True, \n",
    "                   figsize=(5, 8),\n",
    "                   title_fontsize=14, \n",
    "                   label_fontsize=11, \n",
    "                   show_title = False,\n",
    "                   show_legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe548bf-e7e5-49b6-95f7-7dc1d60799db",
   "metadata": {},
   "source": [
    "## 複数ファイル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41a6487-c407-446a-9f61-bcca29c57771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_ei_plots_shared_axes(\n",
    "    df,\n",
    "    row_indices,\n",
    "    abf_root=\"./sorted_directory\",\n",
    "    domain_window=(-0.1, 0.5),\n",
    "    no_ticks=True,\n",
    "    figsize=(5,8),\n",
    "    title_fontsize=14,\n",
    "    label_fontsize=11,\n",
    "    show_title=False,\n",
    "    show_legend=False,\n",
    "    show_epsc=True,        # <--- New: controls whether to plot EPSC at all\n",
    "    show_ipsc=True,         # <--- New: controls whether to plot IPSC at all\n",
    "    scale_bar_label=True,\n",
    "    scale_bar_position=\"right_upper\",\n",
    "    show_mean=True,\n",
    "    show_raw_epsc=True,\n",
    "    show_raw_ipsc=True,\n",
    "    color_epsc=\"blue\",\n",
    "    color_ipsc=\"red\"\n",
    "):\n",
    "    \"\"\"\n",
    "    For each row index in row_indices, load the EPSC and IPSC ABFs, then plot them.\n",
    "    All plots use the same X domain (max length found) and same Y range (global min/max).\n",
    "\n",
    "    Args:\n",
    "      df : DataFrame with columns like 'filename_minus55', 'filename_plus10', etc.\n",
    "      row_indices : list of integers (which rows of df to process)\n",
    "      abf_root : path to directory with ABF files\n",
    "      domain_window : tuple of (start_offset, end_offset) relative to stimulus time\n",
    "      no_ticks : if True, hide axes ticks and draw an L-shaped scalebar\n",
    "      figsize : size of each output figure\n",
    "      title_fontsize, label_fontsize : control text sizes\n",
    "      show_title, show_legend : booleans controlling title & legend\n",
    "      scale_bar_label : if True, show numeric labels for the scale bar\n",
    "      scale_bar_position : str, e.g. 'right_upper' or 'right_middle'\n",
    "        This determines where the L-shaped scale bar is placed if no_ticks=True.\n",
    "      show_mean : if True, plot the mean EPSC/IPSC trace over the raw sweeps\n",
    "      show_raw_epsc : if True, plot raw sweeps for the EPSC data\n",
    "      show_raw_ipsc : if True, plot raw sweeps for the IPSC data\n",
    "      color_epsc : color (string) for EPSC traces (both raw and mean)\n",
    "      color_ipsc : color (string) for IPSC traces (both raw and mean)\n",
    "    \"\"\"\n",
    "    # ─────────────────────────────────────────────────────────\n",
    "    # 1) First pass: load data for all rows and find global min/max\n",
    "    #    and the maximum time domain among them\n",
    "    # ─────────────────────────────────────────────────────────\n",
    "    all_rows_data = []  # will hold tuples (data_epsc, data_ipsc)\n",
    "\n",
    "    global_min = None\n",
    "    global_max = None\n",
    "    global_tmax = 0.0   # track largest time across all files\n",
    "\n",
    "    for idx in row_indices:\n",
    "        row = df.iloc[idx]\n",
    "        color = row[\"Color\"]\n",
    "        print(color)\n",
    "        \n",
    "\n",
    "        # Construct ABF paths\n",
    "        epsc_abfpath = None\n",
    "        ipsc_abfpath = None\n",
    "        if \"filename_minus55\" in row:\n",
    "            epsc_abfpath = filename_to_path(row[\"filename_minus55\"], abf_root)\n",
    "        if \"filename_plus10\" in row:\n",
    "            ipsc_abfpath = filename_to_path(row[\"filename_plus10\"], abf_root)\n",
    "\n",
    "        if color == \"blue\":\n",
    "            stim_chan = 3\n",
    "        else:\n",
    "            stim_chan = 0\n",
    "        data_epsc = (load_all_sweeps(epsc_abfpath, stim_chan=stim_chan, domain_window=domain_window) \n",
    "                     if epsc_abfpath else None)\n",
    "        data_ipsc = (load_all_sweeps(ipsc_abfpath, stim_chan=stim_chan, domain_window=domain_window) \n",
    "                     if ipsc_abfpath else None)\n",
    "\n",
    "        all_rows_data.append((data_epsc, data_ipsc))\n",
    "\n",
    "        # Update global min/max and global_tmax\n",
    "        local_vals = []\n",
    "        if data_epsc is not None:\n",
    "            local_vals.append(data_epsc[\"raw_traces\"].min())\n",
    "            local_vals.append(data_epsc[\"raw_traces\"].max())\n",
    "            global_tmax = max(global_tmax, data_epsc[\"time_s\"][-1])\n",
    "\n",
    "        if data_ipsc is not None:\n",
    "            local_vals.append(data_ipsc[\"raw_traces\"].min())\n",
    "            local_vals.append(data_ipsc[\"raw_traces\"].max())\n",
    "            global_tmax = max(global_tmax, data_ipsc[\"time_s\"][-1])\n",
    "\n",
    "        if local_vals:\n",
    "            local_min = min(local_vals)\n",
    "            local_max = max(local_vals)\n",
    "            if global_min is None or local_min < global_min:\n",
    "                global_min = local_min\n",
    "            if global_max is None or local_max > global_max:\n",
    "                global_max = local_max\n",
    "\n",
    "    if global_min is None or global_max is None:\n",
    "        print(\"[INFO] No valid data found in any row. Aborting.\")\n",
    "        return\n",
    "\n",
    "    # Add some padding\n",
    "    y_range = global_max - global_min\n",
    "    y_min = global_min - 0.1 * y_range\n",
    "    y_max = global_max + 0.1 * y_range\n",
    "\n",
    "    # ─────────────────────────────────────────────────────────\n",
    "    # 2) Second pass: plot each row's data using the shared domain & range\n",
    "    # ─────────────────────────────────────────────────────────\n",
    "    for idx, (data_epsc, data_ipsc) in zip(row_indices, all_rows_data):\n",
    "        row = df.iloc[idx]\n",
    "        Opsin = row[\"Opsin\"]\n",
    "        Region = row[\"Region\"]\n",
    "        BrainID = row[\"BrainID\"]\n",
    "        SliceID = row[\"SliceID\"]\n",
    "        CellID = row[\"CellID\"]\n",
    "        StimPower = row[\"StimPower\"]\n",
    "        StimDuration = row[\"StimDuration\"]\n",
    "        APregion = row[\"APregion\"]\n",
    "        \n",
    "        if data_epsc is None and data_ipsc is None:\n",
    "            print(f\"[WARNING] Row {idx}: no valid EPSC or IPSC data. Skipping plot.\")\n",
    "            continue\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "        # Plot EPSC\n",
    "        if show_epsc and data_epsc is not None:\n",
    "            t_epsc = data_epsc[\"time_s\"]\n",
    "            arr_epsc = data_epsc[\"raw_traces\"]\n",
    "            # raw sweeps\n",
    "            if show_raw_epsc:\n",
    "                for trace in arr_epsc:\n",
    "                    ax.plot(t_epsc, trace, color=color_epsc, alpha=0.3, lw=0.7)\n",
    "            # mean trace\n",
    "            if show_mean:\n",
    "                mean_epsc = arr_epsc.mean(axis=0)\n",
    "                ax.plot(t_epsc, mean_epsc, color=color_epsc, lw=2.0, label=\"Mean EPSC\")\n",
    "\n",
    "        # Plot IPSC\n",
    "        if show_ipsc and data_ipsc is not None:\n",
    "            t_ipsc = data_ipsc[\"time_s\"]\n",
    "            arr_ipsc = data_ipsc[\"raw_traces\"]\n",
    "            # raw sweeps\n",
    "            if show_raw_ipsc:\n",
    "                for trace in arr_ipsc:\n",
    "                    ax.plot(t_ipsc, trace, color=color_ipsc, alpha=0.3, lw=0.7)\n",
    "            # mean trace\n",
    "            if show_mean:\n",
    "                mean_ipsc = arr_ipsc.mean(axis=0)\n",
    "                ax.plot(t_ipsc, mean_ipsc, color=color_ipsc, lw=2.0, label=\"Mean IPSC\")\n",
    "\n",
    "        # Stim line (assuming 0 => -domain_window[0])\n",
    "        ax.axvline(\n",
    "            x=-domain_window[0],\n",
    "            color=\"blue\",\n",
    "            lw=1.5,\n",
    "            alpha=0.6,\n",
    "            label=\"Stim\",\n",
    "            ymin=0.9,\n",
    "            ymax=1.0\n",
    "        )\n",
    "\n",
    "        # Set global domain/range\n",
    "        ax.set_xlim(0, global_tmax)\n",
    "        ax.set_ylim(y_min, y_max)\n",
    "\n",
    "        if show_legend:\n",
    "            ax.legend(fontsize=label_fontsize)\n",
    "\n",
    "        if no_ticks:\n",
    "            # Remove spines/ticks\n",
    "            for spine in [\"top\", \"right\", \"left\", \"bottom\"]:\n",
    "                ax.spines[spine].set_visible(False)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "\n",
    "            # Decide how big the scalebar is\n",
    "            scale_ms = 50e-3  # 50 ms\n",
    "            scale_pA = 50     # 50 pA\n",
    "\n",
    "            x_min_plot, x_max_plot = ax.get_xlim()\n",
    "            y_min_plot, y_max_plot = ax.get_ylim()\n",
    "            x_range = x_max_plot - x_min_plot\n",
    "            y_rng = y_max_plot - y_min_plot\n",
    "\n",
    "            # ─────────────────────────────────────────────────────────\n",
    "            # Logic for choosing x_ref, y_ref based on scale_bar_position\n",
    "            # ─────────────────────────────────────────────────────────\n",
    "            if scale_bar_position == \"right_upper\":\n",
    "                # e.g. near upper-right\n",
    "                x_ref = x_max_plot - 0.05*x_range - scale_ms\n",
    "                y_ref = y_max_plot - 0.15*y_rng  # 15% below top\n",
    "            elif scale_bar_position == \"right_middle\":\n",
    "                # e.g. near right-middle\n",
    "                x_ref = x_max_plot - 0.05*x_range - scale_ms\n",
    "                y_ref = y_min_plot + 0.45*y_rng\n",
    "            else:\n",
    "                # fallback: near bottom-right\n",
    "                x_ref = x_max_plot - 0.05*x_range - scale_ms\n",
    "                y_ref = y_min_plot + 0.1*y_rng\n",
    "\n",
    "            # Horizontal segment\n",
    "            ax.plot(\n",
    "                [x_ref, x_ref + scale_ms],\n",
    "                [y_ref, y_ref],\n",
    "                color='k',\n",
    "                lw=2\n",
    "            )\n",
    "            # Vertical segment\n",
    "            ax.plot(\n",
    "                [x_ref, x_ref],\n",
    "                [y_ref, y_ref + scale_pA],\n",
    "                color='k',\n",
    "                lw=2\n",
    "            )\n",
    "            # Scalebar text\n",
    "            if scale_bar_label:\n",
    "                ax.text(\n",
    "                    x_ref + scale_ms/2,\n",
    "                    y_ref - 0.05 * scale_pA,\n",
    "                    f\"{int(scale_ms*1000)} ms\",\n",
    "                    ha=\"center\",\n",
    "                    va=\"top\",\n",
    "                    fontsize=label_fontsize\n",
    "                )\n",
    "                ax.text(\n",
    "                    x_ref - 0.05*scale_ms,\n",
    "                    y_ref + scale_pA/2,\n",
    "                    f\"{scale_pA} pA\",\n",
    "                    ha=\"right\",\n",
    "                    va=\"center\",\n",
    "                    rotation=90,\n",
    "                    fontsize=label_fontsize\n",
    "                )\n",
    "        else:\n",
    "            ax.tick_params(axis='both', which='both', length=3, labelsize=label_fontsize)\n",
    "            ax.set_xlabel(\"Time (s)\", fontsize=label_fontsize)\n",
    "            ax.set_ylabel(\"Current (pA)\", fontsize=label_fontsize)\n",
    "\n",
    "        if show_title:\n",
    "            ax.set_title(f\"{Opsin}_{Region}_{BrainID}_{SliceID}_{CellID}_{StimPower}_{StimDuration}\", \n",
    "                         fontsize=title_fontsize)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Optionally save\n",
    "        out_name = f\"eiplot_{Opsin}_{Region}_{BrainID}_{SliceID}_{CellID}_{StimPower}_{StimDuration}_{APregion}.png\"\n",
    "        savepath = os.path.join(abf_root, \"EIplots\", out_name)\n",
    "        plt.savefig(savepath, dpi=150, transparent=True)\n",
    "        print(f\"Saved: {out_name}\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    print(\"Done creating E-I plots with shared domain/range.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729f503e-d257-4e4a-b7e0-779184899c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#row_indices = [114, 112, 107] # ACC\n",
    "#row_indices = [52, 48, 46] # RSC\n",
    "#row_indices = [47, 170] # RSCとACC\n",
    "#row_indices = [139, 136, 130] # ACC-ChR2 Dual\n",
    "row_indices = [142, 137, 129, 139, 136, 130] # RSC-ChrimsonR Dual\n",
    "row = df_for_eiplot.iloc[i]\n",
    "light_orange = \"#ff4b00\"\n",
    "light_green = \"#03af7a\"\n",
    "create_ei_plots_shared_axes(\n",
    "    df_for_eiplot,\n",
    "    row_indices,\n",
    "    abf_root=\"./sorted_directory\",\n",
    "    domain_window=(-0.1, 1),\n",
    "    no_ticks=True,\n",
    "    figsize=(2.19, 2.69),\n",
    "    title_fontsize=14,\n",
    "    label_fontsize=3,\n",
    "    show_title=False,\n",
    "    show_legend=False,\n",
    "    show_epsc = True,\n",
    "    show_ipsc = True,\n",
    "    scale_bar_label = False,\n",
    "    scale_bar_position=\"right_upper\",\n",
    "    show_mean=True,\n",
    "    show_raw_epsc=False,\n",
    "    show_raw_ipsc=False,\n",
    "    color_epsc=light_green,\n",
    "    color_ipsc=light_green\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
