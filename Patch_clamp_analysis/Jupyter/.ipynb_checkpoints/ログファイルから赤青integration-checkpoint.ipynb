{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837d6685-24e3-4b03-bdfa-7a26acc8a83d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext cudf.pandas\n",
    "import pyabf\n",
    "import sys; sys.path.append(\"./\")\n",
    "import log_search\n",
    "import pandas as pd\n",
    "from scipy.signal import find_peaks, detrend\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from decimal import Decimal, ROUND_HALF_UP, ROUND_FLOOR, ROUND_CEILING\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1aca992-07ac-4260-b01b-ae870c8d80b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_log_for_ptx(filename: str, log_data: pd.DataFrame):\n",
    "    # Filter the dataframe for the target filename\n",
    "    target_row = log_data[log_data['filename'] == filename]\n",
    "    \n",
    "    # If the target_row is empty, return None\n",
    "    if target_row.empty:\n",
    "        return None\n",
    "    \n",
    "    # Extract the log entry for the target row\n",
    "    log_entry = target_row.iloc[0]['log']\n",
    "    \n",
    "    # Check if the log entry contains 'PTX'\n",
    "    contains_ptx = 'PTX' in log_entry\n",
    "    \n",
    "    return contains_ptx\n",
    "    \n",
    "def get_slice_and_cell_id(filename: str, log_data: pd.DataFrame):\n",
    "    # Filter the dataframe for the target filename\n",
    "    target_row = log_data[log_data['filename'] == filename]\n",
    "    \n",
    "    # If the target_row is empty, return None\n",
    "    if target_row.empty:\n",
    "        return None, None\n",
    "    \n",
    "    # Extract the SliceID and CellID for the target row\n",
    "    slice_id = target_row.iloc[0]['SliceID']\n",
    "    cell_id = target_row.iloc[0]['CellID']\n",
    "    \n",
    "    return slice_id, cell_id\n",
    "def filter_by_interval_time(df):\n",
    "    # Drop rows with NaN values in the 'log' column\n",
    "    df = df.dropna(subset=['log'])\n",
    "    \n",
    "    # Regular expression to match \"IT_XXX ms\"\n",
    "    pattern = re.compile(r'IT_\\d+ ms')\n",
    "    \n",
    "    # Filter the DataFrame\n",
    "    filtered_df = df[df['log'].str.contains(pattern)]\n",
    "    \n",
    "    return filtered_df\n",
    "    \n",
    "def custom_merge(df1, df2, key, tolerance=1e-5):\n",
    "    # Convert key columns to numeric dtype\n",
    "    df1[key] = pd.to_numeric(df1[key], errors='coerce')\n",
    "    df2[key] = pd.to_numeric(df2[key], errors='coerce')\n",
    "    \n",
    "    # Drop rows with NaN keys that couldn't be converted\n",
    "    df1 = df1.dropna(subset=[key])\n",
    "    df2 = df2.dropna(subset=[key])\n",
    "    \n",
    "    # Sort values by the key for merge_asof\n",
    "    df1 = df1.sort_values(by=key)\n",
    "    df2 = df2.sort_values(by=key)\n",
    "\n",
    "    # Perform the merge_asof operation\n",
    "    merged_df = pd.merge_asof(df1, df2, on=key, direction='nearest', tolerance=tolerance, suffixes=('', '_forth'))\n",
    "    \n",
    "    # Handle columns that were not present in the original dataframes\n",
    "    for col in df1.columns:\n",
    "        if col != key and col not in merged_df.columns:\n",
    "            merged_df[col] = np.nan\n",
    "    \n",
    "    for col in df2.columns:\n",
    "        if col != key and col + '_forth' not in merged_df.columns:\n",
    "            merged_df[col + '_forth'] = np.nan\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "def get_abf_filenames_only(df):\n",
    "    # Extract the filenames\n",
    "    abf_filenames = df['filename'].tolist()\n",
    "    \n",
    "    return abf_filenames\n",
    "\n",
    "    \n",
    "def extract_period_from_sweep(csv_file_path, sweep_number, start_time, end_time):\n",
    "    \"\"\"\n",
    "    Extracts a specified period from a single sweep in a CSV file based on the provided start and end times.\n",
    "\n",
    "    Parameters:\n",
    "    - csv_file_path: str, path to the input CSV file\n",
    "    - sweep_number: int, the sweep number to extract the period from\n",
    "    - start_time: float, start time of the period to extract\n",
    "    - end_time: float, end time of the period to extract\n",
    "\n",
    "    Returns:\n",
    "    - extracted_df: pandas DataFrame, the extracted period of data\n",
    "    \"\"\"\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(csv_file_path, skiprows=11)\n",
    "    #df = pd.read_csv(csv_file_path, skiprows=11, dtype=str)\n",
    "    #df = df.map(Decimal)\n",
    "\n",
    "    # Extract the time points\n",
    "    time_points = df['Time (seconds)']\n",
    "\n",
    "    # Identify the columns related to the specified sweep\n",
    "    sweep_columns = [col for col in df.columns if f\"Sweep {sweep_number} Vm_scaled (mV)\" in col]\n",
    "\n",
    "    # Filter the DataFrame based on the specified period and selected sweep columns\n",
    "    extracted_df = df[(time_points >= start_time) & (time_points <= end_time)][['Time (seconds)'] + sweep_columns]\n",
    "\n",
    "    # Rename Vm_scaled columns to a consistent name\n",
    "    vm_scaled_columns = [col for col in sweep_columns if 'Vm_scaled' in col]\n",
    "    for col in vm_scaled_columns:\n",
    "        extracted_df.rename(columns={col: 'Vm_scaled'}, inplace=True)\n",
    "\n",
    "    return extracted_df\n",
    "\n",
    "def set_baseline_for_vm_scaled(df, baseline_start_time, baseline_end_time):\n",
    "    \"\"\"\n",
    "    Sets a baseline for the Vm_scaled data in the DataFrame. If a second set of baseline times is provided,\n",
    "    it adjusts the baseline to zero by detrending between two baseline periods.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame, the input data\n",
    "    - baseline_start_time1: float, start time of the first baseline period\n",
    "    - baseline_end_time1: float, end time of the first baseline period\n",
    "    - baseline_start_time2: float, start time of the second baseline period (optional)\n",
    "    - baseline_end_time2: float, end time of the second baseline period (optional)\n",
    "\n",
    "    Returns:\n",
    "    - df: pandas DataFrame, the data with baseline subtracted for Vm_scaled\n",
    "    \"\"\"\n",
    "    # Extract the time points\n",
    "    time_points = df['Time (seconds)']\n",
    "\n",
    "    # Identify the columns related to Vm_scaled\n",
    "    vm_scaled_columns = [col for col in df.columns if \"Vm_scaled\" in col]\n",
    "\n",
    "    for col in vm_scaled_columns:\n",
    "        baseline_period = df[(time_points >= baseline_start_time) & (time_points <= baseline_end_time)][col]\n",
    "        baseline_value = baseline_period.mean()\n",
    "\n",
    "        # Subtract the baseline from the Vm_scaled column\n",
    "        df[col] = df[col] - baseline_value\n",
    "        df[col] = detrend(df[col])\n",
    "\n",
    "    return df\n",
    "\n",
    "def reset_time_to_zero(df):\n",
    "    \"\"\"\n",
    "    Resets the first time point of the Time (seconds) column to be 0.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame, the input data\n",
    "\n",
    "    Returns:\n",
    "    - df: pandas DataFrame, the data with the time points reset\n",
    "    \"\"\"\n",
    "    # Get the first time point\n",
    "    first_time_point = df['Time (seconds)'].iloc[0]\n",
    "\n",
    "    # Subtract the first time point from all time points to reset the first time to 0\n",
    "    df['Time (seconds)'] = df['Time (seconds)'] - first_time_point\n",
    "\n",
    "    return df\n",
    "\n",
    "def extract_baseline_corrected_time_reset_data(csv_file_path, sweep_number, start_time, end_time_offset):\n",
    "    baseline_start_time = 0  # Specify the start time for the baseline period\n",
    "    baseline_end_time = 1  # Specify the end time for the baseline period\n",
    "    end_time = start_time + end_time_offset\n",
    "    \n",
    "    # Extract the period from the specified sweep\n",
    "    extracted_data = extract_period_from_sweep(csv_file_path, sweep_number, start_time, end_time)\n",
    "    \n",
    "    # Reset the time to zero\n",
    "    time_reset_data = reset_time_to_zero(extracted_data)\n",
    "    \n",
    "    # Set the baseline for the Vm_scaled data\n",
    "    baseline_corrected_data = set_baseline_for_vm_scaled(time_reset_data, baseline_start_time, baseline_end_time)\n",
    "    return baseline_corrected_data\n",
    "\n",
    "# Function to process data for each group\n",
    "def process_stimulus_data(group, csv_file_path, inter_stimulus_interval):\n",
    "    sweep_number = group.name\n",
    "    \n",
    "    first_stim_time = group[(group['ADC Name'] == 'Stim3')]['First Peak Time'].values[0]\n",
    "    third_stim_time = group[(group['ADC Name'] == 'Stim3')]['Second Peak Time'].values[0]\n",
    "    forth_stim_time = group[(group['ADC Name'] == 'Stim_2')]['Second Peak Time'].values[0]\n",
    "\n",
    "    # Calculate the start times for extraction\n",
    "    first_start_time = first_stim_time - 1\n",
    "    third_start_time = third_stim_time - 1\n",
    "    forth_start_time = forth_stim_time - 1\n",
    "\n",
    "    # Extract data for the first stimulus\n",
    "    first_stim_data = extract_baseline_corrected_time_reset_data(csv_file_path, sweep_number, first_start_time, end_time_offset)\n",
    "    first_stim_data['Sweep'] = sweep_number\n",
    "    first_stim_data['Time (seconds)'] = first_stim_data['Time (seconds)'].round(4)\n",
    "    \n",
    "    # Extract data for the third and fourth stimuli\n",
    "    third_stim_data = extract_baseline_corrected_time_reset_data(csv_file_path, sweep_number, third_start_time, end_time_offset)\n",
    "    forth_stim_data = extract_baseline_corrected_time_reset_data(csv_file_path, sweep_number, forth_start_time, end_time_offset)\n",
    "\n",
    "    third_stim_data['Time (seconds)'] = third_stim_data['Time (seconds)'].round(5)\n",
    "    forth_stim_data['Time (seconds)'] = forth_stim_data['Time (seconds)'].round(5)\n",
    "    forth_stim_data['Time (seconds)'] += inter_stimulus_interval\n",
    "\n",
    "    # Align and sum the Vm_scaled values\n",
    "    aligned_data = custom_merge(third_stim_data, forth_stim_data, 'Time (seconds)', tolerance=1e-5)\n",
    "    \n",
    "    for col in ['Vm_scaled']:\n",
    "        aligned_data[f'Combined {col}'] = aligned_data[col].fillna(0) + aligned_data[f'{col}_forth'].fillna(0)\n",
    "    \n",
    "    aligned_data['Sweep'] = sweep_number\n",
    "\n",
    "    return first_stim_data, aligned_data\n",
    "\n",
    "def sweep_filter(data, col_name):\n",
    "    # Define the threshold value\n",
    "    threshold = 20\n",
    "    \n",
    "    # Identify sweeps where Combined Vm_scaled exceeds the threshold\n",
    "    sweeps_to_exclude = data[data[col_name] > threshold]['Sweep'].unique()\n",
    "    return sweeps_to_exclude\n",
    "\n",
    "def create_a4_sheet(image_paths, output_dir, dpi=300):\n",
    "    # A4 size in pixels at 300 DPI\n",
    "    a4_width, a4_height = 2480, 3508\n",
    "    \n",
    "    # Create a blank A4 image\n",
    "    a4_image = Image.new('RGB', (a4_width, a4_height), (255, 255, 255))\n",
    "    \n",
    "    # Calculate the maximum width and height for each image to fit 3x4 grid\n",
    "    max_img_width = a4_width // 3\n",
    "    max_img_height = a4_height // 4\n",
    "    \n",
    "    x_offset = 0\n",
    "    y_offset = 0\n",
    "    \n",
    "    sheet_number = 1\n",
    "\n",
    "    for i, img_path in enumerate(image_paths):\n",
    "        img = Image.open(img_path)\n",
    "        \n",
    "        # Resize image to fit within the max dimensions\n",
    "        img.thumbnail((max_img_width, max_img_height))\n",
    "        \n",
    "        # Check if the current position is beyond the bounds, if so, save and create a new sheet\n",
    "        if y_offset + max_img_height > a4_height:\n",
    "            output_path = os.path.join(output_dir, f'a4_sheet_{sheet_number}.jpg')\n",
    "            a4_image.save(output_path, 'JPEG', dpi=(dpi, dpi))\n",
    "            a4_image = Image.new('RGB', (a4_width, a4_height), (255, 255, 255))\n",
    "            x_offset, y_offset = 0, 0\n",
    "            sheet_number += 1\n",
    "        \n",
    "        # Paste the image onto the A4 sheet\n",
    "        a4_image.paste(img, (x_offset, y_offset))\n",
    "        \n",
    "        # Update offsets\n",
    "        x_offset += max_img_width\n",
    "        if x_offset + max_img_width > a4_width:\n",
    "            x_offset = 0\n",
    "            y_offset += max_img_height\n",
    "    \n",
    "    # Save the last sheet if it has any images\n",
    "    if x_offset > 0 or y_offset > 0:\n",
    "        output_path = os.path.join(output_dir, f'a4_sheet_{sheet_number}.jpg')\n",
    "        a4_image.save(output_path, 'JPEG', dpi=(dpi, dpi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a322f4a-aa58-40e2-b9d8-cbdc793abd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_abf_file(filename):\n",
    "    # Load the data from the ABF file\n",
    "    # Find the file recursively\n",
    "    abf_file_path = log_search.find_file_recursively(\"/home/mitsuhiro/Data/PatchClamp\", filename)\n",
    "\n",
    "    # Find the CSV file in the same directory as the ABF file\n",
    "    log_file_path =  log_search.find_csv_in_same_directory(abf_file_path)\n",
    "    log_data = pd.read_csv(log_file_path)\n",
    "\n",
    "    # Filter the dataframe for the target filename\n",
    "    target_row = log_data[log_data['filename'] == filename]\n",
    "    \n",
    "    # Extract the log entry for the target row\n",
    "    log_entry = target_row.iloc[0]['log']\n",
    "    \n",
    "    # Use regular expression to find \"IT_XXX ms\"\n",
    "    match = re.search(r'IT_(\\d+) ms', log_entry)\n",
    "    inter_stimulus_interval_ms = int(match.group(1))\n",
    "    inter_stimulus_interval_s = inter_stimulus_interval_ms / 1000.0\n",
    "    inter_stimulus_interval = inter_stimulus_interval_s\n",
    "    \n",
    "    # Process the data\n",
    "    abf = pyabf.ABF(abf_file_path)\n",
    "    \n",
    "    # Extract metadata\n",
    "    metadata = {\n",
    "        \"Protocol\": abf.protocol,\n",
    "        \"ADCs\": \", \".join(abf.adcNames),\n",
    "        \"DACs\": \", \".join(abf.dacNames),\n",
    "        \"Units\": \", \".join(abf.adcUnits),\n",
    "        #\"Epochs\": abf.epochPerDacSection,\n",
    "        \"Sweep Points\": abf.sweepPointCount,\n",
    "        \"Sweep Length (seconds)\": abf.sweepLengthSec,\n",
    "        \"Sweep Count\": abf.sweepCount,\n",
    "        \"Sample Rate\": abf.dataRate,\n",
    "        \"Channel Count\": abf.channelCount,\n",
    "    }\n",
    "    \n",
    "    # Create a dictionary to store all data\n",
    "    data = {\"Time (seconds)\": abf.sweepX}\n",
    "    \n",
    "    # Extract data for each ADC and each sweep\n",
    "    for adc_index, adc_name in enumerate(abf.adcNames):\n",
    "        for sweep in range(abf.sweepCount):\n",
    "            abf.setSweep(sweep, channel=adc_index, baseline=[0,1])\n",
    "            data[f\"Sweep {sweep} {adc_name} ({abf.adcUnits[adc_index]})\"] = abf.sweepY\n",
    "    \n",
    "    # Create a DataFrame from the dictionary\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Convert metadata to DataFrame and transpose it\n",
    "    metadata_df = pd.DataFrame(metadata, index=[0]).T\n",
    "    metadata_df.columns = ['Value']\n",
    "    metadata_df.index.name = 'Metadata'\n",
    "    \n",
    "    # Save metadata and data to CSV\n",
    "    csv_file_path = abf_file_path.replace(\"abf\", \"csv\")\n",
    "    with open(csv_file_path, 'w', newline='') as f:\n",
    "        metadata_df.to_csv(f)\n",
    "        f.write(\"\\n\")\n",
    "        df.to_csv(f, index=False)\n",
    "    \n",
    "    print(f\"ABF file converted to CSV with metadata and data from all ADCs saved to {csv_file_path}\")\n",
    "\n",
    "    \n",
    "    #df = pd.read_csv(csv_file_path, skiprows=11)\n",
    "\n",
    "    # Extract the time points\n",
    "    time_points = df['Time (seconds)']\n",
    "    \n",
    "    # Define the ADC names for \"Stim_2\" and \"Stim3\"\n",
    "    adc_names_units = [(\"Stim_2\", \"V\"), (\"Stim3\", \"V\")]\n",
    "    \n",
    "    # Number of sweeps per ADC\n",
    "    sweeps_per_adc = abf.sweepNumber + 1\n",
    "    \n",
    "    # Minimum interval between peaks in seconds\n",
    "    min_interval = 1.0\n",
    "    \n",
    "    # Minimum peak height\n",
    "    min_peak_height = 4.0\n",
    "    \n",
    "    # Analyze each ADC data for significant peaks\n",
    "    peak_data = []\n",
    "    \n",
    "    for adc_name, unit in adc_names_units:\n",
    "        for sweep in range(sweeps_per_adc):\n",
    "            column_name = f\"Sweep {sweep} {adc_name} ({unit})\"\n",
    "            data = df[column_name]\n",
    "            \n",
    "            # Find the peaks in the data\n",
    "            peaks, properties = find_peaks(data, height=min_peak_height)\n",
    "            \n",
    "            # Filter peaks based on the minimum interval\n",
    "            valid_peaks = []\n",
    "            for peak in peaks:\n",
    "                if not valid_peaks or (time_points[peak] - time_points[valid_peaks[-1]]) >= min_interval:\n",
    "                    valid_peaks.append(peak)\n",
    "                if len(valid_peaks) == 2:\n",
    "                    break\n",
    "            \n",
    "            if len(valid_peaks) == 2:\n",
    "                sorted_peaks = sorted(valid_peaks, key=lambda x: time_points[x])\n",
    "            elif len(valid_peaks) == 1:\n",
    "                sorted_peaks = [valid_peaks[0], None]\n",
    "            else:\n",
    "                sorted_peaks = [None, None]\n",
    "            \n",
    "            peak_values = [data[peak] if peak is not None else None for peak in sorted_peaks]\n",
    "            peak_times = [time_points[peak] if peak is not None else None for peak in sorted_peaks]\n",
    "            peak_data.append((adc_name, sweep, peak_values[0], peak_times[0], peak_values[1], peak_times[1]))\n",
    "    \n",
    "    # Create a DataFrame to display the peak data\n",
    "    peak_df = pd.DataFrame(peak_data, columns=[\"ADC Name\", \"Sweep\", \"First Peak Value\", \"First Peak Time\", \"Second Peak Value\", \"Second Peak Time\"])\n",
    "    \n",
    "    peak_df_csv_file_path = csv_file_path.replace(\".csv\", \"_peak.csv\")\n",
    "    peak_df.to_csv(peak_df_csv_file_path)\n",
    "    print(f'Peak data saved at {peak_df_csv_file_path}')\n",
    "    \n",
    "    # Load the peak data\n",
    "    #peak_df = pd.read_csv(peak_df_csv_file_path)\n",
    "    \n",
    "    # Initialize empty DataFrames to store combined and first stimulus data for all sweeps\n",
    "    all_combined_data = pd.DataFrame()\n",
    "    all_first_stim_data = pd.DataFrame()\n",
    "    # Add any data processing steps here if needed\n",
    "    # Apply the process_stimulus_data function to each group\n",
    "    results = peak_df.groupby('Sweep').apply(lambda group: process_stimulus_data(group, csv_file_path, inter_stimulus_interval))\n",
    "    \n",
    "    # Concatenate the results\n",
    "    all_first_stim_data = pd.concat([result[0] for result in results], ignore_index=True)\n",
    "    all_combined_data = pd.concat([result[1] for result in results], ignore_index=True)\n",
    "\n",
    "    #all_first_stim_data.to_csv(all_first_stim_data_path)\n",
    "    #all_combined_data.to_csv(all_combined_data_path)\n",
    "    \n",
    "\n",
    "    # Filter out the sweeps from the dataset\n",
    "    filtered_combined_data = all_combined_data[~all_combined_data['Sweep'].isin(sweep_filter(all_combined_data, 'Combined Vm_scaled'))]\n",
    "    filtered_first_stim_data = all_first_stim_data[~all_first_stim_data['Sweep'].isin(sweep_filter(all_first_stim_data, 'Vm_scaled'))]\n",
    "\n",
    "    filtered_first_stim_data_path = csv_file_path.replace('.csv', '_firststim.csv')\n",
    "    filtered_combined_data_path = csv_file_path.replace('.csv', '_combined.csv')\n",
    "    filtered_first_stim_data.to_csv(filtered_first_stim_data_path)\n",
    "    filtered_combined_data.to_csv(filtered_combined_data_path)\n",
    "    print(f'firt stim data and combined data are saved at {filtered_first_stim_data_path} and {filtered_combined_data_path}, respectively')\n",
    "    \n",
    "    # Calculate the mean and SEM for Vm_scaled for first stimulus data\n",
    "    mean_first_stim_data = filtered_first_stim_data.groupby('Time (seconds)')['Vm_scaled'].mean()\n",
    "    sem_first_stim_data = filtered_first_stim_data.groupby('Time (seconds)')['Vm_scaled'].sem()\n",
    "    \n",
    "    # Calculate the mean and SEM for Vm_scaled for combined data\n",
    "    mean_combined_data = filtered_combined_data.groupby('Time (seconds)')['Combined Vm_scaled'].mean()\n",
    "    sem_combined_data = filtered_combined_data.groupby('Time (seconds)')['Combined Vm_scaled'].sem()\n",
    "\n",
    "    # Retrieve the SliceID and CellID using the function\n",
    "    slice_id, cell_id = get_slice_and_cell_id(filename, log_data)\n",
    "    \n",
    "    # Generate the plot\n",
    "    # Plot the mean and SEM for Vm_scaled for both first stimulus and combined data\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot each sweep of the first data\n",
    "    for sweep in filtered_first_stim_data['Sweep'].unique():\n",
    "        sweep_data = filtered_first_stim_data[filtered_first_stim_data['Sweep'] == sweep]\n",
    "        plt.plot(sweep_data['Time (seconds)'], sweep_data['Vm_scaled'], color='blue', alpha=0.1, linewidth=0.2)\n",
    "    \n",
    "    \n",
    "    # Plot first stimulus data\n",
    "    plt.plot(mean_first_stim_data.index, mean_first_stim_data, label='Actual Mean Vm_scaled', linestyle='-', color='blue', alpha = 1, linewidth = 0.5)\n",
    "    #plt.fill_between(mean_first_stim_data.index, mean_first_stim_data - sem_first_stim_data, mean_first_stim_data + sem_first_stim_data, alpha=0.2, color='blue', label='First Stim SEM Vm_scaled')\n",
    "\n",
    "    # Plot each sweep of the combined data\n",
    "    for sweep in filtered_combined_data['Sweep'].unique():\n",
    "        sweep_data = filtered_combined_data[filtered_combined_data['Sweep'] == sweep]\n",
    "        plt.plot(sweep_data['Time (seconds)'], sweep_data['Combined Vm_scaled'], color='red', alpha=0.1, linewidth=0.2)\n",
    "    \n",
    "    # Plot combined data\n",
    "    plt.plot(mean_combined_data.index, mean_combined_data, label='Estimated Mean Vm_scaled', linestyle='-', color='red', alpha = 1, linewidth = 0.5)\n",
    "    #plt.fill_between(mean_combined_data.index, mean_combined_data - sem_combined_data, mean_combined_data + sem_combined_data, alpha=0.2, color='red', label='Combined SEM Vm_scaled')\n",
    "    \n",
    "    plt.xlabel('Time (seconds)')\n",
    "    plt.ylabel('Vm_scaled (mV)')\n",
    "    if check_log_for_ptx(filename, log_data):\n",
    "        plt.title(f'Vm of actual and estimated at interval {inter_stimulus_interval_ms} ms with PTX for {slice_id} and {cell_id}',\n",
    "                 fontsize = 20)\n",
    "    else:\n",
    "        plt.title(f'Vm of actual and estimated at interval {inter_stimulus_interval_ms} ms for {slice_id} and {cell_id}',\n",
    "                 fontsize = 20)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Set the x-axis limits\n",
    "    plt.xlim(0.5, 2.5)\n",
    "    \n",
    "    plt_save_path = csv_file_path.replace('csv', 'jpg')\n",
    "    plt.savefig(plt_save_path, format=\"jpg\")\n",
    "    print(f'Plot was saved at {plt_save_path}.')\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eebdba-076a-40b2-94b1-abb6722a47ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Parameters\n",
    "end_time_offset = 5  # Specify the end time for extraction\n",
    "# Load the CSV file\n",
    "log_filename = '2025.02.26.csv'\n",
    "# Find the file recursively\n",
    "directory_log_filename = log_search.find_file_recursively(\"./Data\", log_filename)\n",
    "log = pd.read_csv(directory_log_filename)\n",
    "\n",
    "# Apply the function\n",
    "filtered_log = filter_by_interval_time(log)\n",
    "\n",
    "# Get the abf filenames from the filtered dataframe\n",
    "abf_filenames = get_abf_filenames_only(filtered_log)\n",
    "\n",
    "# Example usage\n",
    "#plot_abf_file('24505010.abf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cd846b-1b1e-46d0-a7e5-9161e0d89be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import concurrent.futures\n",
    "\n",
    "def process_file(file):\n",
    "    try:\n",
    "        plot_abf_file(file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    # Map the function to your list of abf_filenames\n",
    "    executor.map(process_file, abf_filenames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1fd9ee-9da3-4494-89c0-6c800c8a7f34",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "jpg_files = log_search.find_jpgs_in_same_directory(directory_log_filename)\n",
    "if jpg_files:\n",
    "    directory = os.path.dirname(directory_log_filename)\n",
    "    output_directory = os.path.join(directory, 'output_sheets')\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    create_a4_sheet(jpg_files, output_directory)\n",
    "else:\n",
    "    print(\"No JPG files found in the directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1068a663-4c07-4dbf-88a2-89c52291a206",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = log_search.find_file_recursively(\"./Data\", os.path.basename(\"ほげほげ\"))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7907b503-73ac-4c70-b54e-621d0f7bdeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "lambda_rate = 10  # Mean firing rate in Hz\n",
    "duration = 0.5  # Duration of spike train in seconds\n",
    "\n",
    "# Generate Poisson spike train\n",
    "ISIs = np.random.exponential(scale=1/lambda_rate, size=int(lambda_rate*duration))\n",
    "spike_times = np.cumsum(ISIs)\n",
    "\n",
    "# Keep only spikes within the desired duration\n",
    "spike_times = spike_times[spike_times <= duration]\n",
    "\n",
    "print(\"Spike times (in seconds):\", spike_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1626ea42-677f-46ae-a921-2dbd7349d099",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
