{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64a4cf00-be0d-4e97-80ec-816f9d8b54a4",
   "metadata": {},
   "source": [
    "# ログ検索でcsvファイルを用意しておく\n",
    "Data/PatchClamp/analysis/ログ検索.ipynb\n",
    "http://192.168.11.19:8888/lab/tree/Data/PatchClamp/analysis/%E3%83%AD%E3%82%B0%E6%A4%9C%E7%B4%A2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837d6685-24e3-4b03-bdfa-7a26acc8a83d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext cudf.pandas\n",
    "import pyabf\n",
    "import sys; sys.path.append(\"./\")\n",
    "import log_search\n",
    "import pandas as pd\n",
    "from scipy.signal import find_peaks, detrend\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from decimal import Decimal, ROUND_HALF_UP, ROUND_FLOOR, ROUND_CEILING\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1aca992-07ac-4260-b01b-ae870c8d80b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_log_for_ptx(filename: str, log_data: pd.DataFrame):\n",
    "    # Filter the dataframe for the target filename\n",
    "    target_row = log_data[log_data['filename'] == filename]\n",
    "    \n",
    "    # If the target_row is empty, return None\n",
    "    if target_row.empty:\n",
    "        return None\n",
    "    \n",
    "    # Extract the log entry for the target row\n",
    "    log_entry = target_row.iloc[0]['log']\n",
    "    \n",
    "    # Check if the log entry contains 'PTX'\n",
    "    contains_ptx = 'PTX' in log_entry\n",
    "    \n",
    "    return contains_ptx\n",
    "    \n",
    "def get_slice_and_cell_id(filename: str, log_data: pd.DataFrame):\n",
    "    # Filter the dataframe for the target filename\n",
    "    target_row = log_data[log_data['filename'] == filename]\n",
    "    \n",
    "    # If the target_row is empty, return None\n",
    "    if target_row.empty:\n",
    "        return None, None\n",
    "    \n",
    "    # Extract the SliceID and CellID for the target row\n",
    "    slice_id = target_row.iloc[0]['SliceID']\n",
    "    cell_id = target_row.iloc[0]['CellID']\n",
    "    \n",
    "    return slice_id, cell_id\n",
    "\n",
    "\n",
    "\n",
    "def get_abf_filenames_only(df):\n",
    "    # Extract the filenames\n",
    "    abf_filenames = df['filename'].tolist()\n",
    "    \n",
    "    return abf_filenames\n",
    "def extract_period_from_sweep(csv_file_path, sweep_number, start_time, end_time):\n",
    "    \"\"\"\n",
    "    Extracts a specified period from a single sweep in a CSV file based on the provided start and end times.\n",
    "\n",
    "    Parameters:\n",
    "    - csv_file_path: str, path to the input CSV file\n",
    "    - sweep_number: int, the sweep number to extract the period from\n",
    "    - start_time: float, start time of the period to extract\n",
    "    - end_time: float, end time of the period to extract\n",
    "\n",
    "    Returns:\n",
    "    - extracted_df: pandas DataFrame, the extracted period of data\n",
    "    \"\"\"\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(csv_file_path, skiprows=11)\n",
    "    #df = pd.read_csv(csv_file_path, skiprows=11, dtype=str)\n",
    "    #df = df.map(Decimal)\n",
    "\n",
    "    # Extract the time points\n",
    "    time_points = df['Time (seconds)']\n",
    "\n",
    "    # Identify the columns related to the specified sweep\n",
    "    sweep_columns = [col for col in df.columns if f\"Sweep {sweep_number} Im_scaled (pA)\" in col]\n",
    "\n",
    "    # Filter the DataFrame based on the specified period and selected sweep columns\n",
    "    extracted_df = df[(time_points >= start_time) & (time_points <= end_time)][['Time (seconds)'] + sweep_columns]\n",
    "\n",
    "    # Rename Vm_scaled columns to a consistent name\n",
    "    vm_scaled_columns = [col for col in sweep_columns if 'Im_scaled' in col]\n",
    "    for col in vm_scaled_columns:\n",
    "        extracted_df.rename(columns={col: 'Im_scaled'}, inplace=True)\n",
    "\n",
    "    return extracted_df\n",
    "\n",
    "def set_baseline_for_im_scaled(df, baseline_start_time, baseline_end_time):\n",
    "    \"\"\"\n",
    "    Sets a baseline for the Im_scaled data in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame, the input data\n",
    "    - baseline_start_time: float, start time of the baseline period\n",
    "    - baseline_end_time: float, end time of the baseline period\n",
    "\n",
    "    Returns:\n",
    "    - df: pandas DataFrame, the data with baseline subtracted for Im_scaled\n",
    "    \"\"\"\n",
    "    # Extract the time points\n",
    "    time_points = df['Time (seconds)']\n",
    "\n",
    "    # Identify the columns related to Im_scaled\n",
    "    im_scaled_columns = [col for col in df.columns if \"Im_scaled\" in col]\n",
    "\n",
    "    for col in im_scaled_columns:\n",
    "        baseline_period = df[(time_points >= baseline_start_time) & (time_points <= baseline_end_time)][col]\n",
    "        baseline_value = baseline_period.mean()\n",
    "\n",
    "        # Subtract the baseline from the Im_scaled column\n",
    "        df[col] = df[col] - baseline_value\n",
    "        df[col] = detrend(df[col])\n",
    "\n",
    "    return df\n",
    "\n",
    "def reset_time_to_zero(df):\n",
    "    \"\"\"\n",
    "    Resets the first time point of the Time (seconds) column to be 0.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame, the input data\n",
    "\n",
    "    Returns:\n",
    "    - df: pandas DataFrame, the data with the time points reset\n",
    "    \"\"\"\n",
    "    # Get the first time point\n",
    "    first_time_point = df['Time (seconds)'].iloc[0]\n",
    "    # Subtract the first time point from all time points to reset the first time to 0\n",
    "    df['Time (seconds)'] = df['Time (seconds)'] - first_time_point\n",
    "    \n",
    "    return df\n",
    "\n",
    "def extract_baseline_corrected_time_reset_data(csv_file_path, sweep_number, start_time, end_time_offset):\n",
    "    baseline_start_time = 0  # Specify the start time for the baseline period\n",
    "    baseline_end_time = 1  # Specify the end time for the baseline period\n",
    "    end_time = start_time + end_time_offset\n",
    "    \n",
    "    # Extract the period from the specified sweep\n",
    "    extracted_data = extract_period_from_sweep(csv_file_path, sweep_number, start_time, end_time)\n",
    "    \n",
    "    # Reset the time to zero\n",
    "    time_reset_data = reset_time_to_zero(extracted_data)\n",
    "    \n",
    "    # Set the baseline for the Vm_scaled data\n",
    "    baseline_corrected_data = set_baseline_for_im_scaled(time_reset_data, baseline_start_time, baseline_end_time)\n",
    "    return baseline_corrected_data\n",
    "    \n",
    "\n",
    "# Function to process data for each group\n",
    "def process_stimulus_data(group, csv_file_path):\n",
    "    sweep_number = group.name\n",
    "    \n",
    "    # Extract the first, third, and fourth stimulus times from the peak data\n",
    "    red_stim_time = group[(group['ADC Name'] == 'Stim3')]['Peak Time'].values[0]\n",
    "    blue_stim_time = group[(group['ADC Name'] == 'Stim_2')]['Peak Time'].values[0]\n",
    "\n",
    "    \n",
    "    # Calculate the start times for extraction\n",
    "    red_start_time = red_stim_time - 1\n",
    "    blue_start_time = blue_stim_time - 1\n",
    "    #print(red_stim_time)\n",
    "\n",
    "    if not np.isnan(red_start_time):\n",
    "        # Extract and process data for the red stimulus\n",
    "        red_stim_data = extract_baseline_corrected_time_reset_data(csv_file_path, sweep_number, red_start_time, end_time_offset)\n",
    "        red_stim_data['Sweep'] = sweep_number\n",
    "    else:\n",
    "        red_stim_data = None\n",
    "    \n",
    "    # Extract and process data for the blue stimulus\n",
    "    #print(blue_start_time)\n",
    "    blue_stim_data = extract_baseline_corrected_time_reset_data(csv_file_path, sweep_number, blue_start_time, end_time_offset)\n",
    "    blue_stim_data['Sweep'] = sweep_number\n",
    "\n",
    "    return blue_stim_data, red_stim_data\n",
    "\n",
    "def sweep_filter(data, col_name):\n",
    "    # Define the threshold value\n",
    "    threshold = -2000\n",
    "    \n",
    "    # Identify sweeps where Combined Vm_scaled exceeds the threshold\n",
    "    sweeps_to_exclude = data[data[col_name] < threshold]['Sweep'].unique()\n",
    "    return sweeps_to_exclude\n",
    "\n",
    "def create_a4_sheet(image_paths, output_dir, dpi=300):\n",
    "    # A4 size in pixels at 300 DPI\n",
    "    a4_width, a4_height = 2480, 3508\n",
    "    \n",
    "    # Create a blank A4 image\n",
    "    a4_image = Image.new('RGB', (a4_width, a4_height), (255, 255, 255))\n",
    "    \n",
    "    # Calculate the maximum width and height for each image to fit 3x4 grid\n",
    "    max_img_width = a4_width // 3\n",
    "    max_img_height = a4_height // 4\n",
    "    \n",
    "    x_offset = 0\n",
    "    y_offset = 0\n",
    "    \n",
    "    sheet_number = 1\n",
    "\n",
    "    for i, img_path in enumerate(image_paths):\n",
    "        img = Image.open(img_path)\n",
    "        \n",
    "        # Resize image to fit within the max dimensions\n",
    "        img.thumbnail((max_img_width, max_img_height))\n",
    "        \n",
    "        # Check if the current position is beyond the bounds, if so, save and create a new sheet\n",
    "        if y_offset + max_img_height > a4_height:\n",
    "            output_path = os.path.join(output_dir, f'a4_sheet_{sheet_number}.jpg')\n",
    "            a4_image.save(output_path, 'JPEG', dpi=(dpi, dpi))\n",
    "            a4_image = Image.new('RGB', (a4_width, a4_height), (255, 255, 255))\n",
    "            x_offset, y_offset = 0, 0\n",
    "            sheet_number += 1\n",
    "        \n",
    "        # Paste the image onto the A4 sheet\n",
    "        a4_image.paste(img, (x_offset, y_offset))\n",
    "        \n",
    "        # Update offsets\n",
    "        x_offset += max_img_width\n",
    "        if x_offset + max_img_width > a4_width:\n",
    "            x_offset = 0\n",
    "            y_offset += max_img_height\n",
    "    \n",
    "    # Save the last sheet if it has any images\n",
    "    if x_offset > 0 or y_offset > 0:\n",
    "        output_path = os.path.join(output_dir, f'a4_sheet_{sheet_number}.jpg')\n",
    "        a4_image.save(output_path, 'JPEG', dpi=(dpi, dpi))\n",
    "\n",
    "def filter_by_stim(df):\n",
    "    # Drop rows with NaN values in the 'log' column\n",
    "    df = df.dropna(subset=['log'])\n",
    "    \n",
    "    # Regular expression to match \"IT_XXX ms\"\n",
    "    pattern = re.compile(r'.*([BR]_([0-1](\\.\\d{1,2})?) A).*$$')\n",
    "    \n",
    "    # Filter the DataFrame\n",
    "    filtered_df = df[df['log'].str.contains(pattern)]\n",
    "    \n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a322f4a-aa58-40e2-b9d8-cbdc793abd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_abf_file(filename):\n",
    "    # Load the data from the ABF file\n",
    "    # Find the file recursively\n",
    "    abf_file_path = log_search.find_file_recursively(\"./Data\", filename)\n",
    "\n",
    "    # Find the CSV file in the same directory as the ABF file\n",
    "    log_file_path =  log_search.find_csv_in_same_directory(abf_file_path)\n",
    "    log_data = pd.read_csv(log_file_path)\n",
    "\n",
    "    # Filter the dataframe for the target filename\n",
    "    target_row = log_data[log_data['filename'] == filename]\n",
    "    \n",
    "    # Extract the log entry for the target row\n",
    "    log_entry = target_row.iloc[0]['log']\n",
    "    \n",
    "    # Process the data\n",
    "    abf = pyabf.ABF(abf_file_path)\n",
    "    \n",
    "    # Extract metadata\n",
    "    metadata = {\n",
    "        \"Protocol\": abf.protocol,\n",
    "        \"ADCs\": \", \".join(abf.adcNames),\n",
    "        \"DACs\": \", \".join(abf.dacNames),\n",
    "        \n",
    "        \"Units\": \", \".join(abf.adcUnits),\n",
    "        #\"Epochs\": abf.epochPerDacSection,\n",
    "        \"Sweep Points\": abf.sweepPointCount,\n",
    "        \"Sweep Length (seconds)\": abf.sweepLengthSec,\n",
    "        \"Sweep Count\": abf.sweepCount,\n",
    "        \"Sample Rate\": abf.dataRate,\n",
    "        \"Channel Count\": abf.channelCount,\n",
    "    }\n",
    "    \n",
    "    # Create a dictionary to store all data\n",
    "    data = {\"Time (seconds)\": abf.sweepX}\n",
    "    \n",
    "    # Extract data for each ADC and each sweep\n",
    "    for adc_index, adc_name in enumerate(abf.adcNames):\n",
    "        for sweep in range(abf.sweepCount):\n",
    "            abf.setSweep(sweep, channel=adc_index, baseline=[0,1])\n",
    "            data[f\"Sweep {sweep} {adc_name} ({abf.adcUnits[adc_index]})\"] = abf.sweepY\n",
    "    \n",
    "    # Create a DataFrame from the dictionary\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Convert metadata to DataFrame and transpose it\n",
    "    metadata_df = pd.DataFrame(metadata, index=[0]).T\n",
    "    metadata_df.columns = ['Value']\n",
    "    metadata_df.index.name = 'Metadata'\n",
    "    \n",
    "    # Save metadata and data to CSV\n",
    "    csv_file_path = abf_file_path.replace(\"abf\", \"csv\")\n",
    "    with open(csv_file_path, 'w', newline='') as f:\n",
    "        metadata_df.to_csv(f)\n",
    "        f.write(\"\\n\")\n",
    "        df.to_csv(f, index=False)\n",
    "    \n",
    "    print(f\"ABF file converted to CSV with metadata and data from all ADCs saved to {csv_file_path}\")\n",
    "\n",
    "    \n",
    "    #df = pd.read_csv(csv_file_path, skiprows=11)\n",
    "\n",
    "    # Extract the time points\n",
    "    time_points = df['Time (seconds)']\n",
    "    \n",
    "    # Define the ADC names for \"Stim_2\" and \"Stim3\"\n",
    "    adc_names_units = [(\"Stim_2\", \"V\"), (\"Stim3\", \"V\")]\n",
    "    \n",
    "    # Number of sweeps per ADC\n",
    "    sweeps_per_adc = abf.sweepNumber + 1\n",
    "    \n",
    "    # Minimum interval between peaks in seconds\n",
    "    min_interval = 1.0\n",
    "    \n",
    "    # Minimum peak height\n",
    "    min_peak_height = 4.0\n",
    "    \n",
    "    peak_data = []\n",
    "    \n",
    "    for adc_name, unit in adc_names_units:\n",
    "        for sweep in range(sweeps_per_adc):\n",
    "            column_name = f\"Sweep {sweep} {adc_name} ({unit})\"\n",
    "            data = df[column_name]\n",
    "            \n",
    "            # Find the peaks in the data\n",
    "            peaks, properties = find_peaks(data, height=min_peak_height)\n",
    "            \n",
    "            # Filter peaks based on the minimum interval\n",
    "            first_valid_peak = None\n",
    "            for peak in peaks:\n",
    "                if not first_valid_peak or (time_points[peak] - time_points[first_valid_peak]) >= min_interval:\n",
    "                    first_valid_peak = peak\n",
    "                    break\n",
    "            \n",
    "            if first_valid_peak is not None:\n",
    "                peak_value = data[first_valid_peak]\n",
    "                peak_time = time_points[first_valid_peak]\n",
    "            else:\n",
    "                peak_value = None\n",
    "                peak_time = None\n",
    "            \n",
    "            peak_data.append((adc_name, sweep, peak_value, peak_time))\n",
    "    \n",
    "    # Create a DataFrame to display the peak data\n",
    "    peak_df = pd.DataFrame(peak_data, columns=[\"ADC Name\", \"Sweep\", \"Peak Value\", \"Peak Time\"])\n",
    "    \n",
    "    peak_df_csv_file_path = csv_file_path.replace(\".csv\", \"_peak.csv\")\n",
    "    peak_df.to_csv(peak_df_csv_file_path)\n",
    "    print(f'Peak data saved at {peak_df_csv_file_path}')\n",
    "    \n",
    "    # Load the peak data\n",
    "    #peak_df = pd.read_csv(peak_df_csv_file_path)\n",
    "    \n",
    "    # Initialize empty DataFrames to store combined and first stimulus data for all sweeps\n",
    "    all_blue_stim_data = pd.DataFrame()\n",
    "    all_red_stim_data = pd.DataFrame()\n",
    "\n",
    "    # Add any data processing steps here if needed\n",
    "    # Apply the process_stimulus_data function to each group\n",
    "    results = peak_df.groupby('Sweep').apply(lambda group: process_stimulus_data(group, csv_file_path))\n",
    "    \n",
    "    # Concatenate the results\n",
    "    all_blue_data = pd.concat([result[0] for result in results], ignore_index=True)\n",
    "    try:\n",
    "        all_red_data = pd.concat([result[1] for result in results], ignore_index=True)\n",
    "    except Exception as e:\n",
    "        all_red_data = None\n",
    "        print(e)\n",
    "    \n",
    "\n",
    "    # Filter out the sweeps from the dataset\n",
    "    filtered_all_blue_stim_path = csv_file_path.replace('.csv', '_bluestim.csv')\n",
    "    filtered_all_red_stim_path = csv_file_path.replace('.csv', '_redstim.csv')\n",
    "    if all_red_data is not None:\n",
    "        filtered_all_red_stim_data = all_red_data[~all_red_data['Sweep'].isin(sweep_filter(all_red_data, 'Im_scaled'))]\n",
    "        filtered_all_red_stim_data.to_csv(filtered_all_red_stim_path)\n",
    "        print(f'red stim data and combined data are saved at {filtered_all_red_stim_path}')\n",
    "    filtered_all_blue_stim_data = all_blue_data[~all_blue_data['Sweep'].isin(sweep_filter(all_blue_data, 'Im_scaled'))]    \n",
    "    filtered_all_blue_stim_data.to_csv(filtered_all_blue_stim_path)\n",
    "    \n",
    "    print(f'blue stim data and combined data are saved at {filtered_all_blue_stim_path}')\n",
    "    \n",
    "    # Calculate the mean and SEM for Vm_scaled for blue stimulus data\n",
    "    mean_blue_stim_data = filtered_all_blue_stim_data.groupby('Time (seconds)')['Im_scaled'].mean()\n",
    "    sem_blue_stim_data = filtered_all_blue_stim_data.groupby('Time (seconds)')['Im_scaled'].sem()\n",
    "    \n",
    "    # Calculate the mean and SEM for Im_scaled for red stimulus data\n",
    "    if all_red_data is not None:   \n",
    "        mean_red_stim_data = filtered_all_red_data.groupby('Time (seconds)')['Im_scaled'].mean()\n",
    "        sem_red_stim_data = filtered_all_red_data.groupby('Time (seconds)')['Im_scaled'].sem()\n",
    "    \n",
    "    # Retrieve the SliceID and CellID using the function\n",
    "    slice_id, cell_id = get_slice_and_cell_id(filename, log_data)\n",
    "    \n",
    "    # Generate the plot\n",
    "    # Plot the mean and SEM for Vm_scaled for both first stimulus and combined data\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot each sweep of the first data\n",
    "    for sweep in filtered_all_blue_stim_data['Sweep'].unique():\n",
    "        sweep_data = filtered_all_blue_stim_data[filtered_all_blue_stim_data['Sweep'] == sweep]\n",
    "        plt.plot(sweep_data['Time (seconds)'], sweep_data['Im_scaled'], color='blue', alpha=0.1, linewidth=0.2)\n",
    "    \n",
    "    # Plot first stimulus data\n",
    "    plt.plot(mean_blue_stim_data.index, mean_blue_stim_data, label='Actual Mean Im_scaled', linestyle='-', color='blue', alpha = 1, linewidth = 0.5)\n",
    "    #plt.fill_between(mean_blue_stim_data.index, mean_blue_stim_data - sem_blue_stim_data, mean_blue_stim_data + sem_blue_stim_data, alpha=0.2, color='blue', label='Blue Stim SEM Im_scaled')\n",
    "    \n",
    "    if all_red_data is not None:  \n",
    "        # Plot each sweep of the first data\n",
    "        for sweep in filtered_all_red_stim_data['Sweep'].unique():\n",
    "            sweep_data = filtered_all_red_stim_data[filtered_all_red_stim_data['Sweep'] == sweep]\n",
    "            plt.plot(sweep_data['Time (seconds)'], sweep_data['Im_scaled'], color='red', alpha=0.1, linewidth=0.2)\n",
    "        \n",
    "        # Plot first stimulus data\n",
    "        plt.plot(mean_red_stim_data.index, mean_red_stim_data, label='Actual Mean Im_scaled', linestyle='-', color='red', alpha = 1, linewidth = 0.5)\n",
    "        #plt.fill_between(mean_red_stim_data.index, mean_red_stim_data - sem_red_stim_data, mean_red_stim_data + sem_red_stim_data, alpha=0.2, color='red', label='Red Stim SEM Im_scaled')\n",
    "    \n",
    "    plt.xlabel('Time (seconds)')\n",
    "    plt.ylabel('Im_scaled (pA)')\n",
    "    plt.title(f'Im')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Set the x-axis limits\n",
    "    plt.xlim(0.5, 2.5)\n",
    "\n",
    "    plt_save_path = csv_file_path.replace('csv', 'jpg')\n",
    "    plt.savefig(plt_save_path, format=\"jpg\")\n",
    "    print(f'Plot was saved at {plt_save_path}.')\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eebdba-076a-40b2-94b1-abb6722a47ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Parameters\n",
    "end_time_offset = 5  # Specify the end time for extraction\n",
    "# Load the CSV file\n",
    "log_filename = '2024.9.13.csv'\n",
    "# Find the file recursively\n",
    "directory_log_filename = log_search.find_file_recursively(\"./Data\", log_filename)\n",
    "log = pd.read_csv(directory_log_filename)\n",
    "\n",
    "# Apply the function\n",
    "filtered_log = filter_by_stim(log)\n",
    "\n",
    "# Get the abf filenames from the filtered dataframe\n",
    "abf_filenames = get_abf_filenames_only(filtered_log)\n",
    "\n",
    "# Example usage\n",
    "#plot_abf_file('24524003.abf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cd846b-1b1e-46d0-a7e5-9161e0d89be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for file in abf_filenames:\n",
    "    try:\n",
    "        plot_abf_file(file)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1fd9ee-9da3-4494-89c0-6c800c8a7f34",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "jpg_files = log_search.find_jpgs_in_same_directory(directory_log_filename)\n",
    "if jpg_files:\n",
    "    directory = os.path.dirname(directory_log_filename)\n",
    "    output_directory = os.path.join(directory, 'output_sheets')\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    create_a4_sheet(jpg_files, output_directory)\n",
    "else:\n",
    "    print(\"No JPG files found in the directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1068a663-4c07-4dbf-88a2-89c52291a206",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = log_search.find_file_recursively(\"./Data\", os.path.basename(\"ほげほげ\"))\n",
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
